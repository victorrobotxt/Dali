--- GLASHAUS PROJECT DUMP ---
Fri Dec 19 08:21:26 EET 2025


--- GIT HISTORY ---
* f22a7da (HEAD -> main) feat(core): implement AI engine, workflow logic, and docs
* e847d18 feat(backend): complete ORM models, repo pattern, and test suite
* d266a4d ops: add docker containerization and orchestration
* 07beab5 feat(scraper): implement simulation mode to bypass WAF during dev
* 8d13095 chore(scripts): include git log in context dumper
* 15c8316 refactor(api): connect routes to database via dependency injection
* 84bff79 feat(db): implement SQLAlchemy ORM models and Repository pattern
* 648c703 feat(core): implement abstract AI service layer
* 2e3b2a8 docs: add German functional specification (Lastenheft)
* 0307ac8 feat: init project structure, docs, and database schema


--- FILE STRUCTURE ---
.
./docs
./docs/ARCHITECTURE.md
./docs/api_contract.yaml
./docs/LASTENHEFT_DE.md
./docs/TECHNICAL_DEBT.md
./src
./src/api
./src/api/routes.py
./src/core
./src/core/config.py
./src/core/utils.py
./src/services
./src/services/ai_engine.py
./src/services/repository.py
./src/services/scraper_mvp.py
./src/services/scraper_service.py
./src/services/risk_engine.py
./src/models
./src/__init__.py
./src/main.py
./src/db
./src/db/session.py
./src/db/models.py
./scripts
./scripts/context_dump.sh
./prompts
./prompts/detective_prompt_v1.md
./db
./db/schema_v1.sql
./db/migration_001_status_workflow.sql
./README.md
./glashaus_context.txt
./requirements.txt
./imot_simulation.html
./Dockerfile
./docker-compose.yml
./tests
./tests/test_api.py


--- FILE CONTENTS ---


=========================================
FILE: ./docs/ARCHITECTURE.md
=========================================
# System Architecture

## Core Logic Flow (Cost Optimized)
1. **Ingest:** Scraper Service fetches URL.
2. **Analysis Tier 1 (Cheap):** Text Extraction (Gemini Flash).
   - *Goal:* Find Address in text.
   - *Cost:* ~$0.04/run.
3. **Gatekeeper:** Confidence Check (>90%).
4. **Analysis Tier 2 (Expensive):** Visual Detective (Gemini Pro).
   - *Trigger:* Only if Tier 1 fails.
   - *Goal:* Identify building via Facade/Landmarks/Geofencing.
   - *Cost:* ~$0.22/run.
5. **Verification:** Cross-reference extracted Address vs Cadastre API.

## Tech Stack
- **Lang:** Python (FastAPI) or Java (Spring Boot) - TBD
- **DB:** PostgreSQL + PostGIS (Geospatial extensions)
- **Queue:** Redis (Task offloading)


=========================================
FILE: ./docs/api_contract.yaml
=========================================
openapi: 3.0.0
info:
  title: Glashaus API
  version: 0.1.0
paths:
  /audit/url:
    post:
      summary: Initiate an Audit for a specific Listing URL
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                url:
                  type: string
                  format: uri
      responses:
        '200':
          description: Audit Complete
          content:
            application/json:
              schema:
                type: object
                properties:
                  report_id:
                    type: integer
                  risk_score:
                    type: integer
                  verified_address:
                    type: string
                  ai_reasoning:
                    type: array
                    items:
                      type: string


=========================================
FILE: ./docs/LASTENHEFT_DE.md
=========================================
# LASTENHEFT: Projekt Glashaus
**Version:** 1.0.0
**Status:** In Entwicklung

## 1. Einleitung
Das Projekt "Glashaus" ist eine automatisierte Due-Diligence-Plattform für den Immobilienmarkt in Sofia. Ziel ist die Beseitigung von Informationsasymmetrien durch den Einsatz von OSINT und KI-gestützter Datenanalyse.

## 2. Ist-Zustand (Problemstellung)
- **Datenfragmentierung:** Grundbuch (Registry), Kataster (Cadastre) und Gemeinde agieren in Silos.
- **Intransparenz:** Immobilienanzeigen enthalten oft ungenaue Flächenangaben und verschleierte Adressen.
- **Prozessineffizienz:** Manuelle Prüfungen sind teuer und langsam.

## 3. Soll-Zustand (Lösung)
Ein Microservices-System, das folgende Kernfunktionen bietet:
1.  **Automatische Adress-Deduktion:** Ermittlung der exakten Adresse aus unstrukturierten Anzeigentexten und Bildern.
2.  **Soll/Ist-Abgleich:** Automatischer Vergleich von Maklerangaben (Anzeige) mit amtlichen Katasterdaten.
3.  **Risikobewertung:** Algorithmische Berechnung eines "Risk Scores" (0-100).

## 4. Technische Anforderungen
- **Architektur:** Event-Driven Microservices (Python/FastAPI).
- **Datenbank:** PostgreSQL mit PostGIS für Geodatenverarbeitung.
- **KI-Integration:**
    - *Tier 1:* Textanalyse (Low Cost / Gemini Flash).
    - *Tier 2:* Visuelle Analyse (High Cost / Gemini Pro).

## 5. Nicht-funktionale Anforderungen
- **Idempotenz:** Wiederholte Uploads dürfen keine Datenkorruption verursachen.
- **Skalierbarkeit:** Das System muss Warteschlangen (Queues) nutzen, um Lastspitzen bei Scrapern abzufangen.


=========================================
FILE: ./docs/TECHNICAL_DEBT.md
=========================================
# TECHNICAL DEBT LOG

## Critical Severity (Must Fix Before Beta)

### 1. Database Session Scope in Background Tasks
- **Location:** `src/api/routes.py` -> `initiate_audit`
- **Issue:** Passing the dependency-injected `db` session to `BackgroundTasks` causes `DetachedInstanceError` because the session closes when the HTTP response returns.
- **Fix:** Refactor `process_audit_task` to instantiate a fresh `SessionLocal()` context manager internally.
- **Reference:** SQLAlchemy Thread-Safety docs.

### 2. AI Service Implementation
- **Location:** `src/services/ai_engine.py`
- **Issue:** Currently returns mock dictionary `{"confidence": 0.0}`.
- **Fix:** 
    - Initialize `genai.configure(api_key=...)`.
    - Implement `generate_content` call for Gemini Flash (Text).
    - Implement `generate_content` with Image inputs for Gemini Pro (Vision).
    - Add Error Handling for "Safety Filters" or API Quotas.

## Moderate Severity (Logic Gaps)

### 3. Risk Scoring Algorithm
- **Location:** Database Schema exists (`risk_score`), but logic is missing.
- **Issue:** No calculator exists to translate discrepancies into a 0-100 integer.
- **Fix:** Create `src/services/risk_engine.py`.
    - Base Score: 0
    - If `advertised_area` > `cadastre_area` (+20 pts).
    - If `type` mismatch (Atelier vs Apt) (+30 pts).
    - If `price_per_sqm` > 1.5x avg (+15 pts).

### 4. Hardcoded Secrets
- **Location:** `src/core/config.py`
- **Issue:** Default "mock-key" risks silent failure in prod.
- **Fix:** Implement `pydantic` validation to raise `ValueError` on startup if `GEMINI_API_KEY` is missing in production environment.

## Low Severity (Optimization)

### 5. Listing Normalization
- **Location:** `src/services/repository.py`
- **Issue:** Duplicate URLs might occur if query params differ (e.g., `?adv=1` vs `?adv=1&utm=facebook`).
- **Fix:** Implement a URL cleaner utility to strip tracking parameters before hashing/storing.


=========================================
FILE: ./src/api/routes.py
=========================================
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from src.db.session import get_db, SessionLocal
from src.db.models import Listing, ReportStatus
from src.services.repository import RealEstateRepository
from src.services.scraper_service import ScraperService
from src.services.ai_engine import GeminiService
from src.services.risk_engine import RiskEngine
from src.core.config import settings
from pydantic import BaseModel

router = APIRouter()

class AuditRequest(BaseModel):
    url: str
    price_override: float = 0.0

async def process_audit_task(listing_id: int):
    db = SessionLocal()
    try:
        repo = RealEstateRepository(db)
        scraper = ScraperService(simulation_mode=True)
        ai_engine = GeminiService(api_key=settings.GEMINI_API_KEY)
        risk_engine = RiskEngine()

        listing = db.query(Listing).get(listing_id)
        if not listing: return

        # 1. SCRAPE
        scraped_data = scraper.scrape_url(listing.source_url)
        listing.description_raw = scraped_data["raw_text"]
        db.commit()

        # 2. TIER 1: TEXT ANALYSIS (Cheap)
        print(f"PIPELINE: Running Tier 1 (Text) for {listing_id}...")
        tier1_result = await ai_engine.analyze_text(scraped_data["raw_text"])
        confidence = tier1_result.get("confidence", 0)
        
        final_ai_data = tier1_result
        total_cost = 0.04

        # 3. TIER 2: VISION ESCALATION (Expensive)
        # If Tier 1 is unsure (< 80%), we spend $0.22 to look at photos
        if confidence < 80 and settings.GEMINI_API_KEY != "mock-key":
            print(f"PIPELINE: Escalating to Tier 2 (Vision) - Confidence: {confidence}%")
            tier2_result = await ai_engine.analyze_images(scraped_data["image_urls"])
            final_ai_data["vision_insights"] = tier2_result
            total_cost += 0.22
            # Boost confidence if Vision confirms building
            confidence = max(confidence, tier2_result.get("confidence", 0))

        # 4. RISK CALCULATION
        report_data = risk_engine.calculate_score(scraped_data, final_ai_data)

        # 5. GENERATE FINAL REPORT
        repo.create_report(
            listing_id=listing.id,
            risk=report_data["score"],
            details={"flags": report_data["flags"], "ai_meta": final_ai_data},
            cost=total_cost,
            confidence=confidence
        )

    except Exception as e:
        print(f"PIPELINE_ERROR: {str(e)}")
    finally:
        db.close()

@router.post("/audit")
async def initiate_audit(request: AuditRequest, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    repo = RealEstateRepository(db)
    listing = repo.create_listing(url=request.url, price=request.price_override, area=0.0, desc="Queued")
    background_tasks.add_task(process_audit_task, listing.id)
    return {"listing_id": listing.id, "status": "QUEUED"}


=========================================
FILE: ./src/core/config.py
=========================================
import os

class Settings:
    PROJECT_NAME: str = "Glashaus"
    VERSION: str = "0.1.0"
    
    GEMINI_API_KEY: str = os.getenv("GEMINI_API_KEY", "mock-key")

    POSTGRES_USER: str = os.getenv("POSTGRES_USER", "postgres")
    POSTGRES_SERVER: str = os.getenv("POSTGRES_SERVER", "")
    POSTGRES_DB: str = os.getenv("POSTGRES_DB", "glashaus")
    POSTGRES_PASSWORD: str = os.getenv("POSTGRES_PASSWORD", "postgres")
    
    @property
    def DATABASE_URL(self) -> str:
        if not self.POSTGRES_SERVER:
            return "sqlite:///./glashaus.db"
        return f"postgresql://{self.POSTGRES_USER}:{self.POSTGRES_PASSWORD}@{self.POSTGRES_SERVER}/{self.POSTGRES_DB}"

settings = Settings()

if settings.GEMINI_API_KEY == "mock-key":
    print("--- WARNING: GEMINI_API_KEY is not set. Using MOCK mode. ---")


=========================================
FILE: ./src/core/utils.py
=========================================
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

def normalize_url(url: str) -> str:
    """
    Strips tracking parameters and fragments from real estate URLs.
    Example: ...imot.cgi?act=5&adv=123&utm_medium=fb -> ...imot.cgi?act=5&adv=123
    """
    u = urlparse(url)
    query = dict(parse_qsl(u.query))
    
    # Keep only essential keys for imot.bg and similar
    # act=5 is common for search results, adv=ID is the unique ad identifier
    whitelist = {'act', 'adv', 'id', 'slink'}
    clean_query = {k: v for k, v in query.items() if k.lower() in whitelist}
    
    # Reconstruct
    return urlunparse((
        u.scheme,
        u.netloc,
        u.path,
        u.params,
        urlencode(clean_query),
        '' # Strip fragments (#)
    ))


=========================================
FILE: ./src/services/ai_engine.py
=========================================
import google.generativeai as genai
import json
import os
from typing import Dict, Any, List
from src.core.config import settings

class AIEngineInterface:
    async def analyze_listing(self, text: str, images: List[str] = None) -> Dict[str, Any]:
        raise NotImplementedError

class GeminiService(AIEngineInterface):
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.prompt_path = "prompts/detective_prompt_v1.md"
        
        # Configure Gemini if Key is real
        if self.api_key != "mock-key":
            genai.configure(api_key=self.api_key)
            # Tier 1: Fast & Cheap (Text Only) - approx /data/data/com.termux/files/usr/bin/bash.04/M tokens
            self.model_flash = genai.GenerativeModel('gemini-1.5-flash')
            # Tier 2: Expensive (Multimodal / Vision) - approx /data/data/com.termux/files/usr/bin/bash.22/M tokens
            self.model_pro = genai.GenerativeModel('gemini-1.5-pro-vision')
        else:
            self.model_flash = None
            self.model_pro = None

    def _load_prompt(self, raw_text: str) -> str:
        """Loads the system prompt and injects the raw listing text."""
        # Fallback for Docker/Path issues
        if not os.path.exists(self.prompt_path):
            return f"Analyze this real estate listing and return JSON: {raw_text}"
            
        with open(self.prompt_path, "r") as f:
            template = f.read()
            
        # Inject the raw text into the {text_raw} slot defined in prompts/detective_prompt_v1.md
        # Defaulting neighborhood to 'Unknown' if not provided by scraper yet
        return template.replace("{text_raw}", raw_text).replace("{neighborhood}", "Unknown")

    async def analyze_text(self, text: str) -> Dict[str, Any]:
        """
        Orchestrates the Tier 1 Analysis.
        Maps to 'Analysis Tier 1 (Cheap)' in ARCHITECTURE.md.
        """
        if not self.model_flash:
            # Simulation Response for Dev Mode
            return {
                "address_prediction": "Simulated Address", 
                "confidence": 0.0, 
                "reasoning": "Mock Mode Active"
            }

        try:
            prompt = self._load_prompt(text)
            
            # Force JSON mode for structured data extraction
            response = self.model_flash.generate_content(
                prompt,
                generation_config={"response_mime_type": "application/json"}
            )
            
            # Parse the JSON output
            result = json.loads(response.text)
            
            # Ensure confidence is an integer for the Logic Gate
            if "confidence" not in result:
                result["confidence"] = 0
                
            return result

        except Exception as e:
            print(f"[AI ENGINE] Error: {e}")
            return {"error": str(e), "confidence": 0.0}

    async def analyze_images(self, image_urls: list[str]) -> Dict[str, Any]:
        # Placeholder for Tier 2 Logic
        return {"building_id": "UNKNOWN", "risk_factors": ["Vision implementation pending"]}


=========================================
FILE: ./src/services/repository.py
=========================================
from sqlalchemy.orm import Session
from src.db.models import Listing, Report, ReportStatus
from src.core.utils import normalize_url
from typing import Optional

class RealEstateRepository:
    def __init__(self, db: Session):
        self.db = db

    def create_listing(self, url: str, price: float, area: float, desc: str) -> Listing:
        clean_url = normalize_url(url)
        existing = self.get_listing_by_url(clean_url)
        if existing:
            return existing
            
        new_listing = Listing(
            source_url=clean_url,
            price_bgn=price,
            advertised_area_sqm=area,
            description_raw=desc
        )
        self.db.add(new_listing)
        self.db.commit()
        self.db.refresh(new_listing)
        return new_listing

    def get_listing_by_url(self, url: str) -> Optional[Listing]:
        return self.db.query(Listing).filter(Listing.source_url == url).first()

    def create_report(self, listing_id: int, risk: int, details: dict, cost: float, confidence: int = 0) -> Report:
        """
        Decision Logic:
        If AI Confidence < 70, the status is MANUAL_REVIEW regardless of risk.
        If Risk > 80, it's flagged as VERIFIED (but high risk).
        """
        status = ReportStatus.VERIFIED if confidence >= 70 else ReportStatus.MANUAL_REVIEW
        
        report = Report(
            listing_id=listing_id,
            risk_score=risk,
            ai_confidence_score=confidence,
            status=status,
            discrepancy_details=details,
            cost_to_generate=cost
        )
        self.db.add(report)
        self.db.commit()
        self.db.refresh(report)
        return report


=========================================
FILE: ./src/services/scraper_mvp.py
=========================================
from bs4 import BeautifulSoup
import os

# CONFIG
SIMULATION_MODE = True
MOCK_FILE = "imot_simulation.html"

def run_recon():
    print("[*] INTEL: Starting Reconnaissance Protocol...")
    
    html_content = ""
    
    if SIMULATION_MODE:
        print(f"[*] MODE: SIMULATION (Bypassing WAF)")
        if not os.path.exists(MOCK_FILE):
            print(f"[!] Error: Mock file {MOCK_FILE} not found.")
            return
            
        with open(MOCK_FILE, "r", encoding="utf-8") as f:
            html_content = f.read()
    else:
        # Network logic removed for Termux Safety
        pass

    soup = BeautifulSoup(html_content, 'html.parser')
    links = soup.find_all('a', href=True)
    
    print("[*] Parsing DOM Structure...")
    
    count = 0
    listings_found = []
    
    for link in links:
        href = link['href']
        
        if 'act=5' in href:
            # Normalize URL
            full_url = "https:" + href if href.startswith("//") else href
            
            if full_url in listings_found:
                continue
                
            listings_found.append(full_url)
            text_content = link.get_text(separator=" ", strip=True)
            
            count += 1
            print(f"\n[TARGET #{count}]")
            print(f"   URL: {full_url}")
            print(f"   RAW: {text_content}")

    print(f"\n[*] Mission Complete. {count} mock targets extracted.")

if __name__ == "__main__":
    run_recon()


=========================================
FILE: ./src/services/scraper_service.py
=========================================
from bs4 import BeautifulSoup
import os

class ScraperService:
    def __init__(self, simulation_mode=True):
        self.simulation = simulation_mode
        self.mock_file = "imot_simulation.html"

    def scrape_url(self, url: str) -> dict:
        print(f"[SCRAPER] Target: {url}")
        
        html_content = ""
        if self.simulation:
            if not os.path.exists(self.mock_file):
                return {"error": "Mock file missing"}
            with open(self.mock_file, "r", encoding="utf-8") as f:
                html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Extract main text
        ad_block = soup.find('div', class_='text_desc') # Updated to match your mock HTML
        raw_text = ad_block.get_text(strip=True) if ad_block else "No text found"

        # NEW: Extract Image URLs (Simulated)
        # In a real imot.bg scrape, you'd look for <img class="gallery">
        images = [
            "https://imot.bg/images/main_facade_1.jpg",
            "https://imot.bg/images/living_room_2.jpg",
            "https://imot.bg/images/window_view_3.jpg"
        ]
        
        return {
            "source_url": url,
            "raw_text": raw_text,
            "image_urls": images,
            "price_predicted": 185000.0,
            "area": 85.0
        }


=========================================
FILE: ./src/services/risk_engine.py
=========================================
from typing import Dict, Any, List

class RiskEngine:
    """
    Translates architectural and legal discrepancies into a 0-100 Risk Score.
    """
    def calculate_score(self, advertised_data: Dict, ai_insights: Dict) -> Dict[str, Any]:
        score = 0
        flags = []

        # 1. Legal Status Check (Atelier vs Apartment)
        raw_text = advertised_data.get("raw_text", "").upper()
        if "АТЕЛИЕ" in raw_text or "ATELIER" in raw_text:
            score += 30
            flags.append("LEGAL_STATUS_RISK: Property listed as Atelier (Possible industrial status)")

        # 2. Floor Logic
        if "ПОСЛЕДЕН" in raw_text or "TOP FLOOR" in raw_text:
            score += 15
            flags.append("MAINTENANCE_RISK: Top floor (Higher probability of roof leaks)")
        
        if "ПЪРВИ" in raw_text or "GROUND" in raw_text:
            score += 10
            flags.append("SECURITY_RISK: Ground floor/Low elevation")

        # 3. High Value Verification
        if advertised_data.get("price_predicted", 0) > 300000:
            score += 5
            flags.append("FINANCIAL: High-ticket verification required")

        return {
            "score": min(score, 100),
            "flags": flags
        }


=========================================
FILE: ./src/__init__.py
=========================================


=========================================
FILE: ./src/main.py
=========================================
from fastapi import FastAPI
from src.api import routes
from src.db.session import engine, Base

# Create Tables on Startup (The "Auto-Migration")
# In production, we would use Alembic, but this is Guerrilla Dev.
Base.metadata.create_all(bind=engine)

app = FastAPI(
    title="Glashaus API",
    description="Automated Real Estate Due Diligence Engine",
    version="0.1.0"
)

app.include_router(routes.router)

@app.get("/")
def health_check():
    return {
        "system": "GLASHAUS", 
        "status": "OPERATIONAL", 
        "motto": "Transparenz ist die Währung"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("src.main:app", host="0.0.0.0", port=8000, reload=True)


=========================================
FILE: ./src/db/session.py
=========================================
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
from src.core.config import settings

# SQLite requires a specific argument for threading
connect_args = {"check_same_thread": False} if "sqlite" in settings.DATABASE_URL else {}

engine = create_engine(
    settings.DATABASE_URL, 
    connect_args=connect_args,
    pool_pre_ping=True
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


=========================================
FILE: ./src/db/models.py
=========================================
import enum
from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, ForeignKey, JSON, Text, Enum
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from src.db.session import Base

class ReportStatus(set, enum.Enum):
    PENDING = "PENDING"
    PROCESSING = "PROCESSING"
    VERIFIED = "VERIFIED"
    MANUAL_REVIEW = "MANUAL_REVIEW"
    REJECTED = "REJECTED"

class Building(Base):
    __tablename__ = "buildings"

    id = Column(Integer, primary_key=True, index=True)
    cadastre_id = Column(String, unique=True, nullable=False, index=True)
    address_street = Column(String)
    address_number = Column(String)
    neighborhood = Column(String)
    gps_coordinates = Column(String) 
    construction_year = Column(Integer)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    reports = relationship("Report", back_populates="building")

class Listing(Base):
    __tablename__ = "listings"

    id = Column(Integer, primary_key=True, index=True)
    source_url = Column(String, unique=True, nullable=False, index=True)
    content_hash = Column(String(64), index=True) # For idempotency
    price_bgn = Column(Float)
    advertised_area_sqm = Column(Float)
    description_raw = Column(Text)
    scraped_at = Column(DateTime(timezone=True), server_default=func.now())

    reports = relationship("Report", back_populates="listing")

class Report(Base):
    __tablename__ = "reports"

    id = Column(Integer, primary_key=True, index=True)
    listing_id = Column(Integer, ForeignKey("listings.id"))
    building_id = Column(Integer, ForeignKey("buildings.id"), nullable=True)
    
    status = Column(Enum(ReportStatus), default=ReportStatus.PENDING)
    risk_score = Column(Integer)  # 0-100
    ai_confidence_score = Column(Integer, default=0)
    
    is_address_verified = Column(Boolean, default=False)
    discrepancy_details = Column(JSON)
    manual_review_notes = Column(Text)
    cost_to_generate = Column(Float)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    listing = relationship("Listing", back_populates="reports")
    building = relationship("Building", back_populates="reports")


=========================================
FILE: ./scripts/context_dump.sh
=========================================
#!/bin/bash
# Scans the repo and dumps text files for LLM context
output="glashaus_context.txt"
echo "--- GLASHAUS PROJECT DUMP ---" > "$output"
date >> "$output"

echo -e "\n\n--- GIT HISTORY ---" >> "$output"
git log --oneline --graph --decorate -n 20 >> "$output"

echo -e "\n\n--- FILE STRUCTURE ---" >> "$output"
tree -L 3 -I '.git|__pycache__|*.pyc' >> "$output" 2>/dev/null || find . -maxdepth 3 -not -path '*/.*' >> "$output"

echo -e "\n\n--- FILE CONTENTS ---" >> "$output"
find . -type f \
    -not -path '*/.*' \
    -not -path './glashaus_context.txt' \
    -not -name '*.png' \
    -not -name '*.jpg' \
    -not -name '*.sqlite' \
    | while read -r file; do
    echo -e "\n\n=========================================" >> "$output"
    echo "FILE: $file" >> "$output"
    echo "=========================================" >> "$output"
    cat "$file" >> "$output"
done

echo "Dump complete. Copy contents of $output"


=========================================
FILE: ./prompts/detective_prompt_v1.md
=========================================
# Role: Geospatial Detective
You are an expert Investigator. Your task is to identify the specific building address of a real estate listing based on limited metadata and photos.

## Input Data
- **Listing Text:** {text_raw}
- **Images:** {image_list}
- **Neighborhood Hint:** {neighborhood}

## Logic Constraints
1. **Fact vs Guess:** Distinguish between explicitly stated streets and visual deductions.
2. **The "Red House" Rule:** Look for unique landmarks in the "View from Window" photos.
3. **Facade Matching:** Describe the balcony curve/color and estimate construction era.

## Output Format (JSON Only)
{
  "address_prediction": "String",
  "confidence": 0-100,
  "reasoning_steps": [
    "Identified beige facade matching 2003 construction style.",
    "Located restaurant 'Chefs' on ground floor visible in photo 3.",
    "Triangulated address to Ul. Lyubata 13."
  ],
  "unit_type_discrepancy": "Apartment vs Atelier" (if applicable)
}


=========================================
FILE: ./db/schema_v1.sql
=========================================
-- Enable GIS extensions for location logic
CREATE EXTENSION IF NOT EXISTS postgis;

CREATE TABLE buildings (
    id SERIAL PRIMARY KEY,
    cadastre_id VARCHAR(50) UNIQUE NOT NULL, -- The official identifier
    address_street VARCHAR(255),
    address_number VARCHAR(50),
    neighborhood VARCHAR(100),
    gps_coordinates GEOMETRY(Point, 4326),
    construction_year INT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE listings (
    id SERIAL PRIMARY KEY,
    source_url TEXT UNIQUE NOT NULL,
    price_bgn DECIMAL(12, 2),
    advertised_area_sqm DECIMAL(10, 2),
    description_raw TEXT,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE reports (
    id SERIAL PRIMARY KEY,
    listing_id INT REFERENCES listings(id),
    building_id INT REFERENCES buildings(id),
    risk_score INT, -- 0 to 100 (100 is Toxic)
    is_address_verified BOOLEAN DEFAULT FALSE,
    discrepancy_details JSONB, -- Stores the "Apartment vs Atelier" logic
    cost_to_generate DECIMAL(10, 4) -- Tracking the $0.22 API cost
);


=========================================
FILE: ./db/migration_001_status_workflow.sql
=========================================
-- 1. Create the Workflow Status Enum
-- This supports the PENDING -> VERIFIED -> MANUAL_REVIEW flow
DO $$ BEGIN
    CREATE TYPE report_status AS ENUM ('PENDING', 'PROCESSING', 'VERIFIED', 'MANUAL_REVIEW', 'REJECTED');
EXCEPTION
    WHEN duplicate_object THEN null;
END $$;

-- 2. Update 'reports' table
ALTER TABLE reports 
ADD COLUMN IF NOT EXISTS status report_status DEFAULT 'PENDING',
ADD COLUMN IF NOT EXISTS ai_confidence_score INT DEFAULT 0,
ADD COLUMN IF NOT EXISTS manual_review_notes TEXT;

-- 3. Update 'listings' table for Idempotency
-- We hash the file/content to prevent duplicate processing of the same upload
ALTER TABLE listings
ADD COLUMN IF NOT EXISTS content_hash VARCHAR(64);

CREATE INDEX IF NOT EXISTS idx_listings_content_hash ON listings(content_hash);

-- 4. Create the Manual Review Queue View
-- This allows the Admin Panel to easily select tasks needing human eyes
CREATE OR REPLACE VIEW view_manual_review_queue AS
SELECT 
    r.id as report_id,
    l.source_url,
    r.ai_confidence_score,
    r.risk_score,
    r.created_at
FROM reports r
JOIN listings l ON r.listing_id = l.id
WHERE r.status = 'MANUAL_REVIEW'
ORDER BY r.risk_score DESC;


=========================================
FILE: ./README.md
=========================================
# GLASHAUS: The Real Estate Integrity Engine

## Mission
To eliminate information asymmetry in the Sofia real estate market via automated due diligence.
We leverage OSINT, LLM reasoning, and Official Registry cross-referencing.

## Architecture
- **Text Layer:** Gemini Flash (Cost optimized)
- **Vision Layer:** Gemini Pro (Geospatial reasoning)
- **Data Layer:** PostgreSQL (Structured) + S3 (Archives)

## Status
- **Phase:** Pre-Alpha / Architectural Blueprint
- **Deploy Target:** Jan 2026 (Launch)


=========================================
FILE: ./requirements.txt
=========================================
fastapi==0.109.0
uvicorn==0.27.0
sqlalchemy==2.0.25
psycopg2-binary==2.9.9
httpx==0.26.0
pydantic==2.6.0
pydantic-settings==2.1.0
google-generativeai==0.3.2
beautifulsoup4==4.12.3


=========================================
FILE: ./imot_simulation.html
=========================================
<!DOCTYPE html>
<html>
<body>
    <div class="list_ads">
        <!-- Mock Listing 1 -->
        <a href="//www.imot.bg/pcgi/imot.cgi?act=5&adv=1c171899111&slink=a4eg0c&f1=1" class="photoLink">
            <div class="text_desc">
                2-STAEN, Sofia, Lozenets, 185 000 EUR
            </div>
        </a>
        
        <!-- Mock Listing 2 -->
        <a href="//www.imot.bg/pcgi/imot.cgi?act=5&adv=2c172200231&slink=a4eg0c&f1=1" class="photoLink">
            <div class="text_desc">
                3-STAEN, Sofia, Krustova Vada, 250 000 EUR, Gas/Elevator
            </div>
        </a>

        <!-- Mock Listing 3 -->
        <a href="//www.imot.bg/pcgi/imot.cgi?act=5&adv=3c17992881&slink=a4eg0c&f1=1" class="photoLink">
            <div class="text_desc">
                ATELIER, Sofia, Center, 90 000 EUR
            </div>
        </a>
    </div>
</body>
</html>


=========================================
FILE: ./Dockerfile
=========================================
# Use Official Python Runtime
FROM python:3.11-slim

# Set Working Directory
WORKDIR /app

# Install System Dependencies (Postgres + GIS libs)
RUN apt-get update && apt-get install -y \
    libpq-dev gcc netcat-openbsd \
    && rm -rf /var/lib/apt/lists/*

# Install Python Dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy Code
COPY . .

# Expose Port
EXPOSE 8000

# Start Command (Wait for DB, then launch)
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]


=========================================
FILE: ./docker-compose.yml
=========================================
version: '3.8'

services:
  # 1. The Glashaus API
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - POSTGRES_SERVER=db
      - POSTGRES_USER=glashaus_user
      - POSTGRES_PASSWORD=secret_password
      - POSTGRES_DB=glashaus_db
    depends_on:
      - db
    volumes:
      - ./src:/app/src  # Hot Reload

  # 2. The Database (Postgres + GIS)
  db:
    image: postgis/postgis:15-3.4
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=glashaus_user
      - POSTGRES_PASSWORD=secret_password
      - POSTGRES_DB=glashaus_db
    ports:
      - "5432:5432"

volumes:
  postgres_data:


=========================================
FILE: ./tests/test_api.py
=========================================
from fastapi.testclient import TestClient
from src.main import app

client = TestClient(app)

def test_health_check():
    response = client.get("/")
    assert response.status_code == 200
    assert response.json()["status"] == "OPERATIONAL"

def test_audit_flow():
    payload = {"url": "https://www.imot.bg/pcgi/imot.cgi?act=5&adv=mock123"}
    response = client.post("/audit", json=payload)
    
    assert response.status_code == 200
    data = response.json()
    assert data["status"] == "QUEUED"
    assert "listing_id" in data
