--- GLASHAUS PROJECT DUMP ---
Mon Dec 29 19:42:07 EET 2025


--- GIT HISTORY ---
* 7510455 (HEAD -> main) feat(forensics): patch valuation blind spots (VAT, Net Area, Ground Floor)  * VAT Normalization: Scraper now detects "No VAT" listings and automatically normalizes price (+20%) to prevent valuation asymmetry.  * Terrace Dilution: Risk Engine now calculates Net-to-Gross area efficiency; flags "Valuation Warning" if living space < 60% of total (e.g., massive terraces).  * Time-Value Risk: Added logic to extract "Act 16" due dates; penalizes liquidity risk if completion is > 1 year out.  * Ground Floor Malus: Added specific valuation penalty for "Parter" units due to security/privacy risks.  * Direct Owner Detection: Added regex to identify 0% commission listings ("Direct from owner") to prioritize high-value targets.  * Prompt Engineering: Updated Detective Prompt to explicitly extract Net Area and Construction deadlines.
* 4ab8a35 docs: sync architecture and specs with v1.1 codebase state
* 8a82ecf chore(maint): cleanup workspace and update project context
* 3f8b25f refactor(config): externalize model configuration - Move hardcoded model name to src/core/config.py - Set default model to 'gemini-3.0-flash' (Current Gen) - Update GeminiService to use settings.GEMINI_MODEL
* 0201f30 feat(pipeline): integrate unified 3D municipal audit into worker task
* 9ac15c3 feat(forensics): implement 3D audit with live NAG registries - Add Expropriation check (The Death List) - Add Act 16 check (The Green List) - Add Building Permits check - Update headers to match browser emulation (X-Requested-With)
* f65c320 feat(forensics): implement automated cadastre auditing and social risk detection  * Implement CadastreForensicsService for direct KAIS integration  * Add audit_target.py CLI for property triangulation and ownership scanning  * Implement "Dirty Cop" algorithm: Detect social housing risks via municipal ownership ratios  * Add deep scanning for real area verification and neighbor analysis  * Persist authentication tokens for session reuse
* f0c6488 update docs and remove junk files
* 27d7a7c feat(forensics): implement legal RAG and geospatial triangulation
* 52420e1 refactor(core): implement hybrid scraping fallback and multimodal AI forensics Key Changes in this Commit:  * Multimodal AI Integration: Upgraded the GeminiService to use gemini-1.5-flash, enabling vision-based forensics by processing listing images alongside text.  * Forensic Property Analysis: Implemented a "Senior Real Estate Forensic Detective" prompt to detect "Atelier" status traps, identify architectural eras (e.g., Pre-1989 Panel vs. Post-2000 Brick), and perform orientation checks.  * Geospatial Verification: Added a new GeospatialService that uses the Google Maps Geocoding API to cross-reference AI-detected landmarks and addresses against the neighborhood claimed by the broker.  * Location Fraud Detection: The audit pipeline now automatically elevates risk scores (minimum of 70) if the geospatial verification finds a mismatch between the ad and visual evidence.  * Enhanced Scraper: Updated ScraperService to better extract neighborhood data and handle image URLs from imot.bg more reliably.
* 698eb25 refactor(core): implement hybrid scraping fallback and multimodal AI forensics
* 6f470a6 refactor(core): production hardening, type safety, and architecture cleanup
* 0f7d54d refactor(core): production hardening, type safety, and architecture cleanup
* d13df83 refactor(core): harden security, async concurrency, and financial precision
* 5a71d4d refactor(core): promote prototype to production-ready architecture
* fdd6519 docs: update architecture, API contract, and debt log to match V1 state
* a107438 feat(core): implement forensic analysis and anti-manipulation logic
* 2f79d2e removed junk
* ed4950e feat(forensics): harden backend with LexSofia logic and municipal registry strikes
* 00b1e10 feat(reporting): integrate forensic registry checks into Legal Brief generation


--- FILE STRUCTURE ---
.
├── Dockerfile
├── README.md
├── alembic.ini
├── bypass_audit.py
├── cookies.txt
├── db
│   ├── migration_001_status_workflow.sql
│   ├── migration_002_fix_currency.sql
│   ├── migrations
│   │   ├── env.py
│   │   └── versions
│   └── schema_v1.sql
├── docker-compose.yml
├── docs
│   ├── ARCHITECTURE.md
│   ├── LASTENHEFT_DE.md
│   ├── SCRAPER_V2_PROTOCOL.md
│   ├── TECHNICAL_DEBT.md
│   └── api_contract.yaml
├── forensics
│   ├── headers.txt
│   └── page_utf8.html
├── glashaus_context.txt
├── index.html
├── manual_session_audit.py
├── nginx
│   └── nginx.conf
├── prompts
│   └── detective_prompt_v1.md
├── requirements.txt
├── scraper_service.py
├── scripts
│   ├── audit_target.py
│   ├── context_dump.sh
│   ├── debug_kais.py
│   └── scrape_lex.py
├── src
│   ├── __init__.py
│   ├── api
│   │   └── routes.py
│   ├── core
│   │   ├── config.py
│   │   ├── logger.py
│   │   ├── patterns.py
│   │   ├── sofia_data.py
│   │   └── utils.py
│   ├── db
│   │   ├── models.py
│   │   └── session.py
│   ├── main.py
│   ├── models
│   ├── schemas.py
│   ├── services
│   │   ├── ai_engine.py
│   │   ├── base_provider.py
│   │   ├── cadastre_service.py
│   │   ├── city_risk_service.py
│   │   ├── compliance_service.py
│   │   ├── forensics_service.py
│   │   ├── geospatial_service.py
│   │   ├── legal_engine.py
│   │   ├── report_generator.py
│   │   ├── repository.py
│   │   ├── risk_engine.py
│   │   ├── scraper_mvp.py
│   │   ├── scraper_service.py
│   │   └── storage_service.py
│   ├── tasks.py
│   └── worker.py
├── test_forensics.py
└── tests
    └── test_api.py

16 directories, 57 files


--- FILE CONTENTS ---


=========================================
FILE: ./docs/ARCHITECTURE.md
=========================================
# System Architecture

## Core Logic Flow (Forensic Loop)
The system uses an Event-Driven architecture to orchestrate parallel forensic checks.

1. **Ingest & Normalize:**
   - [span_0](start_span)Scraper Service fetches URL (handling `windows-1251` encoding)[span_0](end_span).
   - [span_1](start_span)[span_2](start_span)URLs are normalized (mobile subdomain enforcement) and deduplicated via content hash[span_1](end_span)[span_2](end_span).

2. **The "Forensic Audit" Task (Celery Worker):**
   [span_3](start_span)[span_4](start_span)The system runs a **consolidated parallel audit** combining OSINT and Official Registry data[span_3](end_span)[span_4](end_span).
   
   - **Step A: Multimodal AI Analysis (Gemini 3.0 Flash):**
     - **[span_5](start_span)[span_6](start_span)[span_7](start_span)Vision:** Detects "Atelier" traps, estimates construction year, and performs inventory counts (AC/Radiators)[span_5](end_span)[span_6](end_span)[span_7](end_span).
     - **[span_8](start_span)Text:** Extracts unstructured data (Address, Construction Year, Heating types)[span_8](end_span).
   
   - **Step B: Geospatial Triangulation:**
     - [span_9](start_span)[span_10](start_span)Cross-references AI-detected landmarks against the claimed neighborhood using Google Maps Geocoding[span_9](end_span)[span_10](end_span).
   
   - **Step C: Unified Municipal Audit (NAG Registry):**
     - **[span_11](start_span)[span_12](start_span)Expropriation Check (The "Death List"):** Checks for municipal seizure plans[span_11](end_span)[span_12](end_span).
     - **[span_13](start_span)[span_14](start_span)Compliance Check (The "Green List"):** Verifies "Act 16" (Commissioning Certificate) status[span_13](end_span)[span_14](end_span).
     - **[span_15](start_span)Permit History:** Scans for valid building permits[span_15](end_span).
   
   - **Step D: Risk Engine V2:**
     - [span_16](start_span)[span_17](start_span)Merges all data points to calculate a composite `Risk Score` (0-100)[span_16](end_span)[span_17](end_span).
     - **[span_18](start_span)Fatal Flags:** Immediate rejection (e.g., Active Expropriation)[span_18](end_span).
     - **[span_19](start_span)[span_20](start_span)Discrepancy Flags:** Area Fraud (>25%), Missing Radiators vs. Central Heating claims[span_19](end_span)[span_20](end_span).

3. **Report Generation:**
   - [span_21](start_span)[span_22](start_span)`AttorneyReportGenerator` synthesizes a human-readable legal brief from the structured forensic data[span_21](end_span)[span_22](end_span).

## Tech Stack
- **[span_23](start_span)Service Layer:** Python 3.11 (FastAPI)[span_23](end_span)
- **[span_24](start_span)Asynchronous Task Queue:** Celery + Redis[span_24](end_span)
- **[span_25](start_span)Database:** PostgreSQL + PostGIS (Spatial data)[span_25](end_span)
- **[span_26](start_span)[span_27](start_span)AI Model:** Google Gemini 3.0 Flash (Multimodal)[span_26](end_span)[span_27](end_span)
- **[span_28](start_span)[span_29](start_span)Resilience:** Circuit Breakers implemented for Government Registry downtime[span_28](end_span)[span_29](end_span).

## Critical Components
| Component | Function | Status |
| :--- | :--- | :--- |
| **Risk Engine** | Calculates score based on Area Fraud, Legal Status (Atelier), Expropriation, and Radiator/Heating mismatches. | [span_30](start_span)[span_31](start_span)✅ Implemented (V2)[span_30](end_span)[span_31](end_span) |
| **SofiaMunicipalForensics** | Unified interface for `nag.sofia.bg` registries (Expropriation, Permits, Act 16). | [span_32](start_span)✅ Implemented[span_32](end_span) |
| **CadastreForensics** | Handles "Social Housing" risk detection and ownership ratio scanning. | [span_33](start_span)✅ Implemented[span_33](end_span) |

## Database Schema (Live State)
*Synced with `src/db/models.py` and Alembic Migrations 001-003.*

### 1. Listings Table
| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | Integer | [span_34](start_span)PK[span_34](end_span) |
| `source_url` | String | [span_35](start_span)Unique Index[span_35](end_span) |
| `content_hash` | String(64) | **[span_36](start_span)Idempotency**: SHA256(text + price)[span_36](end_span) |
| `price_bgn` | **Numeric(12,2)** | [span_37](start_span)[span_38](start_span)Financial precision (e.g., 150000.00)[span_37](end_span)[span_38](end_span) |
| `advertised_area_sqm` | **Numeric(10,2)** | [span_39](start_span)[span_40](start_span)Area precision (e.g., 65.50)[span_39](end_span)[span_40](end_span) |
| `description_raw` | Text | [span_41](start_span)Full ad body[span_41](end_span) |

### 2. Reports Table
| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | Integer | [span_42](start_span)PK[span_42](end_span) |
| `listing_id` | Integer | [span_43](start_span)FK -> Listings[span_43](end_span) |
| `status` | Enum | [span_44](start_span)`PENDING`, `PROCESSING`, `VERIFIED`, `MANUAL_REVIEW`, `REJECTED`[span_44](end_span) |
| `risk_score` | Integer | [span_45](start_span)0-100 (100 = Fatal)[span_45](end_span) |
| `ai_confidence_score` | Integer | [span_46](start_span)0-100 (Hallucination check)[span_46](end_span) |
| `cost_to_generate` | **Numeric(10,4)** | [span_47](start_span)[span_48](start_span)API Cost tracking (e.g., 0.0045 BGN)[span_47](end_span)[span_48](end_span) |
| `discrepancy_details` | JSON | [span_49](start_span)Specific flags (e.g. "Area mismatch > 25%")[span_49](end_span) |

### 3. Buildings Table (Registry Truth)
| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | Integer | [span_50](start_span)PK[span_50](end_span) |
| `cadastre_id` | String | [span_51](start_span)Unique Official ID[span_51](end_span) |
| `construction_year` | Integer | [span_52](start_span)Registry confirmed year[span_52](end_span) |
| `latitude/longitude` | Float | [span_53](start_span)Geocoded coordinates[span_53](end_span) |


=========================================
FILE: ./docs/api_contract.yaml
=========================================
openapi: 3.0.0
info:
  title: Glashaus API
  version: 1.0.0
paths:
  /audit/url:
    post:
      summary: Initiate an Audit for a specific Listing URL
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                url:
                  type: string
                  format: uri
      responses:
        '200':
          description: Audit Queued
          content:
            application/json:
              schema:
                type: object
                properties:
                  listing_id:
                    type: integer
                  status:
                    type: string
                    example: "QUEUED_IN_REDIS"

  /reports/{listing_id}:
    get:
      summary: Retrieve Forensic Report & Risk Score
      responses:
        '200':
          description: Detailed Audit Report
          content:
            application/json:
              schema:
                type: object
                properties:
                  report_id:
                    type: integer
                  status:
                    type: string
                    enum: [PENDING, PROCESSING, VERIFIED, MANUAL_REVIEW, REJECTED]
                  risk_score:
                    type: integer
                    description: 0-100 (100 is Toxic/Fatal)
                  cost:
                    type: float
                  manual_notes:
                    type: string
                  discrepancies:
                    type: object
                    properties:
                      flags:
                        type: array
                        items: 
                          type: string
                        example: ["CRITICAL: Property is listed for EXPROPRIATION.", "LEGAL: Atelier status."]
                      forensics:
                        type: object
                        properties:
                          city_risk:
                            type: object
                            properties:
                              is_expropriated: 
                                type: boolean
                              registry_status:
                                type: string
                          compliance:
                            type: object
                            properties:
                              has_act16:
                                type: boolean
                          cadastre:
                            type: object
                            properties:
                              official_area:
                                type: number
                              diff_percent:
                                type: number


=========================================
FILE: ./docs/LASTENHEFT_DE.md
=========================================
# LASTENHEFT: Projekt Glashaus
**Version:** 1.1.0
**Status:** In Entwicklung

## 1. Einleitung
[span_66](start_span)Das Projekt "Glashaus" ist eine automatisierte Due-Diligence-Plattform für den Immobilienmarkt in Sofia[span_66](end_span). Ziel ist die Beseitigung von Informationsasymmetrien durch den Einsatz von OSINT und multimodaler KI-Analyse.

## 2. Ist-Zustand (Problemstellung)
- **[span_67](start_span)Datenfragmentierung:** Grundbuch (Registry), Kataster (Cadastre) und Gemeinde (NAG) agieren in Silos[span_67](end_span).
- **[span_68](start_span)Intransparenz:** Immobilienanzeigen enthalten oft ungenaue Flächenangaben, "Atelier"-Fallen und verschleierte Adressen[span_68](end_span).
- **Prozessineffizienz:** Manuelle Prüfungen sind teuer und langsam.

## 3. Soll-Zustand (Lösung)
Ein Microservices-System, das folgende Kernfunktionen bietet:
1.  **[span_69](start_span)[span_70](start_span)Automatische Adress-Deduktion:** Ermittlung der exakten Adresse aus unstrukturierten Anzeigentexten und visueller Landmarken-Analyse[span_69](end_span)[span_70](end_span).
2.  **[span_71](start_span)3D-Audit (Unified Forensics):** Gleichzeitige Prüfung von Kataster (Fläche), Gemeinde (Enteignung/Baugenehmigung) und Gesetz (Nutzungsstatus)[span_71](end_span).
3.  **[span_72](start_span)Social Risk Detection:** Analyse der Eigentümerstruktur (Privat vs. Gemeinde) zur Erkennung von "Social Housing"-Risiken[span_72](end_span).
4.  **[span_73](start_span)Risikobewertung:** Algorithmische Berechnung eines "Risk Scores" (0-100)[span_73](end_span).

## 4. Technische Anforderungen
- **[span_74](start_span)Architektur:** Event-Driven Microservices (Python/FastAPI)[span_74](end_span).
- **[span_75](start_span)Datenbank:** PostgreSQL mit PostGIS für Geodatenverarbeitung[span_75](end_span).
- **KI-Integration:**
    - **[span_76](start_span)[span_77](start_span)Multimodal Engine:** Google Gemini 3.0 Flash[span_76](end_span)[span_77](end_span). [span_78](start_span)Verarbeitet Text und Bilder simultan zur Erkennung von visuellen Diskrepanzen (z.B. "Luxus" im Text vs. "Panel" im Bild)[span_78](end_span).

## 5. Nicht-funktionale Anforderungen
- **[span_79](start_span)Idempotenz:** Wiederholte Uploads dürfen keine Datenkorruption verursachen (`content_hash` Prüfungen)[span_79](end_span).
- **[span_80](start_span)Skalierbarkeit:** Das System nutzt Celery/Redis Warteschlangen, um Lastspitzen bei Scrapern abzufangen[span_80](end_span).


=========================================
FILE: ./docs/TECHNICAL_DEBT.md
=========================================
# TECHNICAL DEBT LOG

## Critical Severity (Must Fix Before Beta)

### 1. Scraper Fragility (DOM Coupling)
- **Location:** `src/services/scraper_service.py`
- **Issue:** Logic relies on specific HTML IDs or classes (e.g., `.price`).
- **Risk:** High. If `imot.bg` updates their frontend classes, extraction fails.
- **Fix:** Implement multi-selector fallbacks or move strictly to Visual DOM analysis (Gemini Vision) for extraction.
- **[span_54](start_span)[span_55](start_span)Update:** Mobile subdomain enforcement (`m.imot.bg`) has improved stability[span_54](end_span)[span_55](end_span).

### 2. Hardcoded Secrets (Config)
- **Location:** `src/core/config.py`
- **[span_56](start_span)[span_57](start_span)Issue:** `GEMINI_API_KEY` type hint implies no default, but mock keys or defaults exist in code logic[span_56](end_span)[span_57](end_span).
- **Fix:** Ensure strict environment variable enforcement in production Docker builds.

## Resolved Items (Fixed)

### ✅ Synchronous Registry Calls
- **[span_58](start_span)Fix:** `SofiaMunicipalForensics` (`src/services/forensics_service.py`) now uses `asyncio.gather` to run Expropriation, Act 16, and Permit checks in parallel[span_58](end_span).

### ✅ Database Session Scope in Background Tasks
- **[span_59](start_span)[span_60](start_span)Fix:** `src/tasks.py` explicitly initializes thread-safe sessions using `with SessionLocal() as db:`[span_59](end_span)[span_60](end_span).

### ✅ Risk Scoring Algorithm
- **[span_61](start_span)[span_62](start_span)Fix:** `RiskEngine` (V2) implemented in `src/services/risk_engine.py`[span_61](end_span)[span_62](end_span). [span_63](start_span)[span_64](start_span)Includes checks for "Infrastructure Mismatch" (Radiators) and "Location Fraud"[span_63](end_span)[span_64](end_span).

### ✅ Financial & Area Precision
- **[span_65](start_span)Fix:** Database schema migrated to `NUMERIC(12,2)` for price and `NUMERIC(10,2)` for area (Migration 002 & 003)[span_65](end_span).


=========================================
FILE: ./docs/SCRAPER_V2_PROTOCOL.md
=========================================
# Scraper Protocol V2 (Desktop/Legacy)

## Problem
The previous scraper targeted `m.imot.bg` (Mobile), which uses:
1.  **Empty HTML Shells:** Content is loaded via JavaScript/XHR, breaking standard parsers.
2.  **Strict WAF:** Cloudflare aggressively challenges TLS fingerprints on mobile endpoints.
3.  **Hidden Data:** Phone numbers and broker details are obfuscated.

## Solution: The "Desktop Fallback" Strategy
We switched the scraper to target `www.imot.bg` (Desktop) with specific headers to force the **Legacy HTML** version.

### Key Specs
* **Target:** `https://www.imot.bg/pcgi/imot.cgi?act=5&adv=...`
* **Encoding:** `windows-1251` (Critical: UTF-8 decoding causes mojibake).
* **Parsing:** Pure **Regex** (No `BeautifulSoup` or `Pydantic` needed).
    * *Reason:* The desktop site uses nested `<table>` structures ("Table Soup") which are hard to traverse with DOM parsers but easy to scrape with loose Regex patterns.

### Extracted Fields
* **Price:** Regex scan for `class="cena"`.
* **Images:** Extracted from `bigPicture()` JS calls (High-Res).
* **Phone:** Scanned from the entire body (bypasses "Show Phone" clicks).
* **Description:** Extracted from `#description_div`.

### Termux Compatibility
* Removed `pydantic` and `playwright` dependencies.
* Scraper is now 100% standard library + `httpx`.


=========================================
FILE: ./src/api/routes.py
=========================================
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from src.db.session import get_db
from src.db.models import Listing, Report, ReportStatus
from src.services.repository import RealEstateRepository
from src.tasks import audit_listing_task
from pydantic import BaseModel
from typing import Optional

router = APIRouter()

class AuditRequest(BaseModel):
    url: str
    price_override: float = 0.0

class ReportUpdate(BaseModel):
    status: str
    manual_notes: Optional[str] = None

@router.post("/audit")
async def initiate_audit(request: AuditRequest, db: Session = Depends(get_db)):
    """
    Submits a URL for auditing via the Celery Worker queue.
    """
    repo = RealEstateRepository(db)
    # Create listing immediately to return ID
    listing = repo.create_listing(url=request.url, price=request.price_override, area=0.0, desc="Queued")
    
    # Offload to Redis/Celery
    audit_listing_task.delay(listing.id)
    
    return {"listing_id": listing.id, "status": "QUEUED_IN_REDIS"}

@router.get("/reports/{listing_id}")
def get_report(listing_id: int, db: Session = Depends(get_db)):
    """
    Retrieves the report status and details for a listing.
    """
    listing = db.query(Listing).filter(Listing.id == listing_id).first()
    if not listing:
        raise HTTPException(status_code=404, detail="Listing not found")
        
    report = db.query(Report).filter(Report.listing_id == listing_id).first()
    if not report:
        # If no report exists yet, the worker is likely still processing
        return {"status": "PROCESSING", "details": "Audit is currently in the queue."}
        
    return {
        "report_id": report.id,
        "status": report.status,
        "risk_score": report.risk_score,
        "ai_confidence": report.ai_confidence_score,
        "discrepancies": report.discrepancy_details,
        "manual_notes": report.manual_review_notes,
        "cost": report.cost_to_generate,
        "created_at": report.created_at
    }

@router.patch("/reports/{report_id}")
def update_report_status(report_id: int, update: ReportUpdate, db: Session = Depends(get_db)):
    """
    Manual Review Action.
    Updates status (e.g., MANUAL_REVIEW -> VERIFIED).
    """
    report = db.query(Report).filter(Report.id == report_id).first()
    if not report:
        raise HTTPException(status_code=404, detail="Report not found")
        
    try:
        # Validate that the string provided matches the Enum
        new_status = ReportStatus(update.status)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid Status Enum")

    report.status = new_status
    if update.manual_notes:
        report.manual_review_notes = update.manual_notes
        
    db.commit()
    return {"id": report.id, "new_status": report.status}


=========================================
FILE: ./src/core/config.py
=========================================
from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import Optional

class Settings(BaseSettings):
    PROJECT_NAME: str = "Glashaus"
    VERSION: str = "0.1.0"
    
    # Database
    POSTGRES_USER: str = "postgres"
    POSTGRES_PASSWORD: str = "postgres"
    POSTGRES_DB: str = "glashaus"
    POSTGRES_HOST: str = "localhost"
    POSTGRES_PORT: int = 5432
    DATABASE_URL: str = f"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"

    # AI & External APIs
    GEMINI_API_KEY: str
    GEMINI_MODEL: str = "gemini-3.0-flash"  # <--- Updated to 3.0-flash
    GOOGLE_MAPS_API_KEY: Optional[str] = None
    
    # Security
    SECRET_KEY: str = "changethis_in_production"
    
    # Infra
    REDIS_URL: str = "redis://localhost:6379/0"

    model_config = SettingsConfigDict(env_file=".env", extra="ignore")

settings = Settings()


=========================================
FILE: ./src/core/utils.py
=========================================
import re
import hashlib

def extract_imot_id(url: str) -> str:
    match = re.search(r'(?:adv=|obiava-)([a-z0-9]+)', url)
    return match.group(1) if match else "unknown"

def normalize_url(url: str) -> str:
    if "imot.bg" in url:
        return url.replace("www.imot.bg", "m.imot.bg")
    return url

def calculate_content_hash(text: str, price: float) -> str:
    clean_text = re.sub(r'\s+', ' ', text).strip().lower()
    raw = f"{clean_text}{price}".encode('utf-8')
    return hashlib.sha256(raw).hexdigest()

def normalize_sofia_street(address: str) -> str:
    """Strips common Sofia prefixes that confuse the Cadastre search."""
    if not address: return ""
    # Remove 'ul.', 'bul.', 'zh.k', 'kv.' etc.
    patterns = [r'(?i)ул\.', r'(?i)бул\.', r'(?i)ж\.к\.', r'(?i)кв\.', r'(?i)гр\. София,?\s*']
    clean = address
    for p in patterns:
        clean = re.sub(p, '', clean)
    return clean.strip()


=========================================
FILE: ./src/core/sofia_data.py
=========================================
SOFIA_ADMIN_MAP = {
    "OBORISHTE": {"strictness": 5, "kindergarten_risk": "Critical"},
    "PODUYANE": {"strictness": 5, "kindergarten_risk": "High"},
    "CENTER": {"strictness": 4, "kindergarten_risk": "High"},
    "LOZENETS": {"strictness": 4, "kindergarten_risk": "Moderate"},
    "VITOSHA": {"strictness": 2, "kindergarten_risk": "Low"},
    "KRUSTOVA VADA": {"strictness": 2, "kindergarten_risk": "Low"},
}


=========================================
FILE: ./src/core/logger.py
=========================================
import structlog
import logging
import sys

def setup_logging():
    # Standard lib logging for libs that don't use structlog
    logging.basicConfig(format="%(message)s", stream=sys.stdout, level=logging.INFO)
    
    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.processors.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.JSONRenderer()
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

# Singleton logger instance
logger = structlog.get_logger()


=========================================
FILE: ./src/core/patterns.py
=========================================
import re
from typing import List

class ForensicPatterns:
    """
    Centralized regex registry for text analysis.
    """
    
    # Financial & Legal Flags
    VAT_EXCLUDED = re.compile(r"(?i)(цената е без ддс|без ддс|vat excluded|no vat|не се начислява ддс)")
    SPACE_HACK = re.compile(r"(?i)(преустроена? гарсониера|усвоен.*?балкон|кухня.*?коридор|бивша.*?кухня|маломерен|боксониера|таванско)")
    ATELIER_STATUTE = re.compile(r"(?i)(статут.*?ателие|статут на ателие|студио|творческо ателие|atelier)")
    
    # Valuation Flags
    GROUND_FLOOR = re.compile(r"(?i)(партер|етаж 1 от|висок партер|кота 0|сутерен|етаж 1 жилищен)")
    FUTURE_COMPLETION = re.compile(r"(?i)(\d{1,2}%.*?сега|\d{1,2}%.*?предварителен|\d{1,2}%.*?акт 16|въвеждане в експлоатация)")
    
    # Ownership Flags
    DIRECT_OWNER = re.compile(r"(?i)(частно лице|собственик|без комисион|директно от)")
    BROKER_EXCLUSION = re.compile(r"(?i)(само за частни|частни лица|агенции да не|без брокери|комисионна от купувача)")

    @classmethod
    def normalize_text(cls, text: str) -> str:
        """Standardize text to catch edge cases."""
        if not text: return ""
        return re.sub(r'\s+', ' ', text).strip().upper()

    @classmethod
    def extract_flags(cls, text: str) -> List[str]:
        flags = []
        if not text: return flags
        
        # We don't flag VAT here because we handle it mathematically in the Scraper
        if cls.SPACE_HACK.search(text):
            flags.append("CONVERSION_RISK")
        if cls.ATELIER_STATUTE.search(text):
            flags.append("ATELIER_DETECTED")
        if cls.GROUND_FLOOR.search(text):
            flags.append("GROUND_FLOOR_RISK")
        if cls.DIRECT_OWNER.search(text):
            flags.append("DIRECT_OWNER_LISTING")
        if cls.BROKER_EXCLUSION.search(text):
            flags.append("RESTRICTED_ACCESS_BROKER")
            
        return flags


=========================================
FILE: ./src/services/ai_engine.py
=========================================
import google.generativeai as genai
from typing import Dict, Any, List
import json
import time
from src.core.logger import logger
from src.core.config import settings

class GeminiService:
    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        # Use the config value (defaults to gemini-3.0-flash)
        self.model = genai.GenerativeModel(settings.GEMINI_MODEL)

    async def analyze_listing_multimodal(self, text_content: str, image_paths: List[str]) -> Dict[str, Any]:
        """
        Analyzes listing text + images to find discrepancies and specific visual risks.
        """
        logger.info(f"Analyzing listing with {settings.GEMINI_MODEL}...")
        
        # Construct the prompt for the Digital Attorney
        prompt = f"""
        You are an expert Real Estate Forensic Auditor in Sofia, Bulgaria.
        Analyze this listing text and the attached images.
        
        TEXT:
        {text_content}
        
        TASK:
        1. Extract the likely address (Street, Number, Neighborhood).
        2. Estimate construction year based on visual style (Panel vs Brick).
        3. Detect "Atelier" status traps (North facing, small windows).
        4. Identify "Visual Lies": Does the text say "Luxury" but images show "Panel"?
        
        Return JSON only:
        {{
            "address_prediction": "str",
            "construction_year_est": int,
            "is_panel_block": bool,
            "is_atelier_trap": bool,
            "visual_defects": ["str"],
            "landmarks": ["str"]
        }}
        """
        
        try:
            # Prepare parts: Text + Images
            parts = [prompt]
            
            # TODO: Append actual image data here in production
            
            response = self.model.generate_content(parts)
            cleaned_text = response.text.replace('', '')
            return json.loads(cleaned_text)
            
        except Exception as e:
            logger.error(f"AI Analysis Failed: {e}")
            return {
                "address_prediction": "Unknown",
                "error": str(e)
            }


=========================================
FILE: ./src/services/repository.py
=========================================
from decimal import Decimal
from sqlalchemy.orm import Session
from src.db.models import Listing, PriceHistory
from src.core.utils import normalize_url

class RealEstateRepository:
    def __init__(self, db: Session):
        self.db = db

    def create_listing_initial(self, url: str) -> Listing:
        clean_url = normalize_url(url)
        existing = self.db.query(Listing).filter(Listing.source_url == clean_url).first()
        if existing: return existing
        new_l = Listing(source_url=clean_url)
        self.db.add(new_l)
        self.db.commit()
        self.db.refresh(new_l)
        return new_l

    # TYPE SAFETY FIX: price is now Decimal
    def update_listing_data(self, listing_id: int, price: Decimal, area: float, desc: str, chash: str):
        listing = self.db.query(Listing).get(listing_id)
        if listing:
            # SQLAlchemy handles Decimal comparison correctly here
            if listing.price_bgn is not None and listing.price_bgn != price:
                history = PriceHistory(listing_id=listing_id, price_bgn=listing.price_bgn)
                self.db.add(history)
            
            listing.price_bgn = price
            listing.advertised_area_sqm = area
            listing.description_raw = desc
            listing.content_hash = chash
            self.db.commit()


=========================================
FILE: ./src/services/scraper_mvp.py
=========================================
from bs4 import BeautifulSoup
import os

# CONFIG
SIMULATION_MODE = True
MOCK_FILE = "imot_simulation.html"

def run_recon():
    print("[*] INTEL: Starting Reconnaissance Protocol...")
    
    html_content = ""
    
    if SIMULATION_MODE:
        print(f"[*] MODE: SIMULATION (Bypassing WAF)")
        if not os.path.exists(MOCK_FILE):
            print(f"[!] Error: Mock file {MOCK_FILE} not found.")
            return
            
        with open(MOCK_FILE, "r", encoding="utf-8") as f:
            html_content = f.read()
    else:
        # Network logic removed for Termux Safety
        pass

    soup = BeautifulSoup(html_content, 'html.parser')
    links = soup.find_all('a', href=True)
    
    print("[*] Parsing DOM Structure...")
    
    count = 0
    listings_found = []
    
    for link in links:
        href = link['href']
        
        if 'act=5' in href:
            # Normalize URL
            full_url = "https:" + href if href.startswith("//") else href
            
            if full_url in listings_found:
                continue
                
            listings_found.append(full_url)
            text_content = link.get_text(separator=" ", strip=True)
            
            count += 1
            print(f"\n[TARGET #{count}]")
            print(f"   URL: {full_url}")
            print(f"   RAW: {text_content}")

    print(f"\n[*] Mission Complete. {count} mock targets extracted.")

if __name__ == "__main__":
    run_recon()


=========================================
FILE: ./src/services/scraper_service.py
=========================================
import httpx
import re
import asyncio
from decimal import Decimal
from typing import List, Optional
from src.schemas import ScrapedListing
from src.core.logger import logger

class ScraperService:
    def __init__(self, client: httpx.AsyncClient, simulation_mode=False):
        self.client = client
        self.headers = {
            # Използваме Desktop User-Agent, за да получим HTML с таблици (Legacy Mode)
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Referer": "https://www.imot.bg/"
        }

    async def scrape_url(self, url: str) -> ScrapedListing:
        # 1. Конвертиране на Mobile URL към Desktop URL (ако е необходимо)
        # Mobile: https://m.imot.bg/pcgi/imot.cgi?act=5&adv=1c176587956641453
        # Desktop: https://www.imot.bg/pcgi/imot.cgi?act=5&adv=1c176587956641453
        target_url = url.replace("m.imot.bg", "www.imot.bg")
        
        log = logger.bind(url=target_url)
        
        try:
            resp = await self.client.get(target_url, headers=self.headers, follow_redirects=True)
            
            # 2. Декодиране (Критично за imot.bg)
            try:
                content = resp.content.decode('windows-1251')
            except UnicodeDecodeError:
                content = resp.content.decode('utf-8', errors='ignore')

            # 3. Проверка за WAF / Captcha
            if "captcha" in content.lower() or "security check" in content.lower():
                raise Exception("BLOCKED: Cloudflare Captcha detected")

            return await asyncio.to_thread(self._parse_html, content, target_url)
            
        except Exception as e:
            log.error("scrape_failed", error=str(e))
            raise e

    def _parse_html(self, content: str, url: str) -> ScrapedListing:
        # --- REGEX ARSENAL (Based on debug_regex_v2.py) ---
        
        # 1. Цена
        p_match = re.search(r'class="cena">\s*([\d\s]+)', content)
        price_decimal = Decimal(p_match.group(1).replace(" ", "")) if p_match else Decimal("0")

        # 2. Площ
        a_match = re.search(r'Площ:<br/><strong>\s*(\d+)', content)
        area = Decimal(a_match.group(1)) if a_match else Decimal("0")

        # 3. Квартал (Малко по-сложен regex за извличане от заглавието или локацията)
        # Търсим: "град Варна, Младост 1"
        loc_match = re.search(r'Местоположение: <b>(.*?)</b>', content)
        neighborhood = "Unknown"
        if loc_match:
            full_loc = loc_match.group(1)
            if "," in full_loc:
                neighborhood = full_loc.split(",")[-1].strip()
            else:
                neighborhood = full_loc

        # 4. Снимки (HD)
        # Търсим src=".../photosimotbg/..."
        raw_imgs = re.findall(r'(?:src|data-src|data-src-gallery)=["\'](https?://[^"\']*/photosimotbg/[^"\']+)["\']', content)
        images = list(set([i for i in raw_imgs if "nophoto" not in i]))

        # 5. VAT Logic (Forensics)
        is_vat_excluded = bool(re.search(r"(?i)(цената е без ддс|без ддс|vat excluded)", content))
        if is_vat_excluded:
            price_decimal = price_decimal * Decimal("1.20")

        # 6. Direct Owner Logic
        is_direct = bool(re.search(r"(?i)(частно лице|собственик|без комисион)", content))

        return ScrapedListing(
            source_url=url,
            raw_text=content[:5000], # Пазим само началото за дебъг
            price_predicted=price_decimal,
            area_sqm=area,
            neighborhood=neighborhood,
            image_urls=images,
            is_vat_excluded=is_vat_excluded,
            is_direct_owner=is_direct,
            price_correction_note="VAT Adjusted" if is_vat_excluded else None
        )


=========================================
FILE: ./src/services/risk_engine.py
=========================================
from typing import Dict, Any
import datetime
from src.core.patterns import ForensicPatterns

class RiskEngine:
    def calculate_score_v2(self, data: Dict) -> Dict[str, Any]:
        score = 0
        flags = []
        is_fatal = False
        
        scraped = data.get("scraped", {})
        ai = data.get("ai", {})
        cad = data.get("cadastre") or {}
        comp = data.get("compliance", {})
        risk = data.get("city_risk", {})
        geo = data.get("geo", {})
        
        raw_text = scraped.get("raw_text", "").upper()
        
        # 1. EXPROPRIATION (The Nuke)
        if risk.get("is_expropriated"):
            score = 100
            is_fatal = True
            flags.append("CRITICAL: Property is listed for EXPROPRIATION (Municipal Seizure).")

        # 2. LOCATION INTEGRITY
        if geo and not geo.get("match"):
            score += 40
            flags.append(geo.get("warning", "Location Fraud Detected."))

        # 3. CONSTRUCTION MATURITY (Time-Value Risk)
        # Check if the promised date is far in the future
        due_date_str = ai.get("act16_due_date")
        if due_date_str and len(due_date_str) >= 4:
            try:
                # Extract year
                due_year = int(due_date_str[:4])
                current_year = datetime.datetime.now().year
                if due_year > current_year + 1:
                    score += 25
                    flags.append(f"LIQUIDITY RISK: Act 16 promised for {due_year}. Asset is not currently habitable.")
            except ValueError:
                pass

        # 4. INFRASTRUCTURE MISMATCH (TEC/Radiator Logic)
        if "ТЕЦ" in raw_text or "ЦЕНТРАЛНО ОТОПЛЕНИЕ" in raw_text:
            inventory = ai.get("heating_inventory", {})
            if inventory.get("radiators", 0) == 0:
                score += 15
                flags.append("WARN: Listing claims Central Heating (TEC), but 0 radiators detected visually.")

        # 5. AREA FRAUD & DILUTION (The Terrace Trap)
        adv_area = float(scraped.get("area_sqm", 0))
        net_area = float(ai.get("net_area_sqm", 0))
        
        # A. Check against Cadastre (Official)
        off_area = float(cad.get("official_area", 0))
        if adv_area > 0 and off_area > 0:
            diff_ratio = (adv_area - off_area) / off_area
            if diff_ratio > 0.25:
                score += 30
                flags.append(f"SCAM: Advertised area {adv_area}m is {diff_ratio:.1%} larger than Official {off_area}m.")
        
        # B. Check Net vs Gross (Terrace Dilution)
        if adv_area > 0 and net_area > 0:
            efficiency_ratio = net_area / adv_area
            if efficiency_ratio < 0.60: # If living area is less than 60% of total
                score += 20
                flags.append(f"VALUATION WARNING: 'Terrace Dilution'. Only {efficiency_ratio:.0%} of the asset is living space ({net_area}m).")

        # 6. ATELIER STATUS
        if ai.get("is_atelier") or "АТЕЛИЕ" in raw_text:
            score += 25
            flags.append("LEGAL: Non-residential 'Atelier' status confirmed.")

        # 7. GROUND FLOOR PENALTY
        if ForensicPatterns.GROUND_FLOOR.search(raw_text):
            score += 10 # Not fatal, but decreases value
            flags.append("VALUATION: Ground floor unit (Security/Privacy/Sewage risk).")
            
        # 8. VAT ADJUSTMENT NOTE
        if scraped.get("is_vat_excluded"):
            flags.append(f"FINANCIAL: Price adjusted +20% for VAT ({scraped.get('price_correction_note')}).")

        final_score = 100 if is_fatal else min(score, 100)
        return {"score": final_score, "flags": flags, "is_fatal": is_fatal}


=========================================
FILE: ./src/services/cadastre_service.py
=========================================


=========================================
FILE: ./src/services/geospatial_service.py
=========================================
import httpx
from src.core.logger import logger
from src.schemas import GeoVerification
from typing import Optional

class GeospatialService:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://maps.googleapis.com/maps/api/geocode/json"

    async def verify_neighborhood(self, ai_prediction: str, ai_landmarks: list, claimed_kvartal: str) -> GeoVerification:
        if self.api_key == "mock-key":
            return GeoVerification(match=True, detected_neighborhood="Mock", confidence=100)

        # Build a search query prioritizing specific clues from Gemini
        search_query = f"{ai_prediction} {' '.join(ai_landmarks)}, Sofia, Bulgaria"
        
        async with httpx.AsyncClient() as client:
            resp = await client.get(self.base_url, params={"address": search_query, "key": self.api_key})
            data = resp.json()

            if data["status"] != "OK" or not data["results"]:
                return GeoVerification(match=True, detected_neighborhood="Not Found", confidence=0)

            result = data["results"][0]
            lat_lng = result["geometry"]["location"]
            formatted_address = result.get("formatted_address", "")
            
            # Extract neighborhood from Google components
            detected = ""
            for comp in result["address_components"]:
                if any(t in comp["types"] for t in ["sublocality", "neighborhood", "political"]):
                    detected = comp["long_name"]
                    break
            
            # Cross-reference
            claimed_norm = claimed_kvartal.lower().replace("гр.", "").strip()
            detected_norm = detected.lower().strip()
            
            # Match if strings overlap (e.g., "Krastova Vada" vs "Manastirski Livadi - East")
            is_match = claimed_norm in detected_norm or detected_norm in claimed_norm
            
            return GeoVerification(
                match=is_match,
                detected_neighborhood=detected,
                confidence=90,
                lat=lat_lng["lat"],
                lng=lat_lng["lng"],
                warning=None if is_match else f"LOCATION FRAUD: Ad claims {claimed_kvartal}, but Vision/Maps identifies {detected}.",
                best_address=formatted_address
            )


=========================================
FILE: ./src/services/base_provider.py
=========================================
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any

class BaseRegistryProvider(ABC):
    @abstractmethod
    def fetch_details(self, address: str) -> Optional[Dict[str, Any]]:
        """Fetch official property data from a government registry."""
        pass

class BaseGeoProvider(ABC):
    @abstractmethod
    def geocode(self, address: str) -> Dict[str, Any]:
        """Convert address to GPS coordinates."""
        pass


=========================================
FILE: ./src/services/storage_service.py
=========================================
import os
import httpx
import asyncio
from typing import List

class StorageService:
    def __init__(self, upload_dir="storage/archive"):
        self.upload_dir = upload_dir
        os.makedirs(upload_dir, exist_ok=True)

    async def _download_single(self, client: httpx.AsyncClient, url: str, filename: str) -> str:
        try:
            resp = await client.get(url, timeout=7.0)
            if resp.status_code == 200:
                path = os.path.join(self.upload_dir, filename)
                with open(path, "wb") as f:
                    f.write(resp.content)
                return path
        except Exception as e:
            print(f"Archive Fail: {url} -> {e}")
        return None

    async def archive_images(self, listing_id: int, urls: List[str]) -> List[str]:
        if not urls: return []
        async with httpx.AsyncClient() as client:
            tasks = []
            for i, url in enumerate(urls):
                filename = f"listing_{listing_id}_{i}.jpg"
                tasks.append(self._download_single(client, url, filename))
            results = await asyncio.gather(*tasks)
            return [r for r in results if r is not None]


=========================================
FILE: ./src/services/legal_engine.py
=========================================
import os
import re
from typing import Optional, List
from src.core.patterns import ForensicPatterns

class LegalKnowledgeBase:
    """Search engine for the local law database."""
    def __init__(self, laws_path: str = "storage/laws"):
        self.laws_path = laws_path

    def get_article(self, law_name: str, article_num: int) -> Optional[str]:
        """Extracts a specific Article text from the law file."""
        file_path = os.path.join(self.laws_path, f"{law_name}.txt")
        if not os.path.exists(file_path):
            return None
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        
        # Matches 'Чл. [num].' until the next Article or end of file
        pattern = rf"(Чл\. {article_num}\..*?)(?=\nЧл\. \d+|\Z)"
        match = re.search(pattern, content, re.DOTALL)
        return match.group(1).strip() if match else None

    def search_context(self, keyword: str, limit: int = 1) -> List[str]:
        """Finds sentences containing specific legal terms."""
        results = []
        if not os.path.exists(self.laws_path): return results
        for law_file in os.listdir(self.laws_path):
            if not law_file.endswith(".txt"): continue
            with open(os.path.join(self.laws_path, law_file), "r", encoding="utf-8") as f:
                content = f.read()
                matches = re.findall(rf"([^.\n]*{keyword}[^.\n]*)", content, re.IGNORECASE)
                for m in matches[:limit]:
                    results.append(f"{m.strip()} (Ref: {law_file})")
        return results

# Initialize the Knowledge Base Instance
kb = LegalKnowledgeBase()

class LegalEngine:
    def analyze_listing(self, scraped_data: dict, ai_data: dict):
        risk_report = {
            "total_legal_score": 0,
            "pillars": {},
            "gatekeeper_verdict": "CLEAR",
            "flags": []
        }
        raw_text = scraped_data.get("raw_text", "").upper()
        regex_flags = ForensicPatterns.extract_flags(raw_text)
        risk_report["flags"].extend(regex_flags)
        
        p1_score = 0
        if ai_data.get("is_atelier") or "АТЕЛИЕ" in raw_text:
            p1_score = 35
            risk_report["flags"].append("LEGAL: Non-residential status (Atelier).")
        
        risk_report["pillars"]["classification"] = p1_score
        risk_report["total_legal_score"] = p1_score
        return risk_report


=========================================
FILE: ./src/services/report_generator.py
=========================================
import datetime
from src.services.legal_engine import kb

class AttorneyReportGenerator:
    def generate_legal_brief(self, listing_data: dict, risk_data: dict, ai_data: dict) -> str:
        score = risk_data.get("score", 0)
        flags = risk_data.get("flags", [])
        forensics = risk_data.get("forensics", {})
        
        status_symbol = "🟢 CLEAR"
        if risk_data.get("is_fatal"): status_symbol = "🔴 DO NOT PROCEED"
        elif score > 60: status_symbol = "🟠 HIGH RISK ASSET"
        elif score > 30: status_symbol = "🟡 CAUTION ADVISED"

        sections = [
            f"# {status_symbol} (Risk Score: {score}/100)",
            f"**Generated:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}",
            "\n## I. Executive Summary",
            self._summary(risk_data),
            "\n## II. Forensic Evidence (Visual)",
            self._visual_forensics(ai_data, forensics.get("geo", {})),
            "\n## III. Statutory Analysis (Bulgarian Law)",
            self._legal_citations(ai_data, listing_data),
            "\n## IV. Cadastral & Registry Data",
            self._cadastre_section(listing_data, forensics.get("cadastre", {})),
            "\n## V. Identified Risk Factors",
            "\n".join([f"- {f}" for f in flags]) if flags else "- No specific flags raised."
        ]
        return "\n".join(sections)

    def _summary(self, risk):
        if risk.get("is_fatal"): return "CRITICAL: The property has fatal legal or registry defects."
        return "Manual verification against Lex.bg records and Registry IDs completed."

    def _visual_forensics(self, ai, geo):
        return f"- **Location Match:** {'✅ OK' if geo.get('match') else '❌ MISMATCH'}\n" \
               f"- **View Check:** {ai.get('neighborhood_match')}\n" \
               f"- **Building Type:** {ai.get('building_type')}\n" \
               f"- **Infrastructure:** {ai.get('heating_inventory', {}).get('ac_units')} AC units detected."

    def _legal_citations(self, ai, scraped):
        citations = []
        if ai.get("is_atelier"):
            text = kb.get_article("naredba_7", 110)
            citations.append(f"**Classification: Atelier detected.**")
            if text: citations.append(f"Citing Ordinance No. 7: {text[:250]}...")
        
        if "площ" in scraped.get("raw_text", "").lower():
            ref = kb.search_context("Застроена площ")
            if ref: citations.append(f"**Area Standard:** {ref[0]}")
            
        return "\n".join(citations) if citations else "No significant statutory discrepancies found."

    def _cadastre_section(self, listing, cad):
        off = cad.get("official_area", 0)
        adv = listing.get("area_sqm", 0)
        return f"- **Official Area:** {off} m2\n- **Advertised Area:** {adv} m2\n- **Cadastre ID:** {cad.get('cadastre_id')}"


=========================================
FILE: ./src/services/compliance_service.py
=========================================
import httpx
import uuid
from src.core.logger import logger

class ComplianceService:
    BASE_URL = 'https://nag.sofia.bg/RegisterCertificateForExploitationBuildings'
    
    def __init__(self, client: httpx.AsyncClient = None):
        self.client = client

    async def check_act_16(self, cadastre_id: str) -> dict:
        log = logger.bind(cadastre_id=cadastre_id)
        if not cadastre_id: 
            return {"has_act16": False, "checked": False}

        # Use injected client (preferred) or create ephemeral one
        # REMOVED verify=False. Production requires valid SSL or mounted CAs.
        local_client = self.client if self.client else httpx.AsyncClient(timeout=10.0)
        
        try:
            search_params = {'searchQueryId': str(uuid.uuid4()), 'Identifier': cadastre_id}
            
            # 1. Search
            await local_client.get(f"{self.BASE_URL}/Search", params=search_params)
            
            # 2. Read
            res = await local_client.post(f"{self.BASE_URL}/Read", data={'page': '1', 'pageSize': '10'})
            res.raise_for_status()
            data = res.json()
            
            has_cert = len(data.get("Data", [])) > 0
            log.info("compliance_check", found=has_cert)
            
            return {
                "has_act16": has_cert, 
                "checked": True,
                "registry_status": "LIVE"
            }

        except (httpx.ConnectError, httpx.TimeoutException, httpx.HTTPStatusError) as e:
            log.warning("registry_down", error=str(e))
            return {"has_act16": False, "checked": False, "registry_status": "OFFLINE"}
            
        finally:
            # Only close if we created it locally
            if not self.client:
                await local_client.aclose()


=========================================
FILE: ./src/services/city_risk_service.py
=========================================
import httpx
from src.core.logger import logger

class CityRiskService:
    BASE_URL = 'https://nag.sofia.bg/RegisterExpropriation'
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'X-Requested-With': 'XMLHttpRequest',
        'Origin': 'https://nag.sofia.bg'
    }

    def __init__(self, client: httpx.AsyncClient = None):
        self.client = client

    async def check_expropriation(self, cadastre_id: str, district: str = "") -> dict:
        log = logger.bind(cadastre_id=cadastre_id)
        if not cadastre_id: return {"is_expropriated": False}

        local_client = self.client if self.client else httpx.AsyncClient(headers=self.HEADERS, timeout=10.0)

        try:
            # 1. Set Context
            await local_client.post(f"{self.BASE_URL}/Search", data={'CadNumber': cadastre_id, 'RegionName': district})
            
            # 2. Fetch
            res = await local_client.post(f"{self.BASE_URL}/Read", data={'page': '1', 'pageSize': '10'})
            res.raise_for_status()
            data = res.json()

            is_risk = data and data.get("Data") and len(data["Data"]) > 0
            
            if is_risk:
                log.critical("expropriation_risk_found", details=str(data["Data"][0]))
                return {
                    "is_expropriated": True, 
                    "details": str(data["Data"][0]),
                    "risk_level": "CRITICAL",
                    "registry_status": "LIVE"
                }
            
            return {"is_expropriated": False, "registry_status": "LIVE"}

        except (httpx.ConnectError, httpx.TimeoutException) as e:
            log.warning("registry_down", error=str(e))
            return {"is_expropriated": False, "registry_status": "OFFLINE"}
        finally:
            if not self.client:
                await local_client.aclose()


=========================================
FILE: ./src/services/forensics_service.py
=========================================
import httpx
import uuid
import time
import asyncio
from src.core.logger import logger
from typing import Optional, Dict, Any

class SofiaMunicipalForensics:
    """
    Direct interface to Sofia Municipality (NAG) Registries.
    Implements the "Nuclear Option" (Expropriation) and "Green Light" (Act 16) checks.
    """
    
    BASE_URL = "https://nag.sofia.bg"
    
    # Headers exactly as captured in curl (User-Agent + X-Requested-With are critical)
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:141.0) Gecko/20100101 Firefox/141.0',
        'Accept': '*/*',
        'Accept-Language': 'en-US,en;q=0.5',
        'X-Requested-With': 'XMLHttpRequest',
        'Connection': 'keep-alive',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-origin',
    }

    def __init__(self, client: httpx.AsyncClient = None):
        self.client = client

    async def run_full_audit(self, cadastre_id: str) -> Dict[str, Any]:
        """
        Runs the 3-Dimensional Audit:
        1. Expropriation (The "Death List")
        2. Act 16 (The "Green List")
        3. Permits (The "Construction History")
        """
        if not cadastre_id:
            return {"error": "No Cadastre ID provided"}

        # Use injected client or create a new ephemeral one
        local_client = self.client if self.client else httpx.AsyncClient(headers=self.HEADERS, timeout=20.0, follow_redirects=True)

        try:
            # Run all 3 checks in parallel for speed
            expropriation, act16, permits = await asyncio.gather(
                self._check_expropriation(local_client, cadastre_id),
                self._check_act16(local_client, cadastre_id),
                self._check_permits(local_client, cadastre_id)
            )

            return {
                "expropriation": expropriation,
                "compliance_act16": act16,
                "building_permits": permits,
                "audit_timestamp": time.time()
            }
        except Exception as e:
            logger.error(f"audit_failed_fatal: {e}")
            return {"error": str(e)}
        finally:
            if not self.client:
                await local_client.aclose()

    async def _check_expropriation(self, client, cid: str) -> Dict:
        """
        Checks 'RegisterExpropriation' (The Death List).
        """
        # 1. SEARCH: Sets the server-side session state
        search_id = str(uuid.uuid4())
        params = {
            'searchQueryId': search_id,
            'RegionName': '', 'RegPlan': '', 'Quarter': '', 'Upi': '',
            'CadNumber': cid,  # <--- TARGET
            'Area': '', 'Obj': '', 'GroupType': '', 'Typology': '', 'StructureZone': '',
            'ApprYear': '', 'X-Requested-With': 'XMLHttpRequest',
            '_': int(time.time() * 1000)
        }
        
        try:
            # Step A: Prime the search
            await client.get(f"{self.BASE_URL}/RegisterExpropriation/Search", params=params)
            
            # Step B: Read the results (Page 1)
            # The server expects a POST to /Read after the GET /Search
            res = await client.post(f"{self.BASE_URL}/RegisterExpropriation/Read", data={'page': 1, 'pageSize': 10})
            data = res.json()
            
            # SAFEGUARD: Ensure hits is a list, even if API returns null
            hits = data.get("Data") or []
            is_fatal = len(hits) > 0
            
            return {
                "is_expropriated": is_fatal,
                "risk_level": "CRITICAL" if is_fatal else "NONE",
                "details": hits[0] if is_fatal else None,
                "found_count": len(hits)
            }
        except Exception as e:
            logger.error(f"expropriation_check_failed: {e}")
            return {"error": str(e), "is_expropriated": False}

    async def _check_act16(self, client, cid: str) -> Dict:
        """
        Checks 'RegisterCertificateForExploitationBuildings' (The Green List).
        """
        search_id = str(uuid.uuid4())
        params = {
            'searchQueryId': search_id,
            'IssuedById': '', 'FromDate': '', 'ToDate': '', 'StatusId': '',
            'DocumentTypeName': '', 'Number': '', 'Status': '', 'Issuer': '',
            'Employer': '', 'ConstructionalOversightName': '', 'Object': '',
            'Region': '', 'Terrain': '', 'RegulationNeighbourhood': '', 'Upi': '',
            'Identifier': cid, # <--- TARGET
            'Address': '', 'RegionId': '', 'TakeEffectFilter': '', 
            'MapOfRestoredProperty': '', 'AdditionalDescriptionEstate': '',
            'AdditionalDescriptionAdministrativeAddress': '', 'Scope': '',
            'X-Requested-With': 'XMLHttpRequest',
            '_': int(time.time() * 1000)
        }

        try:
            await client.get(f"{self.BASE_URL}/RegisterCertificateForExploitationBuildings/Search", params=params)
            res = await client.post(f"{self.BASE_URL}/RegisterCertificateForExploitationBuildings/Read", data={'page': 1, 'pageSize': 10})
            data = res.json()
            
            # SAFEGUARD: Use 'or []' to handle None
            hits = data.get("Data") or []
            has_cert = len(hits) > 0
            
            # Extract description if found
            cert_desc = hits[0].get("Строеж/Обект", "N/A") if has_cert else None
            
            return {
                "has_act16": has_cert,
                "description": cert_desc,
                "raw_hits": len(hits)
            }
        except Exception as e:
            logger.error(f"act16_check_failed: {e}")
            return {"error": str(e), "has_act16": False}

    async def _check_permits(self, client, cid: str) -> Dict:
        """
        Checks 'RegisterBuildingPermitsPortal' (Construction History).
        """
        search_id = str(uuid.uuid4())
        params = {
            'searchQueryId': search_id,
            'IssuedById': '', 'FromDate': '', 'ToDate': '', 'TakeEffectFrom': '',
            'TakeEffectTo': '', 'StatusId': '', 'DocumentTypeName': '', 'Number': '',
            'Status': '', 'Issuer': '', 'Employer': '', 'ConstructionalOversightName': '',
            'Object': '', 'Region': '', 'Terrain': '', 'RegulationNeighbourhood': '',
            'Upi': '', 
            'Identifier': cid, # <--- TARGET
            'Address': '', 'RegionId': '', 'TakeEffectFilter': '', 
            'MapOfRestoredProperty': '', 'AdditionalDescriptionEstate': '',
            'AdditionalDescriptionAdministrativeAddress': '', 'Scope': '',
            'X-Requested-With': 'XMLHttpRequest',
            '_': int(time.time() * 1000)
        }

        try:
            await client.get(f"{self.BASE_URL}/RegisterBuildingPermitsPortal/Search", params=params)
            res = await client.post(f"{self.BASE_URL}/RegisterBuildingPermitsPortal/Read", data={'page': 1, 'pageSize': 10})
            data = res.json()
            
            return {
                "total_permits": data.get("Total", 0),
                "latest_permit": data.get("Data", [])[0] if data.get("Data") else None
            }
        except Exception as e:
            return {"error": str(e), "total_permits": 0}


=========================================
FILE: ./src/__init__.py
=========================================


=========================================
FILE: ./src/main.py
=========================================
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from src.api import routes

app = FastAPI(
    title="Glashaus API",
    description="Automated Real Estate Due Diligence Engine",
    version="1.0.0"
)

# Allow connections from Frontend/Dashboard
origins = [
    "http://localhost",
    "http://localhost:3000",
    "http://localhost:8080",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(routes.router)

@app.get("/")
def health_check():
    return {
        "system": "GLASHAUS", 
        "status": "OPERATIONAL", 
        "version": "1.0.0-PROD"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("src.main:app", host="0.0.0.0", port=8000)


=========================================
FILE: ./src/db/session.py
=========================================
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
from src.core.config import settings

# SQLite requires a specific argument for threading
connect_args = {"check_same_thread": False} if "sqlite" in settings.DATABASE_URL else {}

engine = create_engine(
    settings.DATABASE_URL, 
    connect_args=connect_args,
    pool_pre_ping=True
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


=========================================
FILE: ./src/db/models.py
=========================================
import enum
from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, ForeignKey, JSON, Text, Enum, Numeric
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from src.db.session import Base

class ReportStatus(str, enum.Enum):
    PENDING = "PENDING"
    PROCESSING = "PROCESSING"
    VERIFIED = "VERIFIED"
    MANUAL_REVIEW = "MANUAL_REVIEW"
    REJECTED = "REJECTED"

class Listing(Base):
    __tablename__ = "listings"
    id = Column(Integer, primary_key=True, index=True)
    source_url = Column(String, unique=True, nullable=False, index=True)
    content_hash = Column(String(64), index=True)
    
    # Financial Precision
    price_bgn = Column(Numeric(12, 2)) 
    
    # Area Precision (Fixed from Float)
    advertised_area_sqm = Column(Numeric(10, 2))
    
    description_raw = Column(Text)
    scraped_at = Column(DateTime(timezone=True), server_default=func.now())
    
    reports = relationship("Report", back_populates="listing", cascade="all, delete-orphan")
    price_history = relationship("PriceHistory", back_populates="listing")

class Building(Base):
    __tablename__ = "buildings"
    id = Column(Integer, primary_key=True)
    cadastre_id = Column(String, unique=True, index=True)
    address_full = Column(String)
    latitude = Column(Float)
    longitude = Column(Float)
    construction_year = Column(Integer)
    reports = relationship("Report", back_populates="building")

class PriceHistory(Base):
    __tablename__ = "price_history"
    id = Column(Integer, primary_key=True)
    listing_id = Column(Integer, ForeignKey("listings.id"))
    price_bgn = Column(Numeric(12, 2))
    changed_at = Column(DateTime(timezone=True), server_default=func.now())
    listing = relationship("Listing", back_populates="price_history")

class Report(Base):
    __tablename__ = "reports"
    id = Column(Integer, primary_key=True, index=True)
    listing_id = Column(Integer, ForeignKey("listings.id"))
    building_id = Column(Integer, ForeignKey("buildings.id"), nullable=True)
    status = Column(Enum(ReportStatus), default=ReportStatus.PENDING)
    risk_score = Column(Integer)
    ai_confidence_score = Column(Integer, default=0)
    legal_brief = Column(Text)
    discrepancy_details = Column(JSON)
    image_archive_urls = Column(JSON)
    
    cost_to_generate = Column(Numeric(10, 4)) 
    
    manual_review_notes = Column(Text, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    listing = relationship("Listing", back_populates="reports")
    building = relationship("Building", back_populates="reports")


=========================================
FILE: ./src/worker.py
=========================================
import os
from celery import Celery
from src.core.config import settings

celery_app = Celery(
    "glashaus_worker",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL
)

celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="Europe/Sofia",
    enable_utc=True,
)

# Auto-discover tasks in src/tasks.py
celery_app.autodiscover_tasks(['src.tasks'])


=========================================
FILE: ./src/tasks.py
=========================================
from asgiref.sync import async_to_sync
import asyncio
import httpx
from src.worker import celery_app
from src.db.session import SessionLocal
from src.db.models import Listing, Report, ReportStatus, Building
from src.services.scraper_service import ScraperService
from src.services.ai_engine import GeminiService
from src.services.storage_service import StorageService
from src.services.geospatial_service import GeospatialService
from src.services.cadastre_service import CadastreService
from src.services.forensics_service import SofiaMunicipalForensics
from src.services.risk_engine import RiskEngine
from src.services.report_generator import AttorneyReportGenerator
from src.core.config import settings
from src.core.logger import logger
from src.core.utils import normalize_sofia_street

@celery_app.task(name="src.tasks.audit_listing")
def audit_listing_task(listing_id: int):
    return async_to_sync(run_audit_pipeline)(listing_id)

async def run_audit_pipeline(listing_id: int):
    log = logger.bind(listing_id=listing_id)
    
    async with httpx.AsyncClient(timeout=30.0) as http_client:
        with SessionLocal() as db:
            listing = db.query(Listing).get(listing_id)
            if not listing: return "Error: Listing not found"

            # 1. SCRAPE
            scraper = ScraperService(client=http_client)
            scraped_data = await scraper.scrape_url(listing.source_url)
            
            # 2. VISION
            storage = StorageService()
            img_paths = await storage.archive_images(listing_id, scraped_data.image_urls)
            ai_service = GeminiService(api_key=settings.GEMINI_API_KEY)
            ai_data = await ai_service.analyze_listing_multimodal(scraped_data.raw_text, img_paths)
            
            # 3. GEO TRIANGULATION
            geo_service = GeospatialService(api_key=settings.GOOGLE_MAPS_API_KEY)
            geo_report = await geo_service.verify_neighborhood(
                ai_data.get("address_prediction"), 
                ai_data.get("landmarks", []), 
                scraped_data.neighborhood
            )
            
            # 4. REGISTRY (LINEAR)
            cadastre = CadastreService(client=http_client)
            best_address = normalize_sofia_street(geo_report.best_address or ai_data.get("address_prediction"))
            cad_data = await cadastre.get_official_details(best_address)
            
            # NEW: UNIFIED MUNICIPAL AUDIT
            mun_report = {"expropriation": {}, "compliance_act16": {}}
            building_id = None
            
            if cad_data.cadastre_id:
                # Parallel Municipal Strike
                forensics = SofiaMunicipalForensics(client=http_client)
                mun_report = await forensics.run_full_audit(cad_data.cadastre_id)
                
                # Persist Building Context
                existing_building = db.query(Building).filter(Building.cadastre_id == cad_data.cadastre_id).first()
                if not existing_building:
                    new_building = Building(
                        cadastre_id=cad_data.cadastre_id,
                        address_full=cad_data.address_found or geo_report.best_address,
                        latitude=geo_report.lat,
                        longitude=geo_report.lng,
                        construction_year=ai_data.get("construction_year_est")
                    )
                    db.add(new_building)
                    db.flush()
                    building_id = new_building.id
                else:
                    building_id = existing_building.id

            # 5. SCORING
            forensic_data = {
                "scraped": scraped_data.model_dump(),
                "ai": ai_data,
                "geo": geo_report.model_dump(),
                "cadastre": cad_data.model_dump(),
                # Map new service output to Risk Engine expected keys
                "compliance": mun_report.get("compliance_act16", {}),
                "city_risk": mun_report.get("expropriation", {})
            }
            risk_engine = RiskEngine()
            score_res = risk_engine.calculate_score_v2(forensic_data)
            
            # 6. REPORTING
            report_gen = AttorneyReportGenerator()
            report_text = report_gen.generate_legal_brief(scraped_data.model_dump(), {**score_res, "forensics": forensic_data}, ai_data)
            
            new_report = Report(
                listing_id=listing_id,
                building_id=building_id,
                risk_score=score_res["score"],
                legal_brief=report_text,
                discrepancy_details=forensic_data,
                status=ReportStatus.VERIFIED if score_res["score"] < 40 else ReportStatus.MANUAL_REVIEW
            )
            db.add(new_report)
            db.commit()
            
            return f"Audit Done: {score_res['score']}"


=========================================
FILE: ./src/schemas.py
=========================================
from dataclasses import dataclass, field
from decimal import Decimal
from typing import List, Optional, Literal

@dataclass
class ScrapedListing:
    source_url: str
    raw_text: str
    price_predicted: Decimal
    area_sqm: Decimal
    neighborhood: str 
    image_urls: List[str] = field(default_factory=list)
    
    # Forensic Fields
    is_vat_excluded: bool = False
    is_direct_owner: bool = False
    price_correction_note: Optional[str] = None
    
    # Helper to mimic Pydantic's .model_dump()
    def model_dump(self):
        return {
            k: (float(v) if isinstance(v, Decimal) else v) 
            for k, v in self.__dict__.items()
        }

@dataclass
class HeatingInventory:
    ac_units: int = 0
    radiators: int = 0
    has_central_heating: bool = False

@dataclass
class AIAnalysisResult:
    address_prediction: str
    neighborhood_match: str
    building_type: str
    heating_inventory: HeatingInventory
    light_exposure: str
    landmarks: List[str] = field(default_factory=list)
    visual_red_flags: List[str] = field(default_factory=list)
    confidence_score: int = 0
    
    # Valuation Fields
    net_area_sqm: float = 0.0
    act16_due_date: Optional[str] = None
    is_panel_block: bool = False
    
    def get(self, key, default=None):
        return getattr(self, key, default)

@dataclass
class GeoVerification:
    match: bool
    detected_neighborhood: str
    confidence: int
    lat: Optional[float] = None
    lng: Optional[float] = None
    warning: Optional[str] = None
    best_address: Optional[str] = None
    
    def model_dump(self):
        return self.__dict__

@dataclass
class CadastreData:
    official_area: float = 0.0
    cadastre_id: Optional[str] = None
    status: Literal["LIVE", "OFFLINE", "NOT_FOUND", "ERROR"] = "NOT_FOUND"
    address_found: Optional[str] = None
    
    def model_dump(self):
        return self.__dict__


=========================================
FILE: ./scripts/context_dump.sh
=========================================
#!/bin/bash
# Scans the repo and dumps text files for LLM context

output="glashaus_context.txt"
echo "--- GLASHAUS PROJECT DUMP ---" > "$output"
date >> "$output"

echo -e "\n\n--- GIT HISTORY ---" >> "$output"
git log --oneline --graph --decorate -n 20 >> "$output"

echo -e "\n\n--- FILE STRUCTURE ---" >> "$output"
# Exclude git, pycache, and compiled python files from tree view
tree -L 3 -I '.git|__pycache__|*.pyc' >> "$output" 2>/dev/null || find . -maxdepth 3 -not -path '*/.*' >> "$output"

echo -e "\n\n--- FILE CONTENTS ---" >> "$output"
find . -type f \
    -not -path '*/.*' \
    -not -path '*/__pycache__*' \
    -not -name '*.pyc' \
    -not -name 'cookies.txt' \
    -not -path './glashaus_context.txt' \
    -not -name 'bypass_audit.py' \
    -not -name 'manual_session_audit.py' \
    -not -path './scraper_service.py' \
    -not -name '*.png' \
    -not -name '*.jpg' \
    -not -name '*.sqlite' \
    | while read -r file; do
    echo -e "\n\n=========================================" >> "$output"
    echo "FILE: $file" >> "$output"
    echo "=========================================" >> "$output"
    cat "$file" >> "$output"
done

echo "Dump complete. Clean contents saved to $output"


=========================================
FILE: ./scripts/scrape_lex.py
=========================================
import httpx
from bs4 import BeautifulSoup
import os
import re
import sys

def scrape_law(url: str, filename: str):
    print(f"[*] Targeting Lex.bg: {url}")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 Mobile Safari/604.1"
    }

    try:
        # 1. Fetch raw bytes to handle encoding manually
        with httpx.Client(headers=headers, timeout=60.0) as client:
            resp = client.get(url)
            resp.raise_for_status()
            
            # Decode using windows-1251 as identified in our curl test
            content = resp.content.decode('windows-1251', errors='replace')
            
        # 2. Parse HTML
        soup = BeautifulSoup(content, 'html.parser')
        
        # 3. Clean up the DOM (Remove ads, menus, scripts)
        # We saw these in your 'cnt' div test
        for noise in soup(["script", "style", "iframe", "ins"]):
            noise.decompose()
            
        ad_patterns = [r"adocean", r"mobileMenu", r"tma-ad", r"h-cnt"]
        for div in soup.find_all("div"):
            div_id = div.get("id", "")
            div_class = " ".join(div.get("class", []))
            if any(re.search(p, div_id) or re.search(p, div_class) for p in ad_patterns):
                div.decompose()

        # 4. Target the Law Body
        # Based on your grep, 'cnt' is the wrapper and 'boxi' is the inner content
        law_body = soup.find("div", class_="boxi") or soup.find("div", class_="cnt")
        
        if not law_body:
            print("[!] Could not find law container. Saving whole page.")
            law_body = soup.body

        # 5. Extract text with formatting
        # We use a separator to keep Articles (Чл.) on new lines
        raw_text = law_body.get_text(separator="\n")
        
        # 6. Post-Processing Cleanup
        # Remove massive gaps of whitespace
        clean_text = re.sub(r'\n\s*\n', '\n\n', raw_text)
        # Standardize the 'Article' prefix for easier RAG searching later
        clean_text = re.sub(r'(?m)^Чл\.', '\nЧл.', clean_text)

        # 7. Save to disk
        os.makedirs("storage/laws", exist_ok=True)
        path = f"storage/laws/{filename}.txt"
        
        with open(path, "w", encoding="utf-8") as f:
            f.write(f"SOURCE: {url}\n")
            f.write("-" * 30 + "\n\n")
            f.write(clean_text)
            
        print(f"[SUCCESS] {filename}.txt created ({len(clean_text)} characters)")

    except Exception as e:
        print(f"[FATAL] Error during scraping: {str(e)}")

if __name__ == "__main__":
    # Link 1: Ordinance No. 7 (Crucial for Atelier vs Apartment height/light rules)
    ORD_7_URL = "https://lex.bg/mobile/ldoc/2135476546"
    scrape_law(ORD_7_URL, "naredba_7")

    # Link 2: ZUT (The Foundation)
    ZUT_URL = "https://lex.bg/mobile/ldoc/2135163904"
    scrape_law(ZUT_URL, "zut_law")



=========================================
FILE: ./scripts/debug_kais.py
=========================================
import requests
from bs4 import BeautifulSoup
import time
import re
import argparse
import sys
import json

# CONFIG
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
BASE_URL = "https://kais.cadastre.bg/bg/Map"

def run_forensics(args):
    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "X-Requested-With": "XMLHttpRequest"
    })

    print("[1] HANDSHAKE...")
    try:
        resp = session.get(BASE_URL)
        soup = BeautifulSoup(resp.text, 'html.parser')
        token_el = soup.find('input', {'name': '__RequestVerificationToken'})
        if not token_el:
            print("FAILED: No token found. KAIS might be blocking.")
            return
        token = token_el.get('value')
    except Exception as e:
        print(f"Handshake Failed: {e}")
        return

    # --- QUERY CONSTRUCTION ---
    if args.query:
        search_kw = args.query
    else:
        # Construct "Google-style" query for FastSearch
        parts = []
        if args.district: parts.append(args.district)
        if args.street:   parts.append(args.street)
        if args.block:    parts.append(f"бл {args.block}")
        if args.number:   parts.append(args.number)
        if args.entrance: parts.append(f"вх {args.entrance}")
        search_kw = " ".join(parts)

    print(f"[2] TARGETING: '{search_kw}'...")
    
    # Always use FastSearch (more reliable than Detailed)
    session.get(f"{BASE_URL}/FastSearch", params={"KeyWords": search_kw})
    time.sleep(0.5)

    print("[3] FETCHING LIST...")
    headers = {"X-CSRF-TOKEN": token, "Referer": BASE_URL}

    # Fetch enough results to filter client-side
    resp = session.post(
        f"{BASE_URL}/ReadFoundObjects",
        data={"page": 1, "pageSize": args.limit * 5}, 
        headers=headers
    )

    data = resp.json()
    all_objects = data.get("Data", [])
    print(f"    -> Hits: {len(all_objects)}")

    # --- INTELLIGENT FILTERING ---
    filtered_objects = []
    for obj in all_objects:
        raw_display = f"{obj.get('DisplayText', '')} {obj.get('Address', '')}".lower()
        cad_num = obj.get('Number', '')
        
        # Filter: Exclude Apartments if requested (finding plots/buildings only)
        if args.plots_only:
             # PI (2 dots) or Building (3 dots). Apartments have 4+.
            if cad_num.count('.') > 3: continue 

        match = True
        
        # Filter: Strict Number Match
        if args.number and not args.query:
            # Regex for " 10 ", "No 10", "№10"
            pattern = r'(?:№|no\.?|\s|^)' + re.escape(args.number) + r'(?:\s|$|,|\.)'
            if not re.search(pattern, raw_display):
                match = False

        if match:
            filtered_objects.append(obj)

    target_list = filtered_objects[:args.limit]
    print(f"    -> Relevant Matches: {len(target_list)}")
    print(f"\n[4] FORENSIC SCANNING...")

    for obj in target_list:
        cad_num = obj.get('Number') or "N/A"
        
        try:
            info_url = f"{BASE_URL}/GetObjectInfo"
            resp_deep = session.get(info_url, params=obj)
            full_text = BeautifulSoup(resp_deep.text, 'html.parser').get_text(" ", strip=True)
            
            # --- PARSING LOGIC ---
            
            # Area
            area_m = re.search(r'площ\s*([\d\.]+)\s*кв', full_text, re.IGNORECASE)
            if not area_m: area_m = re.search(r'([\d\.]+)\s*кв\.\s*м', full_text, re.IGNORECASE)
            area = area_m.group(1) if area_m else "?"

            # Floor
            floor_m = re.search(r'(?:етаж|ет\.)\s*(\d+)', full_text, re.IGNORECASE)
            floor = floor_m.group(1) if floor_m else "-"

            # Ownership
            own_m = re.search(r'вид собств\.\s*([^,]+)', full_text, re.IGNORECASE)
            ownership = own_m.group(1).strip() if own_m else "?"

            # Neighbors (Crucial for Triangulation)
            neighbors_m = re.search(r'Съседи\s*:?\s*([\d\.,\s]+)', full_text, re.IGNORECASE)
            neighbors = neighbors_m.group(1).strip() if neighbors_m else "NONE"

            # Property Type Classification
            ft_lower = full_text.lower()
            if "пи " in ft_lower[:20] or "поземлен" in ft_lower: ptype = "LAND (ПИ)"
            elif "сграда" in ft_lower: ptype = "BUILDING"
            elif "жилище" in ft_lower or "апартамент" in ft_lower: ptype = "APARTMENT"
            else: ptype = "UNKNOWN"

            # --- REPORT ---
            print(f"\n--- {cad_num} ---")
            print(f"TYPE:  {ptype}")
            print(f"OWNER: {ownership}")
            print(f"AREA:  {area} m2")
            print(f"FLOOR: {floor}")
            if args.verbose:
                print(f"NEIGHBORS: {neighbors}")
            
            # If specifically looking for ownership/plot info, dump raw text
            if args.dump or ownership == "?" or "Няма данни" in ownership:
                print(f"RAW DUMP: {full_text[:500]}...")

        except Exception as e:
            print(f"    Error: {e}")

        time.sleep(0.2)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='KAIS Forensic Tool')
    
    # Flexible Input
    parser.add_argument('-q', '--query', help='Direct query string (e.g. ID or Address)')
    
    # Structured Input
    parser.add_argument('-d', '--district', help='District name')
    parser.add_argument('-b', '--block', help='Block number')
    parser.add_argument('-s', '--street', help='Street name')
    parser.add_argument('-n', '--number', help='Street number')
    parser.add_argument('-e', '--entrance', help='Entrance')
    
    # Toggles
    parser.add_argument('--plots-only', action='store_true', help='Ignore apartments, find land/buildings')
    parser.add_argument('--verbose', action='store_true', help='Show neighbors and extra details')
    parser.add_argument('--dump', action='store_true', help='Always dump raw text')
    parser.add_argument('--limit', type=int, default=15, help='Max results')

    args = parser.parse_args()
    
    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(1)
        
    run_forensics(args)



=========================================
FILE: ./scripts/audit_target.py
=========================================
import sys
import os
import argparse
Ensure src is in pythonpath
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(file), '..')))
from src.services.cadastre_service import CadastreForensicsService
def run_audit(target: str, mode: str):
service = CadastreForensicsService()
print(f"--- GLASHAUS FORENSICS: {target} ---")

if mode == "scan":
    # Standard Scan
    results = service.search_object(target)
    print(f"[*] Found {len(results)} matches. Analyzing...")
    
    for obj in results[:20]: # Limit default view to 20
        data = service.deep_scan_unit(obj)
        print(f"\nUNIT: {data.get('cadastre_id')}")
        print(f"  TYPE:  {data.get('type')}")
        print(f"  OWNER: {data.get('ownership')}")
        print(f"  AREA:  {data.get('area')} m2")
        if data.get('floor'):
            print(f"  FLOOR: {data.get('floor')}")
        
        # If finding plots, show neighbors
        if data.get('type') == 'LAND':
            print(f"  NEIGHBORS: {data.get('neighbors')}")

elif mode == "social_check":
    # The Social Housing Audit
    report = service.analyze_social_risk(target)
    print("\n--- SOCIAL RISK REPORT ---")
    print(f"TOTAL UNITS SCANNED: {report['total_units']}")
    print(f"PRIVATE OWNED:       {report['private_units']}")
    print(f"MUNICIPAL (SOCIAL):  {report['municipal_units']}")
    print(f"RATIO:               {report['social_housing_ratio']:.2%}")
    print(f"VERDICT:             {report['risk_verdict']}")
    print("--------------------------")

if name == "main":
parser = argparse.ArgumentParser()
parser.add_argument("target", help="Query (Address or ID)")
parser.add_argument("--mode", choices=["scan", "social_check"], default="scan")
args = parser.parse_args()
run_audit(args.target, args.mode)



=========================================
FILE: ./prompts/detective_prompt_v1.md
=========================================
# Role: Senior Real Estate Forensic Detective (Sofia, BG)
You are an expert in Sofia property forensics. Your goal is to cross-examine listing photos against the provided text to detect "Broker Lies", hidden costs, and valuation traps.

## Forensic Tasks:
1. **Landmark Geolocation (Triangulation):** - Search backgrounds for unique Sofia landmarks (Vitosha Mountain angle, TV Tower, NDK, Paradise Mall, etc.).
   - Extract names of shops, metro stations, or street signs visible in the pixels or mentioned in text.

2. **Object Localization (Inventory):** - Count visible AC units and heating radiators.
   - Look for the 'TEC' (Central Heating) nodes or gas boilers.

3. **Architectural Era Analysis:** - Identify if the building is Pre-1989 (Panel/EPC) or Post-2000 (Brick) based on facade style and materials.

4. **The Atelier Trap:** - Identify "shop-style" glass, ground-floor placements, or lack of balconies which suggest non-residential status.

5. **Valuation Traps (CRITICAL):**
   - **Net Area:** Extract the "Clean Living Area" (Чиста площ / Zastroena plosht) separate from common parts/terraces. If not explicitly stated, estimate it.
   - **Time Value:** Extract the promised "Act 16" or "Permission for use" date (e.g., "April 2026").

## Output Format (JSON Only):
{
  "address_prediction": "Specific street or clue",
  "landmarks": ["Name of shop/landmark found"],
  "neighborhood_match": "Confident/Suspicious/Inconsistent",
  "building_type": "Panel / Brick / EPC / New Construction",
  "is_panel_block": boolean,
  "heating_inventory": {
    "ac_units": 0,
    "radiators": 0,
    "has_central_heating": boolean
  },
  "net_area_sqm": float (0 if unknown),
  "act16_due_date": "YYYY-MM or 'Ready'",
  "visual_red_flags": ["List specific discrepancies"],
  "light_exposure": "North/South/East/West",
  "confidence_score": 0-100
}


=========================================
FILE: ./db/schema_v1.sql
=========================================
-- GLASHAUS REFERENCE SCHEMA (Synced with Production)
-- Includes logic from migrations 001 (workflow), 002 (currency), and 003 (area precision)

CREATE EXTENSION IF NOT EXISTS postgis;

CREATE TYPE report_status AS ENUM ('PENDING', 'PROCESSING', 'VERIFIED', 'MANUAL_REVIEW', 'REJECTED');

CREATE TABLE buildings (
    id SERIAL PRIMARY KEY,
    cadastre_id VARCHAR(50) UNIQUE NOT NULL,
    address_full VARCHAR(255),
    latitude FLOAT,
    longitude FLOAT,
    construction_year INT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE listings (
    id SERIAL PRIMARY KEY,
    source_url TEXT UNIQUE NOT NULL,
    content_hash VARCHAR(64), -- Idempotency check
    price_bgn NUMERIC(12, 2), -- Financial Precision
    advertised_area_sqm NUMERIC(10, 2), -- Area Precision
    description_raw TEXT,
    scraped_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
CREATE INDEX idx_listings_content_hash ON listings(content_hash);

CREATE TABLE reports (
    id SERIAL PRIMARY KEY,
    listing_id INT REFERENCES listings(id) ON DELETE CASCADE,
    building_id INT REFERENCES buildings(id),
    status report_status DEFAULT 'PENDING',
    risk_score INT,
    ai_confidence_score INT DEFAULT 0,
    legal_brief TEXT,
    discrepancy_details JSONB,
    image_archive_urls JSONB,
    cost_to_generate NUMERIC(10, 4), -- API Usage Cost
    manual_review_notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE price_history (
    id SERIAL PRIMARY KEY,
    listing_id INT REFERENCES listings(id),
    price_bgn NUMERIC(12, 2),
    changed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);


=========================================
FILE: ./db/migration_001_status_workflow.sql
=========================================
-- 1. Create the Workflow Status Enum
-- This supports the PENDING -> VERIFIED -> MANUAL_REVIEW flow
DO $$ BEGIN
    CREATE TYPE report_status AS ENUM ('PENDING', 'PROCESSING', 'VERIFIED', 'MANUAL_REVIEW', 'REJECTED');
EXCEPTION
    WHEN duplicate_object THEN null;
END $$;

-- 2. Update 'reports' table
ALTER TABLE reports 
ADD COLUMN IF NOT EXISTS status report_status DEFAULT 'PENDING',
ADD COLUMN IF NOT EXISTS ai_confidence_score INT DEFAULT 0,
ADD COLUMN IF NOT EXISTS manual_review_notes TEXT;

-- 3. Update 'listings' table for Idempotency
-- We hash the file/content to prevent duplicate processing of the same upload
ALTER TABLE listings
ADD COLUMN IF NOT EXISTS content_hash VARCHAR(64);

CREATE INDEX IF NOT EXISTS idx_listings_content_hash ON listings(content_hash);

-- 4. Create the Manual Review Queue View
-- This allows the Admin Panel to easily select tasks needing human eyes
CREATE OR REPLACE VIEW view_manual_review_queue AS
SELECT 
    r.id as report_id,
    l.source_url,
    r.ai_confidence_score,
    r.risk_score,
    r.created_at
FROM reports r
JOIN listings l ON r.listing_id = l.id
WHERE r.status = 'MANUAL_REVIEW'
ORDER BY r.risk_score DESC;


=========================================
FILE: ./db/migrations/env.py
=========================================
from logging.config import fileConfig
from sqlalchemy import engine_from_config
from sqlalchemy import pool
from alembic import context
import os
import sys

# Add src to path so we can import models
sys.path.append(os.getcwd())

from src.db.session import Base
from src.core.config import settings
from src.db.models import Listing, Report, Building 

config = context.config

if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Overwrite config URL with Environment Settings
config.set_main_option("sqlalchemy.url", settings.DATABASE_URL)

target_metadata = Base.metadata

def run_migrations_offline() -> None:
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online() -> None:
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()


=========================================
FILE: ./db/migrations/versions/migration_003_area_precision.py
=========================================
"""fix_area_precision

Revision ID: 003
Revises: 002
Create Date: 2025-12-19 20:30:00.000000

"""
from alembic import op
import sqlalchemy as sa

# revision identifiers, used by Alembic.
revision = '003'
down_revision = '002'
branch_labels = None
depends_on = None

def upgrade() -> None:
    # 1. Alter Listings Table
    op.alter_column('listings', 'advertised_area_sqm',
               existing_type=sa.Float(),
               type_=sa.Numeric(precision=10, scale=2),
               postgresql_using='advertised_area_sqm::numeric',
               existing_nullable=True)

def downgrade() -> None:
    op.alter_column('listings', 'advertised_area_sqm',
               existing_type=sa.Numeric(precision=10, scale=2),
               type_=sa.Float(),
               existing_nullable=True)


=========================================
FILE: ./db/migration_002_fix_currency.sql
=========================================
-- Fix Financial Precision for Listings and Reports
-- RUN THIS MANUALLY OR VIA ALEMBIC

ALTER TABLE listings 
ALTER COLUMN price_bgn TYPE NUMERIC(12, 2) 
USING price_bgn::numeric;

ALTER TABLE reports
ALTER COLUMN cost_to_generate TYPE NUMERIC(10, 4)
USING cost_to_generate::numeric;

ALTER TABLE price_history
ALTER COLUMN price_bgn TYPE NUMERIC(12, 2)
USING price_bgn::numeric;


=========================================
FILE: ./README.md
=========================================
# GLASHAUS: The Real Estate Integrity Engine

## Mission
To eliminate information asymmetry in the Sofia real estate market via automated due diligence.
We leverage OSINT, LLM reasoning, and Official Registry cross-referencing.

## Architecture
- **Text Layer:** Gemini Flash (Cost optimized)
- **Vision Layer:** Gemini Pro (Geospatial reasoning)
- **Data Layer:** PostgreSQL (Structured) + S3 (Archives)

## Status
- **Phase:** Pre-Alpha / Architectural Blueprint
- **Deploy Target:** Jan 2026 (Launch)


=========================================
FILE: ./requirements.txt
=========================================
fastapi==0.109.0
uvicorn==0.27.0
sqlalchemy==2.0.25
psycopg2-binary==2.9.9
httpx==0.26.0
playwright==1.41.0
asgiref==3.7.2
pydantic==2.6.0
pydantic-settings==2.1.0
google-generativeai>=0.7.0
beautifulsoup4==4.12.3
celery==5.3.6
redis==5.0.1
alembic==1.13.1
Pillow==10.2.0
structlog>=24.1.0


=========================================
FILE: ./index.html
=========================================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GLASHAUS // INTEL</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@300;400;700&display=swap');
        
        body { 
            font-family: 'JetBrains Mono', monospace; 
            background-color: #050505; 
            color: #E5E5E5;
            overflow-x: hidden;
        }

        /* PALANTIR AESTHETIC COLORS */
        .tech-blue { color: #00F0FF; text-shadow: 0 0 10px rgba(0, 240, 255, 0.3); }
        .tech-amber { color: #FFB000; }
        .border-tech { border: 1px solid #222; }
        
        /* BACKGROUND GRID */
        .grid-bg { 
            background-image: 
                linear-gradient(rgba(255, 255, 255, 0.02) 1px, transparent 1px),
                linear-gradient(90deg, rgba(255, 255, 255, 0.02) 1px, transparent 1px);
            background-size: 40px 40px;
        }

        /* CRT SCANLINE EFFECT */
        .scanline {
            position: fixed;
            top: 0;
            left: 0;
            width: 100vw;
            height: 100vh;
            background: linear-gradient(
                to bottom,
                rgba(255,255,255,0),
                rgba(255,255,255,0) 50%,
                rgba(0,0,0,0.2) 50%,
                rgba(0,0,0,0.2)
            );
            background-size: 100% 4px;
            pointer-events: none;
            z-index: 50;
            opacity: 0.3;
        }

        /* BLINKING CURSOR */
        .cursor-blink { 
            animation: blink 1s step-end infinite; 
            display: inline-block;
            vertical-align: text-bottom;
            width: 10px;
            height: 1.2em;
            background-color: #00F0FF;
        }
        @keyframes blink { 0%, 100% { opacity: 1; } 50% { opacity: 0; } }

        /* HOVER STATES */
        .module-card:hover {
            border-color: #00F0FF;
            background-color: rgba(0, 240, 255, 0.02);
            transform: translateY(-2px);
        }
    </style>
</head>
<body class="grid-bg min-h-screen flex flex-col justify-between relative selection:bg-cyan-900 selection:text-white">

    <div class="scanline"></div>

    <!-- HEADER -->
    <nav class="flex justify-between items-center p-6 border-b border-gray-900 bg-black/90 backdrop-blur-sm relative z-10">
        <div class="text-lg font-bold tracking-tighter flex items-center gap-2">
            GLASHAUS <span class="text-gray-700">//</span> ONTOLOGY v1.0<span class="cursor-blink"></span>
        </div>
        <div class="text-[10px] uppercase tracking-widest text-gray-500 flex items-center gap-2">
            <span class="w-2 h-2 rounded-full bg-green-500 animate-pulse"></span>
            System Status: Operational
        </div>
    </nav>

    <!-- HERO SECTION -->
    <main class="flex-grow flex flex-col justify-center px-6 lg:px-20 max-w-7xl mx-auto w-full relative z-10 py-20">
        
        <div class="mb-6 fade-in">
            <span class="border border-gray-800 bg-gray-900/50 text-[10px] font-bold px-3 py-1 text-gray-400 uppercase tracking-[0.2em]">
                Classified: Institutional Tier
            </span>
        </div>

        <h1 class="text-6xl md:text-8xl font-bold leading-none tracking-tighter mb-8 text-white">
            THE SOFIA<br>
            <span class="text-gray-800">DOCTRINE.</span>
        </h1>

        <p class="text-lg md:text-xl text-gray-500 max-w-2xl mb-16 leading-relaxed font-light">
            Eliminating information asymmetry in the Balkan Capital Market. 
            <span class="text-gray-300">From satellite orbit to the radiator valve.</span>
            Automated forensic visibility for the sovereign investor.
        </p>

        <!-- MODULES -->
        <div class="grid grid-cols-1 md:grid-cols-3 gap-6 mb-16">
            
            <!-- CARD 01 -->
            <div class="module-card border-tech p-8 transition duration-300 cursor-crosshair">
                <div class="text-gray-600 text-[10px] font-bold mb-4 uppercase tracking-widest flex justify-between">
                    Module / 01
                    <span class="tech-amber">● LIVE</span>
                </div>
                <h3 class="text-2xl font-bold mb-3 text-white">INSOLVENCY RADAR</h3>
                <p class="text-xs text-gray-400 leading-6">
                    Real-time liquidity stress testing for developer entities. Identifying the "living dead" via debt-to-inventory ratio analysis before the default event occurs.
                </p>
            </div>

            <!-- CARD 02 -->
            <div class="module-card border-tech p-8 transition duration-300 cursor-crosshair">
                <div class="text-gray-600 text-[10px] font-bold mb-4 uppercase tracking-widest flex justify-between">
                    Module / 02
                    <span class="tech-blue">● ACTIVE</span>
                </div>
                <h3 class="text-2xl font-bold mb-3 text-white">TOXIC COLLATERAL</h3>
                <p class="text-xs text-gray-400 leading-6">
                    Automated detection of "Atelier" status, expropriation risk (The Death List), and Act 16 voids using multimodal AI vision and registry cross-referencing.
                </p>
            </div>

            <!-- CARD 03 -->
            <div class="module-card border-tech p-8 transition duration-300 cursor-crosshair">
                <div class="text-gray-600 text-[10px] font-bold mb-4 uppercase tracking-widest flex justify-between">
                    Module / 03
                    <span class="text-gray-500">● MAPPING</span>
                </div>
                <h3 class="text-2xl font-bold mb-3 text-white">SHADOW LAND BANK</h3>
                <p class="text-xs text-gray-400 leading-6">
                    Predictive acquisition modeling. Mapping municipal "Private Property" parcels before they hit the public tender.
                </p>
            </div>

        </div>

        <div class="flex gap-4">
            <button class="bg-white text-black px-8 py-4 font-bold uppercase tracking-widest text-xs hover:bg-gray-200 transition">
                Authenticate
            </button>
            <button class="border border-gray-800 text-gray-500 px-8 py-4 font-bold uppercase tracking-widest text-xs hover:text-white hover:border-gray-500 transition">
                View Protocol
            </button>
        </div>

    </main>

    <!-- FOOTER -->
    <footer class="p-6 text-[10px] text-gray-700 border-t border-gray-900 flex justify-between uppercase tracking-widest relative z-10">
        <div class="flex gap-6">
            <span>Latency: 12ms</span>
            <span>Node: ZURICH</span>
            <span>Encryption: AES-256</span>
        </div>
        <div>
            © 2024 GLASHAUS INTELLIGENCE
        </div>
    </footer>

</body>
</html>


=========================================
FILE: ./Dockerfile
=========================================
# STAGE 1: Builder
FROM python:3.11-slim as builder

WORKDIR /app
RUN apt-get update && apt-get install -y \
    libpq-dev gcc build-essential \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# STAGE 2: Runtime
FROM python:3.11-slim as runtime

WORKDIR /app

# Install only runtime libs (libpq for Postgres)
RUN apt-get update && apt-get install -y \
    libpq5 netcat-openbsd \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /root/.local /root/.local
ENV PATH=/root/.local/bin:$PATH

# Copy Application Code
COPY . .

# Create a non-root user for security
RUN useradd -m glashaus_user
USER glashaus_user

EXPOSE 8000

CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]


=========================================
FILE: ./docker-compose.yml
=========================================
version: '3.8'

services:
  # 1. GATEWAY (Entry Point)
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - api

  # 2. API (Internal)
  api:
    build: .
    expose:
      - "8000" # Not mapped to host anymore, only accessible via Nginx
    environment:
      - POSTGRES_SERVER=db
      - POSTGRES_USER=glashaus_user
      - POSTGRES_PASSWORD=secret_password
      - POSTGRES_DB=glashaus_db
      - REDIS_URL=redis://redis:6379/0
      # - GEMINI_API_KEY=${GEMINI_API_KEY} # Uncomment for prod
    depends_on:
      - db
      - redis
    volumes:
      - ./src:/app/src

  # 3. WORKER
  worker:
    build: .
    command: celery -A src.worker.celery_app worker --loglevel=info
    environment:
      - POSTGRES_SERVER=db
      - POSTGRES_USER=glashaus_user
      - POSTGRES_PASSWORD=secret_password
      - POSTGRES_DB=glashaus_db
      - REDIS_URL=redis://redis:6379/0
      # - GEMINI_API_KEY=${GEMINI_API_KEY}
    depends_on:
      - db
      - redis
    volumes:
      - ./src:/app/src

  # 4. REDIS
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  # 5. DB
  db:
    image: postgis/postgis:15-3.4
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=glashaus_user
      - POSTGRES_PASSWORD=secret_password
      - POSTGRES_DB=glashaus_db
    ports:
      - "5432:5432"

volumes:
  postgres_data:


=========================================
FILE: ./tests/test_api.py
=========================================
from fastapi.testclient import TestClient
from src.main import app

client = TestClient(app)

def test_health_check():
    response = client.get("/")
    assert response.status_code == 200
    assert response.json()["status"] == "OPERATIONAL"

def test_audit_flow():
    payload = {"url": "https://www.imot.bg/pcgi/imot.cgi?act=5&adv=mock123"}
    response = client.post("/audit", json=payload)
    
    assert response.status_code == 200
    data = response.json()
    assert data["status"] == "QUEUED"
    assert "listing_id" in data


=========================================
FILE: ./alembic.ini
=========================================
[alembic]
script_location = db/migrations
prepend_sys_path = .
sqlalchemy.url = driver://user:pass@localhost/dbname

[post_write_hooks]

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S


=========================================
FILE: ./nginx/nginx.conf
=========================================
events {}

http {
    upstream glashaus_api {
        server api:8000;
    }

    server {
        listen 80;
        
        # Proxy all API requests to the FastAPI container
        location / {
            proxy_pass http://glashaus_api;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        # Health check endpoint
        location /health {
            return 200 'alive';
            add_header Content-Type text/plain;
        }
    }
}


=========================================
FILE: ./forensics/headers.txt
=========================================
HTTP/2 200 
date: Fri, 19 Dec 2025 10:22:43 GMT
content-type: text/html
server: cloudflare
vary: Accept-Encoding
set-cookie: imot_session_redirect=adv%091c171899111%09act%095%09; domain=.imot.bg; path=/
referrer-policy: no-referrer-when-downgrade
strict-transport-security: max-age=15552000
nel: {"report_to":"cf-nel","success_fraction":0.0,"max_age":604800}
cf-cache-status: DYNAMIC
report-to: {"group":"cf-nel","max_age":604800,"endpoints":[{"url":"https://a.nel.cloudflare.com/report/v4?s=j9v062ydriN%2F%2BXubjtcJm8HyyWIt5Iad0Ozkt1xE1LO0Maoge0%2FxNlheTZuu%2FtFcpdrE5nQ8tbcs1nOXtqzEhMuYqUOOk1Zv"}]}
cf-ray: 9b062e33d9d0d0d4-SOF
alt-svc: h3=":443"; ma=86400



=========================================
FILE: ./forensics/page_utf8.html
=========================================


=========================================
FILE: ./test_forensics.py
=========================================
import asyncio
import sys
import json
from src.services.forensics_service import SofiaMunicipalForensics

# The identifier found in your curl examples (Bankya/Verdikal)
TEST_ID = "02659.2196.1102"

async def main():
    print(f"[*] Starting Forensic Audit for Cadastre ID: {TEST_ID}...")
    
    service = SofiaMunicipalForensics()
    try:
        results = await service.run_full_audit(TEST_ID)
        
        print("\n--- AUDIT RESULTS ---")
        print(json.dumps(results, indent=2, ensure_ascii=False))
        
        # Validation Logic
        if results.get("expropriation", {}).get("is_expropriated"):
            print("\n[!] CRITICAL ALERT: Property is flagged for Expropriation!")
        
        if results.get("compliance_act16", {}).get("has_act16"):
            print("\n[+] SUCCESS: Act 16 Certificate Found.")
        else:
            print("\n[-] WARNING: No Act 16 found.")
            
    except Exception as e:
        print(f"\n[!] FATAL ERROR: {e}")

if __name__ == "__main__":
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
