--- GLASHAUS PROJECT DUMP ---
Fri Dec 19 08:44:46 EET 2025


--- GIT HISTORY ---
* a087b30 (HEAD -> main) feat(backend): implement vision layer, harden worker logic, and add regex parsing
* 4987d66 feat(infra): add Nginx gateway, CORS, and live scraping logic
* ae76a80 refactor(core): upgrade architecture to Celery/Redis and harden AI logic
* b7f3797 refactor(core): migrate to Redis/Celery queue and add Alembic migrations
* f22a7da feat(core): implement AI engine, workflow logic, and docs
* e847d18 feat(backend): complete ORM models, repo pattern, and test suite
* d266a4d ops: add docker containerization and orchestration
* 07beab5 feat(scraper): implement simulation mode to bypass WAF during dev
* 8d13095 chore(scripts): include git log in context dumper
* 15c8316 refactor(api): connect routes to database via dependency injection
* 84bff79 feat(db): implement SQLAlchemy ORM models and Repository pattern
* 648c703 feat(core): implement abstract AI service layer
* 2e3b2a8 docs: add German functional specification (Lastenheft)
* 0307ac8 feat: init project structure, docs, and database schema


--- FILE STRUCTURE ---
.
./docs
./docs/ARCHITECTURE.md
./docs/api_contract.yaml
./docs/LASTENHEFT_DE.md
./docs/TECHNICAL_DEBT.md
./src
./src/api
./src/api/routes.py
./src/core
./src/core/config.py
./src/core/utils.py
./src/services
./src/services/ai_engine.py
./src/services/repository.py
./src/services/scraper_mvp.py
./src/services/scraper_service.py
./src/services/risk_engine.py
./src/services/cadastre_service.py
./src/services/geospatial_service.py
./src/models
./src/__init__.py
./src/main.py
./src/db
./src/db/session.py
./src/db/models.py
./src/worker.py
./src/tasks.py
./scripts
./scripts/context_dump.sh
./prompts
./prompts/detective_prompt_v1.md
./db
./db/schema_v1.sql
./db/migration_001_status_workflow.sql
./db/migrations
./db/migrations/env.py
./README.md
./glashaus_context.txt
./requirements.txt
./imot_simulation.html
./Dockerfile
./docker-compose.yml
./tests
./tests/test_api.py
./alembic.ini
./nginx
./nginx/nginx.conf


--- FILE CONTENTS ---


=========================================
FILE: ./docs/ARCHITECTURE.md
=========================================
# System Architecture

## Core Logic Flow (Cost Optimized)
1. **Ingest:** Scraper Service fetches URL.
2. **Analysis Tier 1 (Cheap):** Text Extraction (Gemini Flash).
   - *Goal:* Find Address in text.
   - *Cost:* ~$0.04/run.
3. **Gatekeeper:** Confidence Check (>90%).
4. **Analysis Tier 2 (Expensive):** Visual Detective (Gemini Pro).
   - *Trigger:* Only if Tier 1 fails.
   - *Goal:* Identify building via Facade/Landmarks/Geofencing.
   - *Cost:* ~$0.22/run.
5. **Verification:** Cross-reference extracted Address vs Cadastre API.

## Tech Stack
- **Lang:** Python (FastAPI) or Java (Spring Boot) - TBD
- **DB:** PostgreSQL + PostGIS (Geospatial extensions)
- **Queue:** Redis (Task offloading)


=========================================
FILE: ./docs/api_contract.yaml
=========================================
openapi: 3.0.0
info:
  title: Glashaus API
  version: 0.1.0
paths:
  /audit/url:
    post:
      summary: Initiate an Audit for a specific Listing URL
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                url:
                  type: string
                  format: uri
      responses:
        '200':
          description: Audit Complete
          content:
            application/json:
              schema:
                type: object
                properties:
                  report_id:
                    type: integer
                  risk_score:
                    type: integer
                  verified_address:
                    type: string
                  ai_reasoning:
                    type: array
                    items:
                      type: string


=========================================
FILE: ./docs/LASTENHEFT_DE.md
=========================================
# LASTENHEFT: Projekt Glashaus
**Version:** 1.0.0
**Status:** In Entwicklung

## 1. Einleitung
Das Projekt "Glashaus" ist eine automatisierte Due-Diligence-Plattform für den Immobilienmarkt in Sofia. Ziel ist die Beseitigung von Informationsasymmetrien durch den Einsatz von OSINT und KI-gestützter Datenanalyse.

## 2. Ist-Zustand (Problemstellung)
- **Datenfragmentierung:** Grundbuch (Registry), Kataster (Cadastre) und Gemeinde agieren in Silos.
- **Intransparenz:** Immobilienanzeigen enthalten oft ungenaue Flächenangaben und verschleierte Adressen.
- **Prozessineffizienz:** Manuelle Prüfungen sind teuer und langsam.

## 3. Soll-Zustand (Lösung)
Ein Microservices-System, das folgende Kernfunktionen bietet:
1.  **Automatische Adress-Deduktion:** Ermittlung der exakten Adresse aus unstrukturierten Anzeigentexten und Bildern.
2.  **Soll/Ist-Abgleich:** Automatischer Vergleich von Maklerangaben (Anzeige) mit amtlichen Katasterdaten.
3.  **Risikobewertung:** Algorithmische Berechnung eines "Risk Scores" (0-100).

## 4. Technische Anforderungen
- **Architektur:** Event-Driven Microservices (Python/FastAPI).
- **Datenbank:** PostgreSQL mit PostGIS für Geodatenverarbeitung.
- **KI-Integration:**
    - *Tier 1:* Textanalyse (Low Cost / Gemini Flash).
    - *Tier 2:* Visuelle Analyse (High Cost / Gemini Pro).

## 5. Nicht-funktionale Anforderungen
- **Idempotenz:** Wiederholte Uploads dürfen keine Datenkorruption verursachen.
- **Skalierbarkeit:** Das System muss Warteschlangen (Queues) nutzen, um Lastspitzen bei Scrapern abzufangen.


=========================================
FILE: ./docs/TECHNICAL_DEBT.md
=========================================
# TECHNICAL DEBT LOG

## Critical Severity (Must Fix Before Beta)

### 1. Database Session Scope in Background Tasks
- **Location:** `src/api/routes.py` -> `initiate_audit`
- **Issue:** Passing the dependency-injected `db` session to `BackgroundTasks` causes `DetachedInstanceError` because the session closes when the HTTP response returns.
- **Fix:** Refactor `process_audit_task` to instantiate a fresh `SessionLocal()` context manager internally.
- **Reference:** SQLAlchemy Thread-Safety docs.

### 2. AI Service Implementation
- **Location:** `src/services/ai_engine.py`
- **Issue:** Currently returns mock dictionary `{"confidence": 0.0}`.
- **Fix:** 
    - Initialize `genai.configure(api_key=...)`.
    - Implement `generate_content` call for Gemini Flash (Text).
    - Implement `generate_content` with Image inputs for Gemini Pro (Vision).
    - Add Error Handling for "Safety Filters" or API Quotas.

## Moderate Severity (Logic Gaps)

### 3. Risk Scoring Algorithm
- **Location:** Database Schema exists (`risk_score`), but logic is missing.
- **Issue:** No calculator exists to translate discrepancies into a 0-100 integer.
- **Fix:** Create `src/services/risk_engine.py`.
    - Base Score: 0
    - If `advertised_area` > `cadastre_area` (+20 pts).
    - If `type` mismatch (Atelier vs Apt) (+30 pts).
    - If `price_per_sqm` > 1.5x avg (+15 pts).

### 4. Hardcoded Secrets
- **Location:** `src/core/config.py`
- **Issue:** Default "mock-key" risks silent failure in prod.
- **Fix:** Implement `pydantic` validation to raise `ValueError` on startup if `GEMINI_API_KEY` is missing in production environment.

## Low Severity (Optimization)

### 5. Listing Normalization
- **Location:** `src/services/repository.py`
- **Issue:** Duplicate URLs might occur if query params differ (e.g., `?adv=1` vs `?adv=1&utm=facebook`).
- **Fix:** Implement a URL cleaner utility to strip tracking parameters before hashing/storing.


=========================================
FILE: ./src/api/routes.py
=========================================
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from src.db.session import get_db
from src.db.models import Listing, Report, ReportStatus
from src.services.repository import RealEstateRepository
from src.tasks import audit_listing_task
from pydantic import BaseModel
from typing import Optional

router = APIRouter()

class AuditRequest(BaseModel):
    url: str
    price_override: float = 0.0

class ReportUpdate(BaseModel):
    status: str
    manual_notes: Optional[str] = None

@router.post("/audit")
async def initiate_audit(request: AuditRequest, db: Session = Depends(get_db)):
    """
    Submits a URL for auditing via the Celery Worker queue.
    """
    repo = RealEstateRepository(db)
    # Create listing immediately to return ID
    listing = repo.create_listing(url=request.url, price=request.price_override, area=0.0, desc="Queued")
    
    # Offload to Redis/Celery
    audit_listing_task.delay(listing.id)
    
    return {"listing_id": listing.id, "status": "QUEUED_IN_REDIS"}

@router.get("/reports/{listing_id}")
def get_report(listing_id: int, db: Session = Depends(get_db)):
    """
    Retrieves the report status and details for a listing.
    """
    listing = db.query(Listing).filter(Listing.id == listing_id).first()
    if not listing:
        raise HTTPException(status_code=404, detail="Listing not found")
        
    report = db.query(Report).filter(Report.listing_id == listing_id).first()
    if not report:
        # If no report exists yet, the worker is likely still processing
        return {"status": "PROCESSING", "details": "Audit is currently in the queue."}
        
    return {
        "report_id": report.id,
        "status": report.status,
        "risk_score": report.risk_score,
        "ai_confidence": report.ai_confidence_score,
        "discrepancies": report.discrepancy_details,
        "manual_notes": report.manual_review_notes,
        "cost": report.cost_to_generate,
        "created_at": report.created_at
    }

@router.patch("/reports/{report_id}")
def update_report_status(report_id: int, update: ReportUpdate, db: Session = Depends(get_db)):
    """
    Manual Review Action.
    Updates status (e.g., MANUAL_REVIEW -> VERIFIED).
    """
    report = db.query(Report).filter(Report.id == report_id).first()
    if not report:
        raise HTTPException(status_code=404, detail="Report not found")
        
    try:
        # Validate that the string provided matches the Enum
        new_status = ReportStatus(update.status)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid Status Enum")

    report.status = new_status
    if update.manual_notes:
        report.manual_review_notes = update.manual_notes
        
    db.commit()
    return {"id": report.id, "new_status": report.status}


=========================================
FILE: ./src/core/config.py
=========================================
import os

class Settings:
    PROJECT_NAME: str = "Glashaus"
    VERSION: str = "0.1.0"
    
    GEMINI_API_KEY: str = os.getenv("GEMINI_API_KEY", "mock-key")

    POSTGRES_USER: str = os.getenv("POSTGRES_USER", "postgres")
    POSTGRES_SERVER: str = os.getenv("POSTGRES_SERVER", "")
    POSTGRES_DB: str = os.getenv("POSTGRES_DB", "glashaus")
    POSTGRES_PASSWORD: str = os.getenv("POSTGRES_PASSWORD", "postgres")

    # NEW: Redis Configuration for Celery
    REDIS_URL: str = os.getenv("REDIS_URL", "redis://redis:6379/0")
    
    @property
    def DATABASE_URL(self) -> str:
        if not self.POSTGRES_SERVER:
            return "sqlite:///./glashaus.db"
        return f"postgresql://{self.POSTGRES_USER}:{self.POSTGRES_PASSWORD}@{self.POSTGRES_SERVER}/{self.POSTGRES_DB}"

settings = Settings()

if settings.GEMINI_API_KEY == "mock-key":
    print("--- WARNING: GEMINI_API_KEY is not set. Using MOCK mode. ---")


=========================================
FILE: ./src/core/utils.py
=========================================
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
import hashlib
import re

def normalize_url(url: str) -> str:
    """
    Strips tracking parameters and fragments.
    """
    u = urlparse(url)
    query = dict(parse_qsl(u.query))
    whitelist = {'act', 'adv', 'id', 'slink'}
    clean_query = {k: v for k, v in query.items() if k.lower() in whitelist}
    return urlunparse((
        u.scheme, u.netloc, u.path, u.params,
        urlencode(clean_query), ''
    ))

def calculate_content_hash(text: str, price: float) -> str:
    """
    Generates a SHA256 hash of the description + price.
    Used for Idempotency to detect if a bumped listing is actually identical.
    """
    # Normalize text to ignore whitespace diffs
    clean_text = re.sub(r'\s+', ' ', text).strip().lower()
    raw = f"{clean_text}{price}".encode('utf-8')
    return hashlib.sha256(raw).hexdigest()

def normalize_address(raw_address: str) -> str:
    """
    Standardizes Bulgarian addresses for cross-referencing.
    Input: "ul. 'Pirotska' No 5, et. 2" -> "PIROTSKA 5"
    """
    if not raw_address:
        return "UNKNOWN"
        
    # 1. Transliterate/Cleanup (Simplified for this example)
    clean = raw_address.lower()
    
    # 2. Remove common prefixes
    prefixes = [r"ul\.", r"ulitsa", r"str\.", r"street", r"bulevard", r"bul\.", r"bl\.", r"block"]
    for p in prefixes:
        clean = re.sub(p, "", clean)
        
    # 3. Remove punctuation
    clean = re.sub(r"[,'\"\.]", " ", clean)
    
    # 4. Extract Street Name and Number
    # Looking for: (Text) ... (Number)
    # This is a heuristic; production needs a better geocoder
    match = re.search(r"([a-z\s]+)\s+(\d+)", clean)
    if match:
        street = match.group(1).strip().upper()
        number = match.group(2)
        return f"{street} {number}"
        
    return clean.strip().upper()


=========================================
FILE: ./src/services/ai_engine.py
=========================================
import google.generativeai as genai
import json
import httpx
import io
from PIL import Image
from typing import Dict, Any, List, Optional
from src.core.config import settings
from pydantic import BaseModel, Field

# --- PYDANTIC SCHEMA ---
class AIAnalysisSchema(BaseModel):
    address_prediction: str = Field(description="The deduced street address")
    confidence: int = Field(ge=0, le=100, description="Confidence score 0-100")
    reasoning_steps: List[str] = Field(description="Bullet points explaining the deduction")
    neighborhood: str = Field(description="Inferred neighborhood")
    is_atelier: bool = Field(description="True if property is legally an Atelier/Studio")

class GeminiService:
    def __init__(self, api_key: str):
        self.api_key = api_key
        
        if self.api_key != "mock-key":
            genai.configure(api_key=self.api_key)
            self.model_flash = genai.GenerativeModel('gemini-1.5-flash')
            self.model_pro = genai.GenerativeModel('gemini-1.5-pro-vision')
        else:
            self.model_flash = None
            self.model_pro = None

    def _load_prompt(self, raw_text: str) -> str:
        return f"""
        Analyze this real estate listing text.
        Extract the address, neighborhood, and verify if it is an Atelier.
        LISTING TEXT:
        {raw_text}
        """

    async def analyze_text(self, text: str) -> Dict[str, Any]:
        if not self.model_flash:
            return {
                "address_prediction": "Simulated Address", 
                "confidence": 0, 
                "reasoning_steps": ["Mock Mode"],
                "is_atelier": False
            }

        try:
            prompt = self._load_prompt(text)
            response = self.model_flash.generate_content(
                prompt,
                generation_config={
                    "response_mime_type": "application/json",
                    "response_json_schema": AIAnalysisSchema.model_json_schema()
                }
            )
            structured_data = AIAnalysisSchema.model_validate_json(response.text)
            return structured_data.model_dump()

        except Exception as e:
            print(f"[AI ENGINE] Text Error: {e}")
            return {"error": str(e), "confidence": 0}

    async def analyze_images(self, image_urls: list[str]) -> Dict[str, Any]:
        """
        Downloads images and performs Visual Detective analysis.
        """
        if not self.model_pro:
            return {"confidence": 0, "reasoning": "Vision Mock Mode"}

        images_payload = []
        
        # 1. Download images into memory
        async with httpx.AsyncClient() as client:
            for url in image_urls:
                try:
                    # Filter out empty or invalid URLs
                    if not url or not url.startswith('http'): continue
                    
                    resp = await client.get(url, timeout=5.0)
                    if resp.status_code == 200:
                        img = Image.open(io.BytesIO(resp.content))
                        images_payload.append(img)
                except Exception as e:
                    print(f"[AI VISION] Download failed for {url}: {e}")

        if not images_payload:
            return {"confidence": 0, "reasoning": "No valid images downloaded"}

        # 2. Prompt Gemini Pro Vision
        prompt = """
        Act as a geospatial detective. Analyze these real estate photos.
        1. Identify any unique landmarks (View from window).
        2. Estimate building era based on facade/balcony style.
        3. Look for street signs or shop names.
        Return JSON with 'facade_era', 'landmarks_detected', and 'estimated_location_confidence'.
        """
        
        try:
            # Gemini Pro Vision handles list of [Prompt, Image, Image...]
            response = self.model_pro.generate_content(
                contents=[prompt, *images_payload],
                generation_config={"response_mime_type": "application/json"}
            )
            return json.loads(response.text)
        except Exception as e:
            print(f"[AI VISION ERROR] {e}")
            return {"confidence": 0, "error": str(e)}


=========================================
FILE: ./src/services/repository.py
=========================================
from sqlalchemy.orm import Session
from src.db.models import Listing, Report, ReportStatus
from src.core.utils import normalize_url
from typing import Optional

class RealEstateRepository:
    def __init__(self, db: Session):
        self.db = db

    def create_listing(self, url: str, price: float, area: float, desc: str) -> Listing:
        clean_url = normalize_url(url)
        # 1. URL Check (Fastest)
        existing = self.get_listing_by_url(clean_url)
        if existing:
            return existing
            
        new_listing = Listing(
            source_url=clean_url,
            price_bgn=price,
            advertised_area_sqm=area,
            description_raw=desc
        )
        self.db.add(new_listing)
        self.db.commit()
        self.db.refresh(new_listing)
        return new_listing

    def get_listing_by_url(self, url: str) -> Optional[Listing]:
        return self.db.query(Listing).filter(Listing.source_url == url).first()

    def check_content_duplication(self, content_hash: str) -> Optional[int]:
        """
        Returns the ID of an existing listing if the content hash matches.
        """
        existing = self.db.query(Listing).filter(Listing.content_hash == content_hash).first()
        return existing.id if existing else None

    def update_listing_content(self, listing_id: int, description: str, content_hash: str):
        listing = self.db.query(Listing).get(listing_id)
        if listing:
            listing.description_raw = description
            listing.content_hash = content_hash
            self.db.commit()

    def create_report(self, listing_id: int, risk: int, details: dict, cost: float, confidence: int = 0) -> Report:
        status = ReportStatus.VERIFIED if confidence >= 70 else ReportStatus.MANUAL_REVIEW
        
        report = Report(
            listing_id=listing_id,
            risk_score=risk,
            ai_confidence_score=confidence,
            status=status,
            discrepancy_details=details,
            cost_to_generate=cost
        )
        self.db.add(report)
        self.db.commit()
        self.db.refresh(report)
        return report


=========================================
FILE: ./src/services/scraper_mvp.py
=========================================
from bs4 import BeautifulSoup
import os

# CONFIG
SIMULATION_MODE = True
MOCK_FILE = "imot_simulation.html"

def run_recon():
    print("[*] INTEL: Starting Reconnaissance Protocol...")
    
    html_content = ""
    
    if SIMULATION_MODE:
        print(f"[*] MODE: SIMULATION (Bypassing WAF)")
        if not os.path.exists(MOCK_FILE):
            print(f"[!] Error: Mock file {MOCK_FILE} not found.")
            return
            
        with open(MOCK_FILE, "r", encoding="utf-8") as f:
            html_content = f.read()
    else:
        # Network logic removed for Termux Safety
        pass

    soup = BeautifulSoup(html_content, 'html.parser')
    links = soup.find_all('a', href=True)
    
    print("[*] Parsing DOM Structure...")
    
    count = 0
    listings_found = []
    
    for link in links:
        href = link['href']
        
        if 'act=5' in href:
            # Normalize URL
            full_url = "https:" + href if href.startswith("//") else href
            
            if full_url in listings_found:
                continue
                
            listings_found.append(full_url)
            text_content = link.get_text(separator=" ", strip=True)
            
            count += 1
            print(f"\n[TARGET #{count}]")
            print(f"   URL: {full_url}")
            print(f"   RAW: {text_content}")

    print(f"\n[*] Mission Complete. {count} mock targets extracted.")

if __name__ == "__main__":
    run_recon()


=========================================
FILE: ./src/services/scraper_service.py
=========================================
from bs4 import BeautifulSoup
import os
import httpx
import random
import re

class ScraperService:
    def __init__(self, simulation_mode=False):
        self.simulation = simulation_mode
        self.mock_file = "imot_simulation.html"
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
        ]

    def _parse_price(self, text: str) -> float:
        # Matches: 185 000 EUR, 185000, 185.000
        match = re.search(r'([\d\s\.,]+)\s?(?:EUR|€|leva|BGN)', text, re.IGNORECASE)
        if match:
            clean = match.group(1).replace(" ", "").replace(",", "").strip()
            try:
                return float(clean)
            except ValueError:
                return 0.0
        return 0.0

    def _parse_area(self, text: str) -> float:
        # Matches: 85 kv.m, 85 m2, 85 кв.м
        match = re.search(r'([\d\.,]+)\s?(?:kv|m2|кв)', text, re.IGNORECASE)
        if match:
            clean = match.group(1).replace(",", ".").strip()
            try:
                return float(clean)
            except ValueError:
                return 0.0
        return 0.0

    def scrape_url(self, url: str) -> dict:
        print(f"[SCRAPER] Target: {url} | Mode: {'SIMULATION' if self.simulation else 'LIVE'}")
        
        html_content = ""
        
        if self.simulation:
            if not os.path.exists(self.mock_file):
                return {"raw_text": "Simulation Error: File missing", "image_urls": [], "price_predicted": 0, "area": 0}
            with open(self.mock_file, "r", encoding="utf-8") as f:
                html_content = f.read()
        else:
            try:
                headers = {"User-Agent": random.choice(self.user_agents)}
                with httpx.Client(timeout=10.0, follow_redirects=True) as client:
                    response = client.get(url, headers=headers)
                    response.raise_for_status()
                    html_content = response.text
            except Exception as e:
                print(f"[SCRAPER] Network Error: {e}")
                raise e

        soup = BeautifulSoup(html_content, 'html.parser')
        
        # 1. Extract Text
        ad_block = soup.find('div', class_='text_desc') or soup.find('div', class_='ad-description')
        raw_text = ad_block.get_text(separator=" ", strip=True) if ad_block else ""
        
        if not raw_text and not self.simulation:
            raw_text = soup.body.get_text(separator=" ", strip=True)[:2000]

        # 2. Extract Images (Limit 3)
        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src and 'http' in src and ('jpg' in src or 'png' in src):
                images.append(src)
        images = images[:3]
        
        # 3. Parse Metadata
        price = self._parse_price(raw_text)
        area = self._parse_area(raw_text)

        return {
            "source_url": url,
            "raw_text": raw_text,
            "image_urls": images,
            "price_predicted": price,
            "area": area
        }


=========================================
FILE: ./src/services/risk_engine.py
=========================================
from typing import Dict, Any, List, Optional

class RiskEngine:
    def calculate_score(self, advertised_data: Dict, ai_insights: Dict, cadastre_area: Optional[float] = None) -> Dict[str, Any]:
        score = 0
        flags = []
        
        raw_text = advertised_data.get("raw_text", "").upper()
        advertised_area = advertised_data.get("area", 0.0)

        # 1. LEGAL STATUS (Atelier)
        # Using the AI's structured output if available, else fallback to text
        is_atelier = ai_insights.get("is_atelier", False)
        if is_atelier or "АТЕЛИЕ" in raw_text or "ATELIER" in raw_text:
            score += 30
            flags.append("LEGAL_STATUS_RISK: Property is Atelier (Industrial/Non-residential status)")

        # 2. AREA MATH (Advertised vs Cadastre)
        # The Gap fix: Check if advertised area is suspicious compared to official records
        if cadastre_area and advertised_area > 0:
            if advertised_area > (cadastre_area * 1.15): # 15% tolerance
                score += 20
                flags.append(f"AREA_DISCREPANCY: Advertised ({advertised_area}) > Cadastre ({cadastre_area})")

        # 3. FLOOR RISKS
        if "ПОСЛЕДЕН" in raw_text or "TOP FLOOR" in raw_text:
            score += 15
            flags.append("MAINTENANCE_RISK: Top floor")
        
        if "ПЪРВИ" in raw_text or "GROUND" in raw_text:
            score += 10
            flags.append("SECURITY_RISK: Ground floor")

        # 4. PRICE ANOMALY
        if advertised_data.get("price_predicted", 0) > 300000:
            score += 5
            flags.append("FINANCIAL: High-ticket verification required")

        return {
            "score": min(score, 100),
            "flags": flags
        }


=========================================
FILE: ./src/services/cadastre_service.py
=========================================
from typing import Optional, Dict

class CadastreService:
    """
    Interface for the Official Property Registry (Cadastre).
    In a real deployment, this would use the official API or a headless browser.
    """
    def __init__(self):
        # In the future, API keys for the government portal go here
        pass

    def fetch_details(self, address: str) -> Optional[Dict[str, float]]:
        """
        Simulates a lookup in the national registry.
        Returns: { "official_area": 85.0, "year_built": 2005 }
        """
        print(f"[CADASTRE] Querying registry for: {address}")
        
        # MOCK LOGIC: 
        # For now, we return 'None' to simulate that most addresses 
        # cannot be perfectly matched automatically yet.
        # This prevents the Risk Engine from throwing false positives 
        # until the real integration is ready.
        
        return None 
        
        # EXAMPLE REAL IMPLEMENTATION STUB:
        # response = requests.get(f"https://kais.cadastre.bg/api/search?q={address}")
        # if response.status_code == 200:
        #     return response.json()
        # return None


=========================================
FILE: ./src/services/geospatial_service.py
=========================================
from typing import Optional, Dict

class GeospatialService:
    """
    Handles Google Maps / Street View verification.
    """
    def __init__(self, api_key: str = "mock-key"):
        self.api_key = api_key

    def verify_location(self, address: str) -> Dict[str, any]:
        """
        TODO: Implement Google Maps Geocoding API.
        Should return { "lat": float, "lng": float, "street_view_exists": bool }
        """
        if self.api_key == "mock-key":
            return {"lat": 42.6977, "lng": 23.3219, "verified": False}
        
        # Real implementation will go here
        return {}


=========================================
FILE: ./src/__init__.py
=========================================


=========================================
FILE: ./src/main.py
=========================================
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from src.api import routes

app = FastAPI(
    title="Glashaus API",
    description="Automated Real Estate Due Diligence Engine",
    version="1.0.0"
)

# Allow connections from Frontend/Dashboard
origins = [
    "http://localhost",
    "http://localhost:3000",
    "http://localhost:8080",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(routes.router)

@app.get("/")
def health_check():
    return {
        "system": "GLASHAUS", 
        "status": "OPERATIONAL", 
        "version": "1.0.0-PROD"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("src.main:app", host="0.0.0.0", port=8000)


=========================================
FILE: ./src/db/session.py
=========================================
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
from src.core.config import settings

# SQLite requires a specific argument for threading
connect_args = {"check_same_thread": False} if "sqlite" in settings.DATABASE_URL else {}

engine = create_engine(
    settings.DATABASE_URL, 
    connect_args=connect_args,
    pool_pre_ping=True
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


=========================================
FILE: ./src/db/models.py
=========================================
import enum
from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, ForeignKey, JSON, Text, Enum
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from src.db.session import Base

class ReportStatus(set, enum.Enum):
    PENDING = "PENDING"
    PROCESSING = "PROCESSING"
    VERIFIED = "VERIFIED"
    MANUAL_REVIEW = "MANUAL_REVIEW"
    REJECTED = "REJECTED"

class Building(Base):
    __tablename__ = "buildings"

    id = Column(Integer, primary_key=True, index=True)
    cadastre_id = Column(String, unique=True, nullable=False, index=True)
    address_street = Column(String)
    address_number = Column(String)
    neighborhood = Column(String)
    gps_coordinates = Column(String) 
    construction_year = Column(Integer)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    reports = relationship("Report", back_populates="building")

class Listing(Base):
    __tablename__ = "listings"

    id = Column(Integer, primary_key=True, index=True)
    source_url = Column(String, unique=True, nullable=False, index=True)
    content_hash = Column(String(64), index=True) # For idempotency
    price_bgn = Column(Float)
    advertised_area_sqm = Column(Float)
    description_raw = Column(Text)
    scraped_at = Column(DateTime(timezone=True), server_default=func.now())

    reports = relationship("Report", back_populates="listing")

class Report(Base):
    __tablename__ = "reports"

    id = Column(Integer, primary_key=True, index=True)
    listing_id = Column(Integer, ForeignKey("listings.id"))
    building_id = Column(Integer, ForeignKey("buildings.id"), nullable=True)
    
    status = Column(Enum(ReportStatus), default=ReportStatus.PENDING)
    risk_score = Column(Integer)  # 0-100
    ai_confidence_score = Column(Integer, default=0)
    
    is_address_verified = Column(Boolean, default=False)
    discrepancy_details = Column(JSON)
    manual_review_notes = Column(Text)
    cost_to_generate = Column(Float)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    listing = relationship("Listing", back_populates="reports")
    building = relationship("Building", back_populates="reports")


=========================================
FILE: ./src/worker.py
=========================================
import os
from celery import Celery
from src.core.config import settings

celery_app = Celery(
    "glashaus_worker",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL
)

celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="Europe/Sofia",
    enable_utc=True,
)

# Auto-discover tasks in src/tasks.py
celery_app.autodiscover_tasks(['src.tasks'])


=========================================
FILE: ./src/tasks.py
=========================================
from src.worker import celery_app
from src.db.session import SessionLocal
from src.db.models import Listing, Report, ReportStatus
from src.services.repository import RealEstateRepository
from src.services.scraper_service import ScraperService
from src.services.ai_engine import GeminiService
from src.services.risk_engine import RiskEngine
from src.services.cadastre_service import CadastreService
from src.services.geospatial_service import GeospatialService
from src.core.config import settings
from src.core.utils import calculate_content_hash, normalize_address
import asyncio

@celery_app.task(name="src.tasks.audit_listing")
def audit_listing_task(listing_id: int):
    db = SessionLocal()
    try:
        repo = RealEstateRepository(db)
        
        # Initialize Services
        scraper = ScraperService(simulation_mode=(settings.GEMINI_API_KEY == "mock-key"))
        ai_engine = GeminiService(api_key=settings.GEMINI_API_KEY)
        risk_engine = RiskEngine()
        cadastre_service = CadastreService()
        geo_service = GeospatialService() # Ready for future use

        listing = db.query(Listing).get(listing_id)
        if not listing: return "Listing not found"

        # 1. SCRAPE
        print(f"[TASK] Scraping listing {listing_id}...")
        scraped_data = scraper.scrape_url(listing.source_url)
            
        # 2. IDEMPOTENCY
        current_hash = calculate_content_hash(scraped_data["raw_text"], listing.price_bgn or 0)
        duplicate_id = repo.check_content_duplication(current_hash)
        if duplicate_id and duplicate_id != listing.id:
            repo.update_listing_content(listing.id, scraped_data["raw_text"], current_hash)
            return "Aborted: Duplicate Content"
        repo.update_listing_content(listing.id, scraped_data["raw_text"], current_hash)

        # 3. AI ANALYSIS (Async in Sync Worker)
        loop = asyncio.get_event_loop()
        if loop.is_closed():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
        print(f"[TASK] Running AI Tier 1...")
        tier1_result = loop.run_until_complete(ai_engine.analyze_text(scraped_data["raw_text"]))
        
        # 4. EXTERNAL DATA (Cadastre)
        address_found = tier1_result.get("address_prediction", "")
        clean_address = normalize_address(address_found)
        cadastre_data = cadastre_service.fetch_details(clean_address)
        
        official_area = cadastre_data.get("official_area") if cadastre_data else None

        # 5. TIER 2 VISION (Conditional)
        confidence = tier1_result.get("confidence", 0)
        final_ai_data = tier1_result
        total_cost = 0.04

        # Trigger Vision if confidence is low OR if it looks like an Atelier
        if (confidence < 80 or final_ai_data.get("is_atelier")) and settings.GEMINI_API_KEY != "mock-key":
            print(f"[TASK] Escalating to Tier 2 Vision...")
            tier2_result = loop.run_until_complete(ai_engine.analyze_images(scraped_data["image_urls"]))
            final_ai_data["vision_insights"] = tier2_result
            total_cost += 0.22
            # Boost confidence if vision found landmarks
            if tier2_result.get("estimated_location_confidence", 0) > 50:
                confidence = max(confidence, 85)

        # 6. RISK CALCULATION
        report_data = risk_engine.calculate_score(
            scraped_data, 
            final_ai_data, 
            cadastre_area=official_area
        )
        
        repo.create_report(
            listing_id=listing.id,
            risk=report_data["score"],
            details={"flags": report_data["flags"], "ai_meta": final_ai_data, "cadastre_match": bool(cadastre_data)},
            cost=total_cost,
            confidence=confidence
        )
        return "Audit Complete"

    except Exception as e:
        print(f"[TASK_ERROR] {str(e)}")
        # FAIL-SAFE: Update DB to REJECTED so UI doesn't hang
        try:
            # Re-connect in case main session is dead
            error_db = SessionLocal()
            error_report = error_db.query(Report).filter(Report.listing_id == listing_id).first()
            if not error_report:
                error_report = Report(listing_id=listing_id, status=ReportStatus.REJECTED)
                error_db.add(error_report)
            else:
                error_report.status = ReportStatus.REJECTED
            
            error_report.manual_review_notes = f"System Error: {str(e)}"
            error_db.commit()
            error_db.close()
        except Exception as db_e:
            print(f"[CRITICAL DB ERROR] {db_e}")
            
        return f"Failed: {str(e)}"
    finally:
        db.close()


=========================================
FILE: ./scripts/context_dump.sh
=========================================
#!/bin/bash
# Scans the repo and dumps text files for LLM context
output="glashaus_context.txt"
echo "--- GLASHAUS PROJECT DUMP ---" > "$output"
date >> "$output"

echo -e "\n\n--- GIT HISTORY ---" >> "$output"
git log --oneline --graph --decorate -n 20 >> "$output"

echo -e "\n\n--- FILE STRUCTURE ---" >> "$output"
tree -L 3 -I '.git|__pycache__|*.pyc' >> "$output" 2>/dev/null || find . -maxdepth 3 -not -path '*/.*' >> "$output"

echo -e "\n\n--- FILE CONTENTS ---" >> "$output"
find . -type f \
    -not -path '*/.*' \
    -not -path './glashaus_context.txt' \
    -not -name '*.png' \
    -not -name '*.jpg' \
    -not -name '*.sqlite' \
    | while read -r file; do
    echo -e "\n\n=========================================" >> "$output"
    echo "FILE: $file" >> "$output"
    echo "=========================================" >> "$output"
    cat "$file" >> "$output"
done

echo "Dump complete. Copy contents of $output"


=========================================
FILE: ./prompts/detective_prompt_v1.md
=========================================
# Role: Geospatial Detective
You are an expert Investigator. Your task is to identify the specific building address of a real estate listing based on limited metadata and photos.

## Input Data
- **Listing Text:** {text_raw}
- **Images:** {image_list}
- **Neighborhood Hint:** {neighborhood}

## Logic Constraints
1. **Fact vs Guess:** Distinguish between explicitly stated streets and visual deductions.
2. **The "Red House" Rule:** Look for unique landmarks in the "View from Window" photos.
3. **Facade Matching:** Describe the balcony curve/color and estimate construction era.

## Output Format (JSON Only)
{
  "address_prediction": "String",
  "confidence": 0-100,
  "reasoning_steps": [
    "Identified beige facade matching 2003 construction style.",
    "Located restaurant 'Chefs' on ground floor visible in photo 3.",
    "Triangulated address to Ul. Lyubata 13."
  ],
  "unit_type_discrepancy": "Apartment vs Atelier" (if applicable)
}


=========================================
FILE: ./db/schema_v1.sql
=========================================
-- Enable GIS extensions for location logic
CREATE EXTENSION IF NOT EXISTS postgis;

CREATE TABLE buildings (
    id SERIAL PRIMARY KEY,
    cadastre_id VARCHAR(50) UNIQUE NOT NULL, -- The official identifier
    address_street VARCHAR(255),
    address_number VARCHAR(50),
    neighborhood VARCHAR(100),
    gps_coordinates GEOMETRY(Point, 4326),
    construction_year INT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE listings (
    id SERIAL PRIMARY KEY,
    source_url TEXT UNIQUE NOT NULL,
    price_bgn DECIMAL(12, 2),
    advertised_area_sqm DECIMAL(10, 2),
    description_raw TEXT,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE reports (
    id SERIAL PRIMARY KEY,
    listing_id INT REFERENCES listings(id),
    building_id INT REFERENCES buildings(id),
    risk_score INT, -- 0 to 100 (100 is Toxic)
    is_address_verified BOOLEAN DEFAULT FALSE,
    discrepancy_details JSONB, -- Stores the "Apartment vs Atelier" logic
    cost_to_generate DECIMAL(10, 4) -- Tracking the $0.22 API cost
);


=========================================
FILE: ./db/migration_001_status_workflow.sql
=========================================
-- 1. Create the Workflow Status Enum
-- This supports the PENDING -> VERIFIED -> MANUAL_REVIEW flow
DO $$ BEGIN
    CREATE TYPE report_status AS ENUM ('PENDING', 'PROCESSING', 'VERIFIED', 'MANUAL_REVIEW', 'REJECTED');
EXCEPTION
    WHEN duplicate_object THEN null;
END $$;

-- 2. Update 'reports' table
ALTER TABLE reports 
ADD COLUMN IF NOT EXISTS status report_status DEFAULT 'PENDING',
ADD COLUMN IF NOT EXISTS ai_confidence_score INT DEFAULT 0,
ADD COLUMN IF NOT EXISTS manual_review_notes TEXT;

-- 3. Update 'listings' table for Idempotency
-- We hash the file/content to prevent duplicate processing of the same upload
ALTER TABLE listings
ADD COLUMN IF NOT EXISTS content_hash VARCHAR(64);

CREATE INDEX IF NOT EXISTS idx_listings_content_hash ON listings(content_hash);

-- 4. Create the Manual Review Queue View
-- This allows the Admin Panel to easily select tasks needing human eyes
CREATE OR REPLACE VIEW view_manual_review_queue AS
SELECT 
    r.id as report_id,
    l.source_url,
    r.ai_confidence_score,
    r.risk_score,
    r.created_at
FROM reports r
JOIN listings l ON r.listing_id = l.id
WHERE r.status = 'MANUAL_REVIEW'
ORDER BY r.risk_score DESC;


=========================================
FILE: ./db/migrations/env.py
=========================================
from logging.config import fileConfig
from sqlalchemy import engine_from_config
from sqlalchemy import pool
from alembic import context
import os
import sys

# Add src to path so we can import models
sys.path.append(os.getcwd())

from src.db.session import Base
from src.core.config import settings
from src.db.models import Listing, Report, Building 

config = context.config

if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Overwrite config URL with Environment Settings
config.set_main_option("sqlalchemy.url", settings.DATABASE_URL)

target_metadata = Base.metadata

def run_migrations_offline() -> None:
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online() -> None:
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()


=========================================
FILE: ./README.md
=========================================
# GLASHAUS: The Real Estate Integrity Engine

## Mission
To eliminate information asymmetry in the Sofia real estate market via automated due diligence.
We leverage OSINT, LLM reasoning, and Official Registry cross-referencing.

## Architecture
- **Text Layer:** Gemini Flash (Cost optimized)
- **Vision Layer:** Gemini Pro (Geospatial reasoning)
- **Data Layer:** PostgreSQL (Structured) + S3 (Archives)

## Status
- **Phase:** Pre-Alpha / Architectural Blueprint
- **Deploy Target:** Jan 2026 (Launch)


=========================================
FILE: ./requirements.txt
=========================================
fastapi==0.109.0
uvicorn==0.27.0
sqlalchemy==2.0.25
psycopg2-binary==2.9.9
httpx==0.26.0
pydantic==2.6.0
pydantic-settings==2.1.0
google-generativeai>=0.7.0
beautifulsoup4==4.12.3
celery==5.3.6
redis==5.0.1
alembic==1.13.1
Pillow==10.2.0


=========================================
FILE: ./imot_simulation.html
=========================================
<!DOCTYPE html>
<html>
<body>
    <div class="list_ads">
        <!-- Mock Listing 1 -->
        <a href="//www.imot.bg/pcgi/imot.cgi?act=5&adv=1c171899111&slink=a4eg0c&f1=1" class="photoLink">
            <div class="text_desc">
                2-STAEN, Sofia, Lozenets, 185 000 EUR
            </div>
        </a>
        
        <!-- Mock Listing 2 -->
        <a href="//www.imot.bg/pcgi/imot.cgi?act=5&adv=2c172200231&slink=a4eg0c&f1=1" class="photoLink">
            <div class="text_desc">
                3-STAEN, Sofia, Krustova Vada, 250 000 EUR, Gas/Elevator
            </div>
        </a>

        <!-- Mock Listing 3 -->
        <a href="//www.imot.bg/pcgi/imot.cgi?act=5&adv=3c17992881&slink=a4eg0c&f1=1" class="photoLink">
            <div class="text_desc">
                ATELIER, Sofia, Center, 90 000 EUR
            </div>
        </a>
    </div>
</body>
</html>


=========================================
FILE: ./Dockerfile
=========================================
# Use Official Python Runtime
FROM python:3.11-slim

# Set Working Directory
WORKDIR /app

# Install System Dependencies (Postgres + GIS libs)
RUN apt-get update && apt-get install -y \
    libpq-dev gcc netcat-openbsd \
    && rm -rf /var/lib/apt/lists/*

# Install Python Dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy Code
COPY . .

# Expose Port
EXPOSE 8000

# Start Command (Wait for DB, then launch)
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]


=========================================
FILE: ./docker-compose.yml
=========================================
version: '3.8'

services:
  # 1. GATEWAY (Entry Point)
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - api

  # 2. API (Internal)
  api:
    build: .
    expose:
      - "8000" # Not mapped to host anymore, only accessible via Nginx
    environment:
      - POSTGRES_SERVER=db
      - POSTGRES_USER=glashaus_user
      - POSTGRES_PASSWORD=secret_password
      - POSTGRES_DB=glashaus_db
      - REDIS_URL=redis://redis:6379/0
      # - GEMINI_API_KEY=${GEMINI_API_KEY} # Uncomment for prod
    depends_on:
      - db
      - redis
    volumes:
      - ./src:/app/src

  # 3. WORKER
  worker:
    build: .
    command: celery -A src.worker.celery_app worker --loglevel=info
    environment:
      - POSTGRES_SERVER=db
      - POSTGRES_USER=glashaus_user
      - POSTGRES_PASSWORD=secret_password
      - POSTGRES_DB=glashaus_db
      - REDIS_URL=redis://redis:6379/0
      # - GEMINI_API_KEY=${GEMINI_API_KEY}
    depends_on:
      - db
      - redis
    volumes:
      - ./src:/app/src

  # 4. REDIS
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  # 5. DB
  db:
    image: postgis/postgis:15-3.4
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=glashaus_user
      - POSTGRES_PASSWORD=secret_password
      - POSTGRES_DB=glashaus_db
    ports:
      - "5432:5432"

volumes:
  postgres_data:


=========================================
FILE: ./tests/test_api.py
=========================================
from fastapi.testclient import TestClient
from src.main import app

client = TestClient(app)

def test_health_check():
    response = client.get("/")
    assert response.status_code == 200
    assert response.json()["status"] == "OPERATIONAL"

def test_audit_flow():
    payload = {"url": "https://www.imot.bg/pcgi/imot.cgi?act=5&adv=mock123"}
    response = client.post("/audit", json=payload)
    
    assert response.status_code == 200
    data = response.json()
    assert data["status"] == "QUEUED"
    assert "listing_id" in data


=========================================
FILE: ./alembic.ini
=========================================
[alembic]
script_location = db/migrations
prepend_sys_path = .
sqlalchemy.url = driver://user:pass@localhost/dbname

[post_write_hooks]

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S


=========================================
FILE: ./nginx/nginx.conf
=========================================
events {}

http {
    upstream glashaus_api {
        server api:8000;
    }

    server {
        listen 80;
        
        # Proxy all API requests to the FastAPI container
        location / {
            proxy_pass http://glashaus_api;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        # Health check endpoint
        location /health {
            return 200 'alive';
            add_header Content-Type text/plain;
        }
    }
}
