--- GLASHAUS PROJECT DUMP ---
Fri Dec 19 11:33:39 EET 2025


--- GIT HISTORY ---
* 2e8f2fa (HEAD -> main) refactor(core): harden backend architecture and implement forensic logic
* f639780 feat(legal): implement legal forensic engine and solicitor report generation
* 6489783 feat(core): harden task resilience, abstract providers, and add storage layer
* a087b30 feat(backend): implement vision layer, harden worker logic, and add regex parsing
* 4987d66 feat(infra): add Nginx gateway, CORS, and live scraping logic
* ae76a80 refactor(core): upgrade architecture to Celery/Redis and harden AI logic
* b7f3797 refactor(core): migrate to Redis/Celery queue and add Alembic migrations
* f22a7da feat(core): implement AI engine, workflow logic, and docs
* e847d18 feat(backend): complete ORM models, repo pattern, and test suite
* d266a4d ops: add docker containerization and orchestration
* 07beab5 feat(scraper): implement simulation mode to bypass WAF during dev
* 8d13095 chore(scripts): include git log in context dumper
* 15c8316 refactor(api): connect routes to database via dependency injection
* 84bff79 feat(db): implement SQLAlchemy ORM models and Repository pattern
* 648c703 feat(core): implement abstract AI service layer
* 2e3b2a8 docs: add German functional specification (Lastenheft)
* 0307ac8 feat: init project structure, docs, and database schema


--- FILE STRUCTURE ---
.
./docs
./docs/ARCHITECTURE.md
./docs/api_contract.yaml
./docs/LASTENHEFT_DE.md
./docs/TECHNICAL_DEBT.md
./src
./src/api
./src/api/routes.py
./src/core
./src/core/config.py
./src/core/utils.py
./src/core/sofia_data.py
./src/services
./src/services/ai_engine.py
./src/services/repository.py
./src/services/scraper_mvp.py
./src/services/scraper_service.py
./src/services/risk_engine.py
./src/services/cadastre_service.py
./src/services/geospatial_service.py
./src/services/base_provider.py
./src/services/storage_service.py
./src/services/legal_engine.py
./src/services/report_generator.py
./src/models
./src/__init__.py
./src/main.py
./src/db
./src/db/session.py
./src/db/models.py
./src/worker.py
./src/tasks.py
./scripts
./scripts/context_dump.sh
./prompts
./prompts/detective_prompt_v1.md
./db
./db/schema_v1.sql
./db/migration_001_status_workflow.sql
./db/migrations
./db/migrations/env.py
./README.md
./glashaus_context.txt
./requirements.txt
./imot_simulation.html
./Dockerfile
./docker-compose.yml
./tests
./tests/test_api.py
./alembic.ini
./nginx
./nginx/nginx.conf
./storage
./storage/archive


--- FILE CONTENTS ---


=========================================
FILE: ./docs/ARCHITECTURE.md
=========================================
# System Architecture

## Core Logic Flow (Cost Optimized)
1. **Ingest:** Scraper Service fetches URL.
2. **Analysis Tier 1 (Cheap):** Text Extraction (Gemini Flash).
   - *Goal:* Find Address in text.
   - *Cost:* ~$0.04/run.
3. **Gatekeeper:** Confidence Check (>90%).
4. **Analysis Tier 2 (Expensive):** Visual Detective (Gemini Pro).
   - *Trigger:* Only if Tier 1 fails.
   - *Goal:* Identify building via Facade/Landmarks/Geofencing.
   - *Cost:* ~$0.22/run.
5. **Verification:** Cross-reference extracted Address vs Cadastre API.

## Tech Stack
- **Lang:** Python (FastAPI) or Java (Spring Boot) - TBD
- **DB:** PostgreSQL + PostGIS (Geospatial extensions)
- **Queue:** Redis (Task offloading)


=========================================
FILE: ./docs/api_contract.yaml
=========================================
openapi: 3.0.0
info:
  title: Glashaus API
  version: 0.1.0
paths:
  /audit/url:
    post:
      summary: Initiate an Audit for a specific Listing URL
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                url:
                  type: string
                  format: uri
      responses:
        '200':
          description: Audit Complete
          content:
            application/json:
              schema:
                type: object
                properties:
                  report_id:
                    type: integer
                  risk_score:
                    type: integer
                  verified_address:
                    type: string
                  ai_reasoning:
                    type: array
                    items:
                      type: string


=========================================
FILE: ./docs/LASTENHEFT_DE.md
=========================================
# LASTENHEFT: Projekt Glashaus
**Version:** 1.0.0
**Status:** In Entwicklung

## 1. Einleitung
Das Projekt "Glashaus" ist eine automatisierte Due-Diligence-Plattform fÃ¼r den Immobilienmarkt in Sofia. Ziel ist die Beseitigung von Informationsasymmetrien durch den Einsatz von OSINT und KI-gestÃ¼tzter Datenanalyse.

## 2. Ist-Zustand (Problemstellung)
- **Datenfragmentierung:** Grundbuch (Registry), Kataster (Cadastre) und Gemeinde agieren in Silos.
- **Intransparenz:** Immobilienanzeigen enthalten oft ungenaue FlÃ¤chenangaben und verschleierte Adressen.
- **Prozessineffizienz:** Manuelle PrÃ¼fungen sind teuer und langsam.

## 3. Soll-Zustand (LÃ¶sung)
Ein Microservices-System, das folgende Kernfunktionen bietet:
1.  **Automatische Adress-Deduktion:** Ermittlung der exakten Adresse aus unstrukturierten Anzeigentexten und Bildern.
2.  **Soll/Ist-Abgleich:** Automatischer Vergleich von Maklerangaben (Anzeige) mit amtlichen Katasterdaten.
3.  **Risikobewertung:** Algorithmische Berechnung eines "Risk Scores" (0-100).

## 4. Technische Anforderungen
- **Architektur:** Event-Driven Microservices (Python/FastAPI).
- **Datenbank:** PostgreSQL mit PostGIS fÃ¼r Geodatenverarbeitung.
- **KI-Integration:**
    - *Tier 1:* Textanalyse (Low Cost / Gemini Flash).
    - *Tier 2:* Visuelle Analyse (High Cost / Gemini Pro).

## 5. Nicht-funktionale Anforderungen
- **Idempotenz:** Wiederholte Uploads dÃ¼rfen keine Datenkorruption verursachen.
- **Skalierbarkeit:** Das System muss Warteschlangen (Queues) nutzen, um Lastspitzen bei Scrapern abzufangen.


=========================================
FILE: ./docs/TECHNICAL_DEBT.md
=========================================
# TECHNICAL DEBT LOG

## Critical Severity (Must Fix Before Beta)

### 1. Database Session Scope in Background Tasks
- **Location:** `src/api/routes.py` -> `initiate_audit`
- **Issue:** Passing the dependency-injected `db` session to `BackgroundTasks` causes `DetachedInstanceError` because the session closes when the HTTP response returns.
- **Fix:** Refactor `process_audit_task` to instantiate a fresh `SessionLocal()` context manager internally.
- **Reference:** SQLAlchemy Thread-Safety docs.

### 2. AI Service Implementation
- **Location:** `src/services/ai_engine.py`
- **Issue:** Currently returns mock dictionary `{"confidence": 0.0}`.
- **Fix:** 
    - Initialize `genai.configure(api_key=...)`.
    - Implement `generate_content` call for Gemini Flash (Text).
    - Implement `generate_content` with Image inputs for Gemini Pro (Vision).
    - Add Error Handling for "Safety Filters" or API Quotas.

## Moderate Severity (Logic Gaps)

### 3. Risk Scoring Algorithm
- **Location:** Database Schema exists (`risk_score`), but logic is missing.
- **Issue:** No calculator exists to translate discrepancies into a 0-100 integer.
- **Fix:** Create `src/services/risk_engine.py`.
    - Base Score: 0
    - If `advertised_area` > `cadastre_area` (+20 pts).
    - If `type` mismatch (Atelier vs Apt) (+30 pts).
    - If `price_per_sqm` > 1.5x avg (+15 pts).

### 4. Hardcoded Secrets
- **Location:** `src/core/config.py`
- **Issue:** Default "mock-key" risks silent failure in prod.
- **Fix:** Implement `pydantic` validation to raise `ValueError` on startup if `GEMINI_API_KEY` is missing in production environment.

## Low Severity (Optimization)

### 5. Listing Normalization
- **Location:** `src/services/repository.py`
- **Issue:** Duplicate URLs might occur if query params differ (e.g., `?adv=1` vs `?adv=1&utm=facebook`).
- **Fix:** Implement a URL cleaner utility to strip tracking parameters before hashing/storing.


=========================================
FILE: ./src/api/routes.py
=========================================
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from src.db.session import get_db
from src.db.models import Listing, Report, ReportStatus
from src.services.repository import RealEstateRepository
from src.tasks import audit_listing_task
from pydantic import BaseModel
from typing import Optional

router = APIRouter()

class AuditRequest(BaseModel):
    url: str
    price_override: float = 0.0

class ReportUpdate(BaseModel):
    status: str
    manual_notes: Optional[str] = None

@router.post("/audit")
async def initiate_audit(request: AuditRequest, db: Session = Depends(get_db)):
    """
    Submits a URL for auditing via the Celery Worker queue.
    """
    repo = RealEstateRepository(db)
    # Create listing immediately to return ID
    listing = repo.create_listing(url=request.url, price=request.price_override, area=0.0, desc="Queued")
    
    # Offload to Redis/Celery
    audit_listing_task.delay(listing.id)
    
    return {"listing_id": listing.id, "status": "QUEUED_IN_REDIS"}

@router.get("/reports/{listing_id}")
def get_report(listing_id: int, db: Session = Depends(get_db)):
    """
    Retrieves the report status and details for a listing.
    """
    listing = db.query(Listing).filter(Listing.id == listing_id).first()
    if not listing:
        raise HTTPException(status_code=404, detail="Listing not found")
        
    report = db.query(Report).filter(Report.listing_id == listing_id).first()
    if not report:
        # If no report exists yet, the worker is likely still processing
        return {"status": "PROCESSING", "details": "Audit is currently in the queue."}
        
    return {
        "report_id": report.id,
        "status": report.status,
        "risk_score": report.risk_score,
        "ai_confidence": report.ai_confidence_score,
        "discrepancies": report.discrepancy_details,
        "manual_notes": report.manual_review_notes,
        "cost": report.cost_to_generate,
        "created_at": report.created_at
    }

@router.patch("/reports/{report_id}")
def update_report_status(report_id: int, update: ReportUpdate, db: Session = Depends(get_db)):
    """
    Manual Review Action.
    Updates status (e.g., MANUAL_REVIEW -> VERIFIED).
    """
    report = db.query(Report).filter(Report.id == report_id).first()
    if not report:
        raise HTTPException(status_code=404, detail="Report not found")
        
    try:
        # Validate that the string provided matches the Enum
        new_status = ReportStatus(update.status)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid Status Enum")

    report.status = new_status
    if update.manual_notes:
        report.manual_review_notes = update.manual_notes
        
    db.commit()
    return {"id": report.id, "new_status": report.status}


=========================================
FILE: ./src/core/config.py
=========================================
import os

class Settings:
    PROJECT_NAME: str = "Glashaus"
    VERSION: str = "0.1.0"
    
    GEMINI_API_KEY: str = os.getenv("GEMINI_API_KEY", "mock-key")

    POSTGRES_USER: str = os.getenv("POSTGRES_USER", "postgres")
    POSTGRES_SERVER: str = os.getenv("POSTGRES_SERVER", "")
    POSTGRES_DB: str = os.getenv("POSTGRES_DB", "glashaus")
    POSTGRES_PASSWORD: str = os.getenv("POSTGRES_PASSWORD", "postgres")

    # NEW: Redis Configuration for Celery
    REDIS_URL: str = os.getenv("REDIS_URL", "redis://redis:6379/0")
    
    @property
    def DATABASE_URL(self) -> str:
        if not self.POSTGRES_SERVER:
            return "sqlite:///./glashaus.db"
        return f"postgresql://{self.POSTGRES_USER}:{self.POSTGRES_PASSWORD}@{self.POSTGRES_SERVER}/{self.POSTGRES_DB}"

settings = Settings()

if settings.GEMINI_API_KEY == "mock-key":
    print("--- WARNING: GEMINI_API_KEY is not set. Using MOCK mode. ---")


=========================================
FILE: ./src/core/utils.py
=========================================
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
import hashlib
import re

def normalize_url(url: str) -> str:
    """
    Strips tracking parameters and fragments.
    """
    u = urlparse(url)
    query = dict(parse_qsl(u.query))
    whitelist = {'act', 'adv', 'id', 'slink'}
    clean_query = {k: v for k, v in query.items() if k.lower() in whitelist}
    return urlunparse((
        u.scheme, u.netloc, u.path, u.params,
        urlencode(clean_query), ''
    ))

def calculate_content_hash(text: str, price: float) -> str:
    """
    Generates a SHA256 hash of the description + price.
    Used for Idempotency to detect if a bumped listing is actually identical.
    """
    # Normalize text to ignore whitespace diffs
    clean_text = re.sub(r'\s+', ' ', text).strip().lower()
    raw = f"{clean_text}{price}".encode('utf-8')
    return hashlib.sha256(raw).hexdigest()

def normalize_address(raw_address: str) -> str:
    """
    Standardizes Bulgarian addresses for cross-referencing.
    Input: "ul. 'Pirotska' No 5, et. 2" -> "PIROTSKA 5"
    """
    if not raw_address:
        return "UNKNOWN"
        
    # 1. Transliterate/Cleanup (Simplified for this example)
    clean = raw_address.lower()
    
    # 2. Remove common prefixes
    prefixes = [r"ul\.", r"ulitsa", r"str\.", r"street", r"bulevard", r"bul\.", r"bl\.", r"block"]
    for p in prefixes:
        clean = re.sub(p, "", clean)
        
    # 3. Remove punctuation
    clean = re.sub(r"[,'\"\.]", " ", clean)
    
    # 4. Extract Street Name and Number
    # Looking for: (Text) ... (Number)
    # This is a heuristic; production needs a better geocoder
    match = re.search(r"([a-z\s]+)\s+(\d+)", clean)
    if match:
        street = match.group(1).strip().upper()
        number = match.group(2)
        return f"{street} {number}"
        
    return clean.strip().upper()


=========================================
FILE: ./src/core/sofia_data.py
=========================================
SOFIA_ADMIN_MAP = {
    "OBORISHTE": {"strictness": 5, "kindergarten_risk": "Critical"},
    "PODUYANE": {"strictness": 5, "kindergarten_risk": "High"},
    "CENTER": {"strictness": 4, "kindergarten_risk": "High"},
    "LOZENETS": {"strictness": 4, "kindergarten_risk": "Moderate"},
    "VITOSHA": {"strictness": 2, "kindergarten_risk": "Low"},
    "KRUSTOVA VADA": {"strictness": 2, "kindergarten_risk": "Low"},
}


=========================================
FILE: ./src/services/ai_engine.py
=========================================
import google.generativeai as genai
from pydantic import BaseModel, Field
from src.core.config import settings

class AIAnalysisSchema(BaseModel):
    address_prediction: str = Field(description="Extracted address")
    confidence: int = Field(description="0-100 score")
    neighborhood: str = Field(description="Rayon name")
    is_atelier: bool = Field(description="Legal status is atelier")
    net_living_area: float = Field(description="Usable square meters")
    construction_year: int = Field(description="Finish year")

class GeminiService:
    def __init__(self, api_key: str):
        if api_key != "mock-key":
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel('gemini-1.5-flash')
        else: self.model = None

    async def analyze_text(self, text: str) -> dict:
        if not self.model: return {"address_prediction": "Mock", "confidence": 0, "neighborhood": "Unknown", "is_atelier": False, "net_living_area": 0, "construction_year": 2024}
        prompt = f"Perform forensic property analysis on: {text}"
        resp = self.model.generate_content(prompt, generation_config={"response_mime_type": "application/json", "response_json_schema": AIAnalysisSchema.model_json_schema()})
        return AIAnalysisSchema.model_validate_json(resp.text).model_dump()


=========================================
FILE: ./src/services/repository.py
=========================================
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.orm import Session
from src.db.models import Listing, Report, PriceHistory
from src.core.utils import normalize_url

class RealEstateRepository:
    def __init__(self, db: Session):
        self.db = db

    def create_listing_initial(self, url: str) -> Listing:
        clean_url = normalize_url(url)
        existing = self.db.query(Listing).filter(Listing.source_url == clean_url).first()
        if existing: return existing
        new_l = Listing(source_url=clean_url)
        self.db.add(new_l)
        self.db.commit()
        self.db.refresh(new_l)
        return new_l

    def update_listing_data(self, listing_id: int, price: float, area: float, desc: str, chash: str):
        listing = self.db.query(Listing).get(listing_id)
        if listing:
            if listing.price_bgn and listing.price_bgn != price:
                history = PriceHistory(listing_id=listing_id, price_bgn=listing.price_bgn)
                self.db.add(history)
            listing.price_bgn = price
            listing.advertised_area_sqm = area
            listing.description_raw = desc
            listing.content_hash = chash
            self.db.commit()


=========================================
FILE: ./src/services/scraper_mvp.py
=========================================
from bs4 import BeautifulSoup
import os

# CONFIG
SIMULATION_MODE = True
MOCK_FILE = "imot_simulation.html"

def run_recon():
    print("[*] INTEL: Starting Reconnaissance Protocol...")
    
    html_content = ""
    
    if SIMULATION_MODE:
        print(f"[*] MODE: SIMULATION (Bypassing WAF)")
        if not os.path.exists(MOCK_FILE):
            print(f"[!] Error: Mock file {MOCK_FILE} not found.")
            return
            
        with open(MOCK_FILE, "r", encoding="utf-8") as f:
            html_content = f.read()
    else:
        # Network logic removed for Termux Safety
        pass

    soup = BeautifulSoup(html_content, 'html.parser')
    links = soup.find_all('a', href=True)
    
    print("[*] Parsing DOM Structure...")
    
    count = 0
    listings_found = []
    
    for link in links:
        href = link['href']
        
        if 'act=5' in href:
            # Normalize URL
            full_url = "https:" + href if href.startswith("//") else href
            
            if full_url in listings_found:
                continue
                
            listings_found.append(full_url)
            text_content = link.get_text(separator=" ", strip=True)
            
            count += 1
            print(f"\n[TARGET #{count}]")
            print(f"   URL: {full_url}")
            print(f"   RAW: {text_content}")

    print(f"\n[*] Mission Complete. {count} mock targets extracted.")

if __name__ == "__main__":
    run_recon()


=========================================
FILE: ./src/services/scraper_service.py
=========================================
from bs4 import BeautifulSoup
import os
import httpx
import random
import re

class ScraperService:
    def __init__(self, simulation_mode=False):
        self.simulation = simulation_mode
        self.mock_file = "imot_simulation.html"
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
        ]

    def _parse_price(self, text: str) -> float:
        # Matches: 185 000 EUR, 185000, 185.000
        match = re.search(r'([\d\s\.,]+)\s?(?:EUR|â‚¬|leva|BGN)', text, re.IGNORECASE)
        if match:
            clean = match.group(1).replace(" ", "").replace(",", "").strip()
            try:
                return float(clean)
            except ValueError:
                return 0.0
        return 0.0

    def _parse_area(self, text: str) -> float:
        # Matches: 85 kv.m, 85 m2, 85 ÐºÐ².Ð¼
        match = re.search(r'([\d\.,]+)\s?(?:kv|m2|ÐºÐ²)', text, re.IGNORECASE)
        if match:
            clean = match.group(1).replace(",", ".").strip()
            try:
                return float(clean)
            except ValueError:
                return 0.0
        return 0.0

    def scrape_url(self, url: str) -> dict:
        print(f"[SCRAPER] Target: {url} | Mode: {'SIMULATION' if self.simulation else 'LIVE'}")
        
        html_content = ""
        
        if self.simulation:
            if not os.path.exists(self.mock_file):
                return {"raw_text": "Simulation Error: File missing", "image_urls": [], "price_predicted": 0, "area": 0}
            with open(self.mock_file, "r", encoding="utf-8") as f:
                html_content = f.read()
        else:
            try:
                headers = {"User-Agent": random.choice(self.user_agents)}
                with httpx.Client(timeout=10.0, follow_redirects=True) as client:
                    response = client.get(url, headers=headers)
                    response.raise_for_status()
                    html_content = response.text
            except Exception as e:
                print(f"[SCRAPER] Network Error: {e}")
                raise e

        soup = BeautifulSoup(html_content, 'html.parser')
        
        # 1. Extract Text
        ad_block = soup.find('div', class_='text_desc') or soup.find('div', class_='ad-description')
        raw_text = ad_block.get_text(separator=" ", strip=True) if ad_block else ""
        
        if not raw_text and not self.simulation:
            raw_text = soup.body.get_text(separator=" ", strip=True)[:2000]

        # 2. Extract Images (Limit 3)
        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src and 'http' in src and ('jpg' in src or 'png' in src):
                images.append(src)
        images = images[:3]
        
        # 3. Parse Metadata
        price = self._parse_price(raw_text)
        area = self._parse_area(raw_text)

        return {
            "source_url": url,
            "raw_text": raw_text,
            "image_urls": images,
            "price_predicted": price,
            "area": area
        }


=========================================
FILE: ./src/services/risk_engine.py
=========================================
from typing import Dict, Any, Optional

class RiskEngine:
    def calculate_score(self, advertised: Dict, ai: Dict, cadastre_area: Optional[float] = None) -> Dict[str, Any]:
        score = 0
        flags = []
        
        # 1. LEGAL / ATELIER
        if ai.get("is_atelier", False):
            score += 40
            flags.append("LEGAL: Property registered as Atelier (non-residential). Potential loan issues.")

        # 2. AREA SQUEEZING
        adv_area = advertised.get("area", 0)
        if cadastre_area and adv_area > 0:
            diff = (adv_area - cadastre_area) / cadastre_area
            if diff > 0.20:
                score += 30
                flags.append(f"AREA: Advertised area is {diff:.1%} larger than Cadastre record.")

        # 3. HEATING / UTILITIES (Sofia Specific)
        vision = ai.get("vision_insights", {})
        heating = vision.get("heating_source", "unknown").lower()
        if "electricity" in heating or "air conditioning" in heating:
            score += 5
            flags.append("COST: Heating is electric (expected higher winter bills).")
        
        # 4. FLOOR POSITION
        raw_text = advertised.get("raw_text", "").lower()
        if any(x in raw_text for x in ["Ð¿ÑŠÑ€Ð²Ð¸", "Ð¿Ð°Ñ€Ñ‚ÐµÑ€", "ground"]):
            score += 15
            flags.append("SECURITY: Ground floor unit.")

        return {"score": min(score, 100), "flags": flags}


=========================================
FILE: ./src/services/cadastre_service.py
=========================================
from typing import Optional, Dict

class CadastreService:
    """
    Interface for the Official Property Registry (Cadastre).
    In a real deployment, this would use the official API or a headless browser.
    """
    def __init__(self):
        # In the future, API keys for the government portal go here
        pass

    def fetch_details(self, address: str) -> Optional[Dict[str, float]]:
        """
        Simulates a lookup in the national registry.
        Returns: { "official_area": 85.0, "year_built": 2005 }
        """
        print(f"[CADASTRE] Querying registry for: {address}")
        
        # MOCK LOGIC: 
        # For now, we return 'None' to simulate that most addresses 
        # cannot be perfectly matched automatically yet.
        # This prevents the Risk Engine from throwing false positives 
        # until the real integration is ready.
        
        return None 
        
        # EXAMPLE REAL IMPLEMENTATION STUB:
        # response = requests.get(f"https://kais.cadastre.bg/api/search?q={address}")
        # if response.status_code == 200:
        #     return response.json()
        # return None


=========================================
FILE: ./src/services/geospatial_service.py
=========================================
from typing import Optional, Dict

class GeospatialService:
    """
    Handles Google Maps / Street View verification.
    """
    def __init__(self, api_key: str = "mock-key"):
        self.api_key = api_key

    def verify_location(self, address: str) -> Dict[str, any]:
        """
        TODO: Implement Google Maps Geocoding API.
        Should return { "lat": float, "lng": float, "street_view_exists": bool }
        """
        if self.api_key == "mock-key":
            return {"lat": 42.6977, "lng": 23.3219, "verified": False}
        
        # Real implementation will go here
        return {}


=========================================
FILE: ./src/services/base_provider.py
=========================================
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any

class BaseRegistryProvider(ABC):
    @abstractmethod
    def fetch_details(self, address: str) -> Optional[Dict[str, Any]]:
        """Fetch official property data from a government registry."""
        pass

class BaseGeoProvider(ABC):
    @abstractmethod
    def geocode(self, address: str) -> Dict[str, Any]:
        """Convert address to GPS coordinates."""
        pass


=========================================
FILE: ./src/services/storage_service.py
=========================================
import os
import httpx
import asyncio
from typing import List

class StorageService:
    def __init__(self, upload_dir="storage/archive"):
        self.upload_dir = upload_dir
        os.makedirs(upload_dir, exist_ok=True)

    async def _download_single(self, client: httpx.AsyncClient, url: str, filename: str) -> str:
        try:
            resp = await client.get(url, timeout=7.0)
            if resp.status_code == 200:
                path = os.path.join(self.upload_dir, filename)
                with open(path, "wb") as f:
                    f.write(resp.content)
                return path
        except Exception as e:
            print(f"Archive Fail: {url} -> {e}")
        return None

    async def archive_images(self, listing_id: int, urls: List[str]) -> List[str]:
        if not urls: return []
        async with httpx.AsyncClient() as client:
            tasks = []
            for i, url in enumerate(urls):
                filename = f"listing_{listing_id}_{i}.jpg"
                tasks.append(self._download_single(client, url, filename))
            results = await asyncio.gather(*tasks)
            return [r for r in results if r is not None]


=========================================
FILE: ./src/services/legal_engine.py
=========================================
from src.core.sofia_data import SOFIA_ADMIN_MAP
from datetime import datetime

class LegalEngine:
    """
    Translates Bulgarian Property Law into code-based risk metrics.
    """
    def analyze_listing(self, scraped_data: dict, ai_data: dict):
        risk_report = {
            "total_legal_score": 0,
            "pillars": {},
            "gatekeeper_verdict": "CLEAR",
            "flags": []
        }

        raw_text = scraped_data.get("raw_text", "").upper()
        rayon = ai_data.get("neighborhood", "Unknown").upper()

        # Pillar I: Statutory Classification (The Atelier Trap)
        p1_score = 0
        if ai_data.get("is_atelier") or "ÐÐ¢Ð•Ð›Ð˜Ð•" in raw_text or "ATELIER" in raw_text:
            p1_score += 35 
            if any(x in raw_text for x in ["Ð¡Ð•Ð’Ð•Ð ", "NORTH"]):
                p1_score += 15
                risk_report["flags"].append("INSOLENCE_FAILURE: North-facing Atelier cannot legally be an apartment.")
            
            admin_data = SOFIA_ADMIN_MAP.get(rayon, {"strictness": 3})
            if admin_data["strictness"] >= 4:
                p1_score += 20
                risk_report["flags"].append(f"ADDRESS_REG_RISK: District {rayon} is notoriously strict for Atelier owners.")

        risk_report["pillars"]["classification"] = p1_score

        # Pillar II: Construction Purgatory (The 'Akt' Matrix)
        p2_score = 0
        current_year = datetime.now().year
        if "ÐÐšÐ¢ 15" in raw_text or "ACT 15" in raw_text:
            build_year = ai_data.get("construction_year")
            if build_year and (current_year - build_year) > 2:
                p2_score += 45
                risk_report["flags"].append("ETERNAL_AKT_15: Building lacks Akt 16 for over 24 months.")
        risk_report["pillars"]["construction"] = p2_score

        # Pillar III: Area Value Integrity (Common Parts Ratio)
        p3_score = 0
        total_area = scraped_data.get("area", 0)
        net_area = ai_data.get("net_living_area", 0)
        if total_area > 0 and net_area > 0:
            ratio = (total_area - net_area) / total_area
            if ratio > 0.25:
                p3_score += 40
                risk_report["flags"].append(f"PREDATORY_COMMON_PARTS: {ratio:.1%} is non-living space.")
        risk_report["pillars"]["area_value"] = p3_score

        # Pillar IV: High-Yield Legal Encumbrances (Toxicity Rank)
        toxicity_score = 0
        if any(x in raw_text for x in ["ÐŸÐžÐ›Ð—Ð’ÐÐÐ•", "ÐŸÐžÐ–Ð˜Ð—ÐÐ•ÐÐž", "USER RIGHT"]):
            toxicity_score = 100
            risk_report["gatekeeper_verdict"] = "ABORT"
            risk_report["flags"].append("FATAL: RIGHT OF USE (Nude Ownership).")
        
        if any(x in raw_text for x in ["Ð˜Ð¡ÐšÐžÐ’Ð ÐœÐžÐ›Ð‘Ð", "Ð¡ÐªÐ”Ð•Ð‘Ð•Ð", "LITIGATION", "CLAIM"]):
            toxicity_score = max(toxicity_score, 95)
            risk_report["gatekeeper_verdict"] = "ABORT"
            risk_report["flags"].append("FATAL: PENDING LITIGATION (Iskova Molba).")

        if any(x in raw_text for x in ["Ð’ÐªÐ—Ð‘Ð ÐÐÐ", "Ð§Ð¡Ð˜", "ÐÐÐŸ", "DISTRAINT"]):
            toxicity_score = max(toxicity_score, 90)
            risk_report["flags"].append("CRITICAL: DISTRAINT (Asset Frozen for Debt).")

        risk_report["pillars"]["toxicity"] = toxicity_score
        risk_report["total_legal_score"] = max(p1_score, p2_score, p3_score, toxicity_score)
        
        return risk_report


=========================================
FILE: ./src/services/report_generator.py
=========================================
class AttorneyReportGenerator:
    """
    Transforms quantitative risk data into a senior legal brief.
    """
    def generate_legal_brief(self, listing_data: dict, legal_analysis: dict, ai_data: dict) -> str:
        verdict = legal_analysis.get("gatekeeper_verdict", "CLEAR")
        score = legal_analysis.get("total_legal_score", 0)
        
        status_symbol = "ðŸŸ¢ CLEAR"
        if verdict == "ABORT": status_symbol = "ðŸ”´ FATAL LEGAL STATUS"
        elif score > 60: status_symbol = "ðŸŸ  HIGH-FRICTION ASSET"
        elif score > 30: status_symbol = "ðŸŸ¡ CAUTION REQUIRED"

        sections = [
            f"# {status_symbol}",
            "## Executive Jurisprudential Summary",
            self._summary(legal_analysis),
            "\n## I. Statutory Classification & Utilities",
            self._classification_text(ai_data),
            "\n## II. Construction Stage Maturity",
            self._construction_text(legal_analysis),
            "\n## III. Economic Yield vs Area Integrity",
            self._area_text(listing_data, ai_data),
            "\n## IV. Legal Toxicity Flags",
            "\n".join([f"- {f}" for f in legal_analysis["flags"]]) if legal_analysis["flags"] else "No public-facing red flags detected."
        ]
        return "\n".join(sections)

    def _summary(self, analysis):
        if analysis["gatekeeper_verdict"] == "ABORT":
            return "Critical title defects detected. Any transfer would be legally precarious. Recommendation: CEASE TRANSACTION."
        if analysis["total_legal_score"] > 50:
            return "This asset exhibits significant administrative friction and financial opacity."
        return "The asset appears to align with standard Sofia residential norms."

    def _classification_text(self, ai):
        if not ai.get("is_atelier"): return "Object is legally a Dwelling (Ð–Ð¸Ð»Ð¸Ñ‰Ðµ)."
        return f"**WARNING:** Atelier Status in {ai.get('neighborhood')}. Expect kindergarten and ID registration friction."

    def _construction_text(self, analysis):
        if analysis["pillars"].get("construction", 0) > 40:
            return "Building shows evidence of 'Eternal Akt 15'. Potential legal/structural non-compliance detected."
        return "Normal administrative progression observed."

    def _area_text(self, listing, ai):
        total = listing.get("area", 0)
        net = ai.get("net_living_area", 0)
        if total > 0 and net > 0:
            ratio = (total - net) / total
            if ratio > 0.23: 
                real_p = (listing.get("price_bgn", 0) / net) if net > 0 else 0
                return f"PREDATORY INFLATION: {ratio:.1%} common parts. Adjusted Price: {real_p:.0f} BGN/sq.m (Net)."
        return "Standard area efficiency ratios detected."


=========================================
FILE: ./src/__init__.py
=========================================


=========================================
FILE: ./src/main.py
=========================================
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from src.api import routes

app = FastAPI(
    title="Glashaus API",
    description="Automated Real Estate Due Diligence Engine",
    version="1.0.0"
)

# Allow connections from Frontend/Dashboard
origins = [
    "http://localhost",
    "http://localhost:3000",
    "http://localhost:8080",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(routes.router)

@app.get("/")
def health_check():
    return {
        "system": "GLASHAUS", 
        "status": "OPERATIONAL", 
        "version": "1.0.0-PROD"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("src.main:app", host="0.0.0.0", port=8000)


=========================================
FILE: ./src/db/session.py
=========================================
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
from src.core.config import settings

# SQLite requires a specific argument for threading
connect_args = {"check_same_thread": False} if "sqlite" in settings.DATABASE_URL else {}

engine = create_engine(
    settings.DATABASE_URL, 
    connect_args=connect_args,
    pool_pre_ping=True
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


=========================================
FILE: ./src/db/models.py
=========================================
import enum
from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, ForeignKey, JSON, Text, Enum
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from src.db.session import Base

class ReportStatus(str, enum.Enum):
    PENDING = "PENDING"
    PROCESSING = "PROCESSING"
    VERIFIED = "VERIFIED"
    MANUAL_REVIEW = "MANUAL_REVIEW"
    REJECTED = "REJECTED"

class Listing(Base):
    __tablename__ = "listings"
    id = Column(Integer, primary_key=True, index=True)
    source_url = Column(String, unique=True, nullable=False, index=True)
    content_hash = Column(String(64), index=True)
    price_bgn = Column(Float)
    advertised_area_sqm = Column(Float)
    description_raw = Column(Text)
    scraped_at = Column(DateTime(timezone=True), server_default=func.now())
    
    reports = relationship("Report", back_populates="listing", cascade="all, delete-orphan")
    price_history = relationship("PriceHistory", back_populates="listing")

class Building(Base):
    __tablename__ = "buildings"
    id = Column(Integer, primary_key=True)
    cadastre_id = Column(String, unique=True, index=True)
    address_full = Column(String)
    # Using float for now for Termux compatibility, Geometry(POINT) for Prod
    latitude = Column(Float)
    longitude = Column(Float)
    construction_year = Column(Integer)
    reports = relationship("Report", back_populates="building")

class PriceHistory(Base):
    __tablename__ = "price_history"
    id = Column(Integer, primary_key=True)
    listing_id = Column(Integer, ForeignKey("listings.id"))
    price_bgn = Column(Float)
    changed_at = Column(DateTime(timezone=True), server_default=func.now())
    listing = relationship("Listing", back_populates="price_history")

class Report(Base):
    __tablename__ = "reports"
    id = Column(Integer, primary_key=True, index=True)
    listing_id = Column(Integer, ForeignKey("listings.id"))
    building_id = Column(Integer, ForeignKey("buildings.id"), nullable=True)
    status = Column(Enum(ReportStatus), default=ReportStatus.PENDING)
    risk_score = Column(Integer)
    ai_confidence_score = Column(Integer, default=0)
    legal_brief = Column(Text)
    discrepancy_details = Column(JSON)
    image_archive_urls = Column(JSON)
    cost_to_generate = Column(Float)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    listing = relationship("Listing", back_populates="reports")
    building = relationship("Building", back_populates="reports")


=========================================
FILE: ./src/worker.py
=========================================
import os
from celery import Celery
from src.core.config import settings

celery_app = Celery(
    "glashaus_worker",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL
)

celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="Europe/Sofia",
    enable_utc=True,
)

# Auto-discover tasks in src/tasks.py
celery_app.autodiscover_tasks(['src.tasks'])


=========================================
FILE: ./src/tasks.py
=========================================
from src.worker import celery_app
from src.db.session import SessionLocal
from src.db.models import Listing, Report, ReportStatus
from src.services.scraper_service import ScraperService
from src.services.ai_engine import GeminiService
from src.services.legal_engine import LegalEngine
from src.services.report_generator import AttorneyReportGenerator
from src.services.storage_service import StorageService
from src.services.repository import RealEstateRepository
from src.core.utils import calculate_content_hash
from src.core.config import settings
import asyncio

@celery_app.task(
    name="src.tasks.audit_listing",
    bind=True,
    autoretry_for=(Exception,),
    retry_backoff=True,
    max_retries=5
)
def audit_listing_task(self, listing_id: int):
    db = SessionLocal()
    loop = asyncio.get_event_loop()
    try:
        repo = RealEstateRepository(db)
        scraper = ScraperService(simulation_mode=(settings.GEMINI_API_KEY == "mock-key"))
        ai_engine = GeminiService(api_key=settings.GEMINI_API_KEY)
        storage = StorageService()
        legal_engine = LegalEngine()
        brief_gen = AttorneyReportGenerator()

        listing = db.query(Listing).get(listing_id)
        if not listing: return "Error: Listing not found"

        # 1. Scrape
        scraped_data = scraper.scrape_url(listing.source_url)
        
        # 2. Parallel Archive Images
        archived_paths = loop.run_until_complete(
            storage.archive_images(listing_id, scraped_data["image_urls"])
        )

        # 3. Update Meta
        chash = calculate_content_hash(scraped_data["raw_text"], scraped_data["price_predicted"])
        repo.update_listing_data(listing_id, scraped_data["price_predicted"], 
                                 scraped_data["area"], scraped_data["raw_text"], chash)

        # 4. AI Analysis
        ai_data = loop.run_until_complete(ai_engine.analyze_text(scraped_data["raw_text"]))

        # 5. Legal Logic
        legal_res = legal_engine.analyze_listing(scraped_data, ai_data)
        brief = brief_gen.generate_legal_brief(scraped_data, legal_res, ai_data)

        # 6. Verdict
        status = ReportStatus.VERIFIED if legal_res["total_legal_score"] < 40 else ReportStatus.MANUAL_REVIEW
        if legal_res.get("gatekeeper_verdict") == "ABORT": status = ReportStatus.REJECTED

        # 7. Final Report
        report = Report(
            listing_id=listing_id,
            status=status,
            risk_score=legal_res["total_legal_score"],
            ai_confidence_score=ai_data.get("confidence", 0),
            legal_brief=brief,
            discrepancy_details=legal_res,
            image_archive_urls=archived_paths,
            cost_to_generate=0.04
        )
        db.add(report)
        db.commit()
        return f"Audit Done: {status}"

    except Exception as exc:
        db.rollback()
        raise self.retry(exc=exc)
    finally:
        db.close()


=========================================
FILE: ./scripts/context_dump.sh
=========================================
#!/bin/bash
# Scans the repo and dumps text files for LLM context
output="glashaus_context.txt"
echo "--- GLASHAUS PROJECT DUMP ---" > "$output"
date >> "$output"

echo -e "\n\n--- GIT HISTORY ---" >> "$output"
git log --oneline --graph --decorate -n 20 >> "$output"

echo -e "\n\n--- FILE STRUCTURE ---" >> "$output"
tree -L 3 -I '.git|__pycache__|*.pyc' >> "$output" 2>/dev/null || find . -maxdepth 3 -not -path '*/.*' >> "$output"

echo -e "\n\n--- FILE CONTENTS ---" >> "$output"
find . -type f \
    -not -path '*/.*' \
    -not -path './glashaus_context.txt' \
    -not -name '*.png' \
    -not -name '*.jpg' \
    -not -name '*.sqlite' \
    | while read -r file; do
    echo -e "\n\n=========================================" >> "$output"
    echo "FILE: $file" >> "$output"
    echo "=========================================" >> "$output"
    cat "$file" >> "$output"
done

echo "Dump complete. Copy contents of $output"


=========================================
FILE: ./prompts/detective_prompt_v1.md
=========================================
# Role: Geospatial & Technical Detective
Analyze the real estate listing. Focus on detecting hidden legal or maintenance risks.

## Constraints
1. **Atelier Check:** Look for "Atelier" in text or industrial/office elements in photos.
2. **Heating:** Identify AC units, radiators, or fireplace.
3. **Era:** Estimate if building is Pre-1989 (Panel/EPC) or Post-2000 (Brick).
4. **Landmarks:** Identify specific street names or shop signs.

## Output Format (JSON Only)
{
  "address_prediction": "String",
  "confidence": 0-100,
  "is_atelier": boolean,
  "reasoning_steps": ["Step 1", "Step 2"],
  "vision_insights": {
    "facade_era": "string",
    "heating_source": "electricity/gas/central",
    "floor_plan_suspicion": "high/low"
  }
}


=========================================
FILE: ./db/schema_v1.sql
=========================================
-- Enable GIS extensions for location logic
CREATE EXTENSION IF NOT EXISTS postgis;

CREATE TABLE buildings (
    id SERIAL PRIMARY KEY,
    cadastre_id VARCHAR(50) UNIQUE NOT NULL, -- The official identifier
    address_street VARCHAR(255),
    address_number VARCHAR(50),
    neighborhood VARCHAR(100),
    gps_coordinates GEOMETRY(Point, 4326),
    construction_year INT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE listings (
    id SERIAL PRIMARY KEY,
    source_url TEXT UNIQUE NOT NULL,
    price_bgn DECIMAL(12, 2),
    advertised_area_sqm DECIMAL(10, 2),
    description_raw TEXT,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE reports (
    id SERIAL PRIMARY KEY,
    listing_id INT REFERENCES listings(id),
    building_id INT REFERENCES buildings(id),
    risk_score INT, -- 0 to 100 (100 is Toxic)
    is_address_verified BOOLEAN DEFAULT FALSE,
    discrepancy_details JSONB, -- Stores the "Apartment vs Atelier" logic
    cost_to_generate DECIMAL(10, 4) -- Tracking the $0.22 API cost
);


=========================================
FILE: ./db/migration_001_status_workflow.sql
=========================================
-- 1. Create the Workflow Status Enum
-- This supports the PENDING -> VERIFIED -> MANUAL_REVIEW flow
DO $$ BEGIN
    CREATE TYPE report_status AS ENUM ('PENDING', 'PROCESSING', 'VERIFIED', 'MANUAL_REVIEW', 'REJECTED');
EXCEPTION
    WHEN duplicate_object THEN null;
END $$;

-- 2. Update 'reports' table
ALTER TABLE reports 
ADD COLUMN IF NOT EXISTS status report_status DEFAULT 'PENDING',
ADD COLUMN IF NOT EXISTS ai_confidence_score INT DEFAULT 0,
ADD COLUMN IF NOT EXISTS manual_review_notes TEXT;

-- 3. Update 'listings' table for Idempotency
-- We hash the file/content to prevent duplicate processing of the same upload
ALTER TABLE listings
ADD COLUMN IF NOT EXISTS content_hash VARCHAR(64);

CREATE INDEX IF NOT EXISTS idx_listings_content_hash ON listings(content_hash);

-- 4. Create the Manual Review Queue View
-- This allows the Admin Panel to easily select tasks needing human eyes
CREATE OR REPLACE VIEW view_manual_review_queue AS
SELECT 
    r.id as report_id,
    l.source_url,
    r.ai_confidence_score,
    r.risk_score,
    r.created_at
FROM reports r
JOIN listings l ON r.listing_id = l.id
WHERE r.status = 'MANUAL_REVIEW'
ORDER BY r.risk_score DESC;


=========================================
FILE: ./db/migrations/env.py
=========================================
from logging.config import fileConfig
from sqlalchemy import engine_from_config
from sqlalchemy import pool
from alembic import context
import os
import sys

# Add src to path so we can import models
sys.path.append(os.getcwd())

from src.db.session import Base
from src.core.config import settings
from src.db.models import Listing, Report, Building 

config = context.config

if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Overwrite config URL with Environment Settings
config.set_main_option("sqlalchemy.url", settings.DATABASE_URL)

target_metadata = Base.metadata

def run_migrations_offline() -> None:
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online() -> None:
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()


=========================================
FILE: ./README.md
=========================================
# GLASHAUS: The Real Estate Integrity Engine

## Mission
To eliminate information asymmetry in the Sofia real estate market via automated due diligence.
We leverage OSINT, LLM reasoning, and Official Registry cross-referencing.

## Architecture
- **Text Layer:** Gemini Flash (Cost optimized)
- **Vision Layer:** Gemini Pro (Geospatial reasoning)
- **Data Layer:** PostgreSQL (Structured) + S3 (Archives)

## Status
- **Phase:** Pre-Alpha / Architectural Blueprint
- **Deploy Target:** Jan 2026 (Launch)


=========================================
FILE: ./requirements.txt
=========================================
fastapi==0.109.0
uvicorn==0.27.0
sqlalchemy==2.0.25
psycopg2-binary==2.9.9
httpx==0.26.0
pydantic==2.6.0
pydantic-settings==2.1.0
google-generativeai>=0.7.0
beautifulsoup4==4.12.3
celery==5.3.6
redis==5.0.1
alembic==1.13.1
Pillow==10.2.0


=========================================
FILE: ./imot_simulation.html
=========================================
<!DOCTYPE html>
<html>
<body>
    <div class="list_ads">
        <!-- Mock Listing 1 -->
        <a href="//www.imot.bg/pcgi/imot.cgi?act=5&adv=1c171899111&slink=a4eg0c&f1=1" class="photoLink">
            <div class="text_desc">
                2-STAEN, Sofia, Lozenets, 185 000 EUR
            </div>
        </a>
        
        <!-- Mock Listing 2 -->
        <a href="//www.imot.bg/pcgi/imot.cgi?act=5&adv=2c172200231&slink=a4eg0c&f1=1" class="photoLink">
            <div class="text_desc">
                3-STAEN, Sofia, Krustova Vada, 250 000 EUR, Gas/Elevator
            </div>
        </a>

        <!-- Mock Listing 3 -->
        <a href="//www.imot.bg/pcgi/imot.cgi?act=5&adv=3c17992881&slink=a4eg0c&f1=1" class="photoLink">
            <div class="text_desc">
                ATELIER, Sofia, Center, 90 000 EUR
            </div>
        </a>
    </div>
</body>
</html>


=========================================
FILE: ./Dockerfile
=========================================
# Use Official Python Runtime
FROM python:3.11-slim

# Set Working Directory
WORKDIR /app

# Install System Dependencies (Postgres + GIS libs)
RUN apt-get update && apt-get install -y \
    libpq-dev gcc netcat-openbsd \
    && rm -rf /var/lib/apt/lists/*

# Install Python Dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy Code
COPY . .

# Expose Port
EXPOSE 8000

# Start Command (Wait for DB, then launch)
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]


=========================================
FILE: ./docker-compose.yml
=========================================
version: '3.8'

services:
  # 1. GATEWAY (Entry Point)
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - api

  # 2. API (Internal)
  api:
    build: .
    expose:
      - "8000" # Not mapped to host anymore, only accessible via Nginx
    environment:
      - POSTGRES_SERVER=db
      - POSTGRES_USER=glashaus_user
      - POSTGRES_PASSWORD=secret_password
      - POSTGRES_DB=glashaus_db
      - REDIS_URL=redis://redis:6379/0
      # - GEMINI_API_KEY=${GEMINI_API_KEY} # Uncomment for prod
    depends_on:
      - db
      - redis
    volumes:
      - ./src:/app/src

  # 3. WORKER
  worker:
    build: .
    command: celery -A src.worker.celery_app worker --loglevel=info
    environment:
      - POSTGRES_SERVER=db
      - POSTGRES_USER=glashaus_user
      - POSTGRES_PASSWORD=secret_password
      - POSTGRES_DB=glashaus_db
      - REDIS_URL=redis://redis:6379/0
      # - GEMINI_API_KEY=${GEMINI_API_KEY}
    depends_on:
      - db
      - redis
    volumes:
      - ./src:/app/src

  # 4. REDIS
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  # 5. DB
  db:
    image: postgis/postgis:15-3.4
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=glashaus_user
      - POSTGRES_PASSWORD=secret_password
      - POSTGRES_DB=glashaus_db
    ports:
      - "5432:5432"

volumes:
  postgres_data:


=========================================
FILE: ./tests/test_api.py
=========================================
from fastapi.testclient import TestClient
from src.main import app

client = TestClient(app)

def test_health_check():
    response = client.get("/")
    assert response.status_code == 200
    assert response.json()["status"] == "OPERATIONAL"

def test_audit_flow():
    payload = {"url": "https://www.imot.bg/pcgi/imot.cgi?act=5&adv=mock123"}
    response = client.post("/audit", json=payload)
    
    assert response.status_code == 200
    data = response.json()
    assert data["status"] == "QUEUED"
    assert "listing_id" in data


=========================================
FILE: ./alembic.ini
=========================================
[alembic]
script_location = db/migrations
prepend_sys_path = .
sqlalchemy.url = driver://user:pass@localhost/dbname

[post_write_hooks]

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S


=========================================
FILE: ./nginx/nginx.conf
=========================================
events {}

http {
    upstream glashaus_api {
        server api:8000;
    }

    server {
        listen 80;
        
        # Proxy all API requests to the FastAPI container
        location / {
            proxy_pass http://glashaus_api;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        # Health check endpoint
        location /health {
            return 200 'alive';
            add_header Content-Type text/plain;
        }
    }
}
