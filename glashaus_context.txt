--- GLASHAUS PROJECT DUMP ---
Sun Dec 28 20:58:50 EET 2025


--- GIT HISTORY ---
* 3f8b25f (HEAD -> main) refactor(config): externalize model configuration - Move hardcoded model name to src/core/config.py - Set default model to 'gemini-3.0-flash' (Current Gen) - Update GeminiService to use settings.GEMINI_MODEL
* 0201f30 feat(pipeline): integrate unified 3D municipal audit into worker task
* 9ac15c3 feat(forensics): implement 3D audit with live NAG registries - Add Expropriation check (The Death List) - Add Act 16 check (The Green List) - Add Building Permits check - Update headers to match browser emulation (X-Requested-With)
* f65c320 feat(forensics): implement automated cadastre auditing and social risk detection  * Implement CadastreForensicsService for direct KAIS integration  * Add audit_target.py CLI for property triangulation and ownership scanning  * Implement "Dirty Cop" algorithm: Detect social housing risks via municipal ownership ratios  * Add deep scanning for real area verification and neighbor analysis  * Persist authentication tokens for session reuse
* f0c6488 update docs and remove junk files
* 27d7a7c feat(forensics): implement legal RAG and geospatial triangulation
* 52420e1 refactor(core): implement hybrid scraping fallback and multimodal AI forensics Key Changes in this Commit:  * Multimodal AI Integration: Upgraded the GeminiService to use gemini-1.5-flash, enabling vision-based forensics by processing listing images alongside text.  * Forensic Property Analysis: Implemented a "Senior Real Estate Forensic Detective" prompt to detect "Atelier" status traps, identify architectural eras (e.g., Pre-1989 Panel vs. Post-2000 Brick), and perform orientation checks.  * Geospatial Verification: Added a new GeospatialService that uses the Google Maps Geocoding API to cross-reference AI-detected landmarks and addresses against the neighborhood claimed by the broker.  * Location Fraud Detection: The audit pipeline now automatically elevates risk scores (minimum of 70) if the geospatial verification finds a mismatch between the ad and visual evidence.  * Enhanced Scraper: Updated ScraperService to better extract neighborhood data and handle image URLs from imot.bg more reliably.
* 698eb25 refactor(core): implement hybrid scraping fallback and multimodal AI forensics
* 6f470a6 refactor(core): production hardening, type safety, and architecture cleanup
* 0f7d54d refactor(core): production hardening, type safety, and architecture cleanup
* d13df83 refactor(core): harden security, async concurrency, and financial precision
* 5a71d4d refactor(core): promote prototype to production-ready architecture
* fdd6519 docs: update architecture, API contract, and debt log to match V1 state
* a107438 feat(core): implement forensic analysis and anti-manipulation logic
* 2f79d2e removed junk
* ed4950e feat(forensics): harden backend with LexSofia logic and municipal registry strikes
* 00b1e10 feat(reporting): integrate forensic registry checks into Legal Brief generation
* e3f1eaf fix(services): implement circuit breakers for government registry availability
* 6c7c90c feat(forensics): implement live registry verification and v2 risk engine
* 21bbc86 fix(core): harden worker db sessions, strict config, and normalize risk logic


--- FILE STRUCTURE ---
.
â”œâ”€â”€ --data-urlencode
â”œâ”€â”€ -G
â”œâ”€â”€ -H
â”œâ”€â”€ -X
â”œâ”€â”€ -d
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ alembic.ini
â”œâ”€â”€ bypass_audit.py
â”œâ”€â”€ cookies.txt
â”œâ”€â”€ db
â”‚Â Â  â”œâ”€â”€ migration_001_status_workflow.sql
â”‚Â Â  â”œâ”€â”€ migration_002_fix_currency.sql
â”‚Â Â  â”œâ”€â”€ migrations
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ env.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ versions
â”‚Â Â  â””â”€â”€ schema_v1.sql
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docs
â”‚Â Â  â”œâ”€â”€ ARCHITECTURE.md
â”‚Â Â  â”œâ”€â”€ LASTENHEFT_DE.md
â”‚Â Â  â”œâ”€â”€ TECHNICAL_DEBT.md
â”‚Â Â  â””â”€â”€ api_contract.yaml
â”œâ”€â”€ forensics
â”‚Â Â  â”œâ”€â”€ headers.txt
â”‚Â Â  â””â”€â”€ page_utf8.html
â”œâ”€â”€ glashaus_context.txt
â”œâ”€â”€ manual_session_audit.py
â”œâ”€â”€ nginx
â”‚Â Â  â””â”€â”€ nginx.conf
â”œâ”€â”€ prompts
â”‚Â Â  â””â”€â”€ detective_prompt_v1.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scraper_service.py
â”œâ”€â”€ scripts
â”‚Â Â  â”œâ”€â”€ audit_target.py
â”‚Â Â  â”œâ”€â”€ context_dump.sh
â”‚Â Â  â”œâ”€â”€ debug_kais.py
â”‚Â Â  â””â”€â”€ scrape_lex.py
â”œâ”€â”€ src
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ api
â”‚Â Â  â”‚Â Â  â””â”€â”€ routes.py
â”‚Â Â  â”œâ”€â”€ core
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ config.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ logger.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ patterns.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ sofia_data.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ utils.py
â”‚Â Â  â”œâ”€â”€ db
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ models.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ session.py
â”‚Â Â  â”œâ”€â”€ main.py
â”‚Â Â  â”œâ”€â”€ models
â”‚Â Â  â”œâ”€â”€ schemas.py
â”‚Â Â  â”œâ”€â”€ services
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ai_engine.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ base_provider.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cadastre_service.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ city_risk_service.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ compliance_service.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ forensics_service.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ geospatial_service.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ legal_engine.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ report_generator.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ repository.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ risk_engine.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ scraper_mvp.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ scraper_service.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ storage_service.py
â”‚Â Â  â”œâ”€â”€ tasks.py
â”‚Â Â  â””â”€â”€ worker.py
â”œâ”€â”€ test_forensics.py
â””â”€â”€ tests
    â””â”€â”€ test_api.py

16 directories, 60 files


--- FILE CONTENTS ---


=========================================
FILE: ./docs/ARCHITECTURE.md
=========================================
# System Architecture

## Core Logic Flow (Forensic Loop)
The system uses an Event-Driven architecture to orchestrate parallel forensic checks.

1. **Ingest & Normalize:** - Scraper Service fetches URL (handling `windows-1251` encoding).
   - URLs are normalized (mobile subdomain enforcement) and deduplicated via content hash.

2. **The "Forensic Audit" Task (Celery Worker):**
   Unlike the initial design, the system now runs a **consolidated parallel audit** rather than a tiered fallback.
   - **Step A: Text Analysis (Gemini Flash):** Extracts unstructured data (Address, Construction Year, Atelier status).
   - **Step B: Official Registry Checks (Async/Parallel):**
     - **Cadastre Service:** Verifies official area vs advertised area using the extracted address.
     - **City Risk Service (NAG):** Checks Sofia Municipal records for Expropriation (seizure) risks.
     - **Compliance Service:** Verifies "Act 16" (Commissioning Certificate) status against the cadastral ID.
   - **Step C: Risk Engine V2:**
     - Merges all data points to calculate a composite `Risk Score` (0-100).
     - Identifies "Fatal" flags (e.g., Expropriation = 100% Risk).

3. **Report Generation:** - `AttorneyReportGenerator` synthesizes a human-readable legal brief from the structured forensic data.

## Tech Stack
- **Service Layer:** Python 3.11 (FastAPI)
- **Asynchronous Task Queue:** Celery + Redis
- **Database:** PostgreSQL + PostGIS (Spatial data)
- **Resilience:** Circuit Breakers implemented for Government Registry downtime (handling `httpx.ConnectError`).

## Critical Components
| Component | Function | Status |
| :--- | :--- | :--- |
| **Risk Engine** | Calculates score based on Area Fraud, Legal Status (Atelier), and Expropriation. | âœ… Implemented (V2) |
| **Forensics Service** | Manages sessions with `nag.sofia.bg` and `kais.cadastre.bg`. | âœ… Implemented |
| **Storage Service** | Archives listing images to disk/S3 for evidence preservation. | âœ… Implemented |

## Database Schema (Live State)
*Synced with `src/db/models.py` and Alembic Migrations 001-003.*

### 1. Listings Table
| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | Integer | PK |
| `source_url` | String | Unique Index |
| `content_hash` | String(64) | **Idempotency**: SHA256(text + price) |
| `price_bgn` | **Numeric(12,2)** | Financial precision (e.g., 150000.00) |
| `advertised_area_sqm` | **Numeric(10,2)** | Area precision (e.g., 65.50) |
| `description_raw` | Text | Full ad body |

### 2. Reports Table
| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | Integer | PK |
| `listing_id` | Integer | FK -> Listings |
| `status` | Enum | `PENDING`, `PROCESSING`, `VERIFIED`, `MANUAL_REVIEW`, `REJECTED` |
| `risk_score` | Integer | 0-100 (100 = Fatal) |
| `ai_confidence_score` | Integer | 0-100 (Hallucination check) |
| `cost_to_generate` | **Numeric(10,4)** | API Cost tracking (e.g., 0.0045 BGN) |
| `discrepancy_details` | JSON | Specific flags (e.g. "Area mismatch > 25%") |

### 3. Buildings Table (Registry Truth)
| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | Integer | PK |
| `cadastre_id` | String | Unique Official ID |
| `construction_year` | Integer | Registry confirmed year |
| `latitude/longitude` | Float | Geocoded coordinates |


=========================================
FILE: ./docs/api_contract.yaml
=========================================
openapi: 3.0.0
info:
  title: Glashaus API
  version: 1.0.0
paths:
  /audit/url:
    post:
      summary: Initiate an Audit for a specific Listing URL
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                url:
                  type: string
                  format: uri
      responses:
        '200':
          description: Audit Queued
          content:
            application/json:
              schema:
                type: object
                properties:
                  listing_id:
                    type: integer
                  status:
                    type: string
                    example: "QUEUED_IN_REDIS"

  /reports/{listing_id}:
    get:
      summary: Retrieve Forensic Report & Risk Score
      responses:
        '200':
          description: Detailed Audit Report
          content:
            application/json:
              schema:
                type: object
                properties:
                  report_id:
                    type: integer
                  status:
                    type: string
                    enum: [PENDING, PROCESSING, VERIFIED, MANUAL_REVIEW, REJECTED]
                  risk_score:
                    type: integer
                    description: 0-100 (100 is Toxic/Fatal)
                  cost:
                    type: float
                  manual_notes:
                    type: string
                  discrepancies:
                    type: object
                    properties:
                      flags:
                        type: array
                        items: 
                          type: string
                        example: ["CRITICAL: Property is listed for EXPROPRIATION.", "LEGAL: Atelier status."]
                      forensics:
                        type: object
                        properties:
                          city_risk:
                            type: object
                            properties:
                              is_expropriated: 
                                type: boolean
                              registry_status:
                                type: string
                          compliance:
                            type: object
                            properties:
                              has_act16:
                                type: boolean
                          cadastre:
                            type: object
                            properties:
                              official_area:
                                type: number
                              diff_percent:
                                type: number


=========================================
FILE: ./docs/LASTENHEFT_DE.md
=========================================
# LASTENHEFT: Projekt Glashaus
**Version:** 1.0.0
**Status:** In Entwicklung

## 1. Einleitung
Das Projekt "Glashaus" ist eine automatisierte Due-Diligence-Plattform fÃ¼r den Immobilienmarkt in Sofia. Ziel ist die Beseitigung von Informationsasymmetrien durch den Einsatz von OSINT und KI-gestÃ¼tzter Datenanalyse.

## 2. Ist-Zustand (Problemstellung)
- **Datenfragmentierung:** Grundbuch (Registry), Kataster (Cadastre) und Gemeinde agieren in Silos.
- **Intransparenz:** Immobilienanzeigen enthalten oft ungenaue FlÃ¤chenangaben und verschleierte Adressen.
- **Prozessineffizienz:** Manuelle PrÃ¼fungen sind teuer und langsam.

## 3. Soll-Zustand (LÃ¶sung)
Ein Microservices-System, das folgende Kernfunktionen bietet:
1.  **Automatische Adress-Deduktion:** Ermittlung der exakten Adresse aus unstrukturierten Anzeigentexten und Bildern.
2.  **Soll/Ist-Abgleich:** Automatischer Vergleich von Maklerangaben (Anzeige) mit amtlichen Katasterdaten.
3.  **Risikobewertung:** Algorithmische Berechnung eines "Risk Scores" (0-100).

## 4. Technische Anforderungen
- **Architektur:** Event-Driven Microservices (Python/FastAPI).
- **Datenbank:** PostgreSQL mit PostGIS fÃ¼r Geodatenverarbeitung.
- **KI-Integration:**
    - *Tier 1:* Textanalyse (Low Cost / Gemini Flash).
    - *Tier 2:* Visuelle Analyse (High Cost / Gemini Pro).

## 5. Nicht-funktionale Anforderungen
- **Idempotenz:** Wiederholte Uploads dÃ¼rfen keine Datenkorruption verursachen.
- **Skalierbarkeit:** Das System muss Warteschlangen (Queues) nutzen, um Lastspitzen bei Scrapern abzufangen.


=========================================
FILE: ./docs/TECHNICAL_DEBT.md
=========================================
# TECHNICAL DEBT LOG

## Critical Severity (Must Fix Before Beta)

### 1. Hardcoded Secrets (Config)
- **Location:** `src/core/config.py`
- **Issue:** `GEMINI_API_KEY` defaults to "mock-key". While validated at runtime, this is bad practice.
- **Fix:** Remove default value entirely and force `.env` loading.

### 2. Scraper Fragility (DOM Coupling)
- **Location:** `src/services/scraper_service.py` & `forensic_check.py`
- **Issue:** Logic relies on specific HTML IDs (e.g., `id='price'`, `id='description_div'`).
- **Risk:** High. If `imot.bg` updates their frontend classes, the "Space Hack" and Price normalization logic will fail silently.
- **Fix:** Implement multi-selector fallbacks or move strictly to Visual DOM analysis (Gemini Vision) for extraction.

## Moderate Severity (Optimization)

### 3. Synchronous Registry Calls in Loops
- **Location:** `src/services/forensics_service.py`
- **Issue:** While the task itself is async, some specialized sub-checks might still block the event loop if not strictly awaited.
- **Fix:** Audit all `httpx` calls to ensure `await` is used consistently across the `gather()` chain.

## Resolved Items (Fixed)

### âœ… Database Session Scope in Background Tasks
- **Fix:** `src/tasks.py` now explicitly initializes a thread-safe session using `with SessionLocal() as db:` inside the Celery worker.

### âœ… Risk Scoring Algorithm
- **Fix:** `RiskEngine` (V2) is implemented (`src/services/risk_engine.py`). It now calculates scores based on Expropriation (Fatal), Act 16 status, and Area discrepancy > 25%.

### âœ… Listing Normalization
- **Fix:** `RealEstateRepository` now normalizes URLs (forcing mobile subdomains) and checks for duplicates before insertion.


=========================================
FILE: ./src/api/routes.py
=========================================
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from src.db.session import get_db
from src.db.models import Listing, Report, ReportStatus
from src.services.repository import RealEstateRepository
from src.tasks import audit_listing_task
from pydantic import BaseModel
from typing import Optional

router = APIRouter()

class AuditRequest(BaseModel):
    url: str
    price_override: float = 0.0

class ReportUpdate(BaseModel):
    status: str
    manual_notes: Optional[str] = None

@router.post("/audit")
async def initiate_audit(request: AuditRequest, db: Session = Depends(get_db)):
    """
    Submits a URL for auditing via the Celery Worker queue.
    """
    repo = RealEstateRepository(db)
    # Create listing immediately to return ID
    listing = repo.create_listing(url=request.url, price=request.price_override, area=0.0, desc="Queued")
    
    # Offload to Redis/Celery
    audit_listing_task.delay(listing.id)
    
    return {"listing_id": listing.id, "status": "QUEUED_IN_REDIS"}

@router.get("/reports/{listing_id}")
def get_report(listing_id: int, db: Session = Depends(get_db)):
    """
    Retrieves the report status and details for a listing.
    """
    listing = db.query(Listing).filter(Listing.id == listing_id).first()
    if not listing:
        raise HTTPException(status_code=404, detail="Listing not found")
        
    report = db.query(Report).filter(Report.listing_id == listing_id).first()
    if not report:
        # If no report exists yet, the worker is likely still processing
        return {"status": "PROCESSING", "details": "Audit is currently in the queue."}
        
    return {
        "report_id": report.id,
        "status": report.status,
        "risk_score": report.risk_score,
        "ai_confidence": report.ai_confidence_score,
        "discrepancies": report.discrepancy_details,
        "manual_notes": report.manual_review_notes,
        "cost": report.cost_to_generate,
        "created_at": report.created_at
    }

@router.patch("/reports/{report_id}")
def update_report_status(report_id: int, update: ReportUpdate, db: Session = Depends(get_db)):
    """
    Manual Review Action.
    Updates status (e.g., MANUAL_REVIEW -> VERIFIED).
    """
    report = db.query(Report).filter(Report.id == report_id).first()
    if not report:
        raise HTTPException(status_code=404, detail="Report not found")
        
    try:
        # Validate that the string provided matches the Enum
        new_status = ReportStatus(update.status)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid Status Enum")

    report.status = new_status
    if update.manual_notes:
        report.manual_review_notes = update.manual_notes
        
    db.commit()
    return {"id": report.id, "new_status": report.status}


=========================================
FILE: ./src/core/config.py
=========================================
from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import Optional

class Settings(BaseSettings):
    PROJECT_NAME: str = "Glashaus"
    VERSION: str = "0.1.0"
    
    # Database
    POSTGRES_USER: str = "postgres"
    POSTGRES_PASSWORD: str = "postgres"
    POSTGRES_DB: str = "glashaus"
    POSTGRES_HOST: str = "localhost"
    POSTGRES_PORT: int = 5432
    DATABASE_URL: str = f"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"

    # AI & External APIs
    GEMINI_API_KEY: str
    GEMINI_MODEL: str = "gemini-3.0-flash"  # <--- Updated to 3.0-flash
    GOOGLE_MAPS_API_KEY: Optional[str] = None
    
    # Security
    SECRET_KEY: str = "changethis_in_production"
    
    # Infra
    REDIS_URL: str = "redis://localhost:6379/0"

    model_config = SettingsConfigDict(env_file=".env", extra="ignore")

settings = Settings()


=========================================
FILE: ./src/core/utils.py
=========================================
import re
import hashlib

def extract_imot_id(url: str) -> str:
    match = re.search(r'(?:adv=|obiava-)([a-z0-9]+)', url)
    return match.group(1) if match else "unknown"

def normalize_url(url: str) -> str:
    if "imot.bg" in url:
        return url.replace("www.imot.bg", "m.imot.bg")
    return url

def calculate_content_hash(text: str, price: float) -> str:
    clean_text = re.sub(r'\s+', ' ', text).strip().lower()
    raw = f"{clean_text}{price}".encode('utf-8')
    return hashlib.sha256(raw).hexdigest()

def normalize_sofia_street(address: str) -> str:
    """Strips common Sofia prefixes that confuse the Cadastre search."""
    if not address: return ""
    # Remove 'ul.', 'bul.', 'zh.k', 'kv.' etc.
    patterns = [r'(?i)ÑƒÐ»\.', r'(?i)Ð±ÑƒÐ»\.', r'(?i)Ð¶\.Ðº\.', r'(?i)ÐºÐ²\.', r'(?i)Ð³Ñ€\. Ð¡Ð¾Ñ„Ð¸Ñ,?\s*']
    clean = address
    for p in patterns:
        clean = re.sub(p, '', clean)
    return clean.strip()


=========================================
FILE: ./src/core/sofia_data.py
=========================================
SOFIA_ADMIN_MAP = {
    "OBORISHTE": {"strictness": 5, "kindergarten_risk": "Critical"},
    "PODUYANE": {"strictness": 5, "kindergarten_risk": "High"},
    "CENTER": {"strictness": 4, "kindergarten_risk": "High"},
    "LOZENETS": {"strictness": 4, "kindergarten_risk": "Moderate"},
    "VITOSHA": {"strictness": 2, "kindergarten_risk": "Low"},
    "KRUSTOVA VADA": {"strictness": 2, "kindergarten_risk": "Low"},
}


=========================================
FILE: ./src/core/logger.py
=========================================
import structlog
import logging
import sys

def setup_logging():
    # Standard lib logging for libs that don't use structlog
    logging.basicConfig(format="%(message)s", stream=sys.stdout, level=logging.INFO)
    
    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.processors.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.JSONRenderer()
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

# Singleton logger instance
logger = structlog.get_logger()


=========================================
FILE: ./src/core/patterns.py
=========================================
import re
from typing import List

class ForensicPatterns:
    """
    Centralized regex registry for text analysis.
    """
    
    VAT_EXCLUDED = re.compile(r"(?i)(Ñ†ÐµÐ½Ð°Ñ‚Ð° Ðµ Ð±ÐµÐ· Ð´Ð´Ñ|Ð±ÐµÐ· Ð´Ð´Ñ|vat excluded|no vat|Ð½Ðµ ÑÐµ Ð½Ð°Ñ‡Ð¸ÑÐ»ÑÐ²Ð° Ð´Ð´Ñ)")
    SPACE_HACK = re.compile(r"(?i)(Ð¿Ñ€ÐµÑƒÑÑ‚Ñ€Ð¾ÐµÐ½Ð°? Ð³Ð°Ñ€ÑÐ¾Ð½Ð¸ÐµÑ€Ð°|ÑƒÑÐ²Ð¾ÐµÐ½.*?Ð±Ð°Ð»ÐºÐ¾Ð½|ÐºÑƒÑ…Ð½Ñ.*?ÐºÐ¾Ñ€Ð¸Ð´Ð¾Ñ€|Ð±Ð¸Ð²ÑˆÐ°.*?ÐºÑƒÑ…Ð½Ñ|Ð¼Ð°Ð»Ð¾Ð¼ÐµÑ€ÐµÐ½|Ð±Ð¾ÐºÑÐ¾Ð½Ð¸ÐµÑ€Ð°|Ñ‚Ð°Ð²Ð°Ð½ÑÐºÐ¾)")
    ATELIER_STATUTE = re.compile(r"(?i)(ÑÑ‚Ð°Ñ‚ÑƒÑ‚.*?Ð°Ñ‚ÐµÐ»Ð¸Ðµ|ÑÑ‚Ð°Ñ‚ÑƒÑ‚ Ð½Ð° Ð°Ñ‚ÐµÐ»Ð¸Ðµ|ÑÑ‚ÑƒÐ´Ð¸Ð¾|Ñ‚Ð²Ð¾Ñ€Ñ‡ÐµÑÐºÐ¾ Ð°Ñ‚ÐµÐ»Ð¸Ðµ|atelier)")
    GROUND_FLOOR = re.compile(r"(?i)(Ð¿Ð°Ñ€Ñ‚ÐµÑ€|ÐµÑ‚Ð°Ð¶ 1 Ð¾Ñ‚|Ð²Ð¸ÑÐ¾Ðº Ð¿Ð°Ñ€Ñ‚ÐµÑ€|ÐºÐ¾Ñ‚Ð° 0|ÑÑƒÑ‚ÐµÑ€ÐµÐ½)")
    FUTURE_COMPLETION = re.compile(r"(?i)(\d{1,2}%.*?ÑÐµÐ³Ð°|\d{1,2}%.*?Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÐµÐ½|\d{1,2}%.*?Ð°ÐºÑ‚ 16)")

    @classmethod
    def normalize_text(cls, text: str) -> str:
        """Standardize text to catch edge cases."""
        if not text: return ""
        return re.sub(r'\s+', ' ', text).strip().upper()

    @classmethod
    def extract_flags(cls, text: str) -> List[str]:
        flags = []
        if not text: return flags
        
        if cls.VAT_EXCLUDED.search(text):
            flags.append("VAT_EXCLUDED")
        if cls.SPACE_HACK.search(text):
            flags.append("CONVERSION_RISK")
        if cls.ATELIER_STATUTE.search(text):
            flags.append("ATELIER_DETECTED")
        if cls.GROUND_FLOOR.search(text):
            flags.append("GROUND_FLOOR_RISK")
            
        return flags


=========================================
FILE: ./src/services/ai_engine.py
=========================================
import google.generativeai as genai
from typing import Dict, Any, List
import json
import time
from src.core.logger import logger
from src.core.config import settings

class GeminiService:
    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        # Use the config value (defaults to gemini-3.0-flash)
        self.model = genai.GenerativeModel(settings.GEMINI_MODEL)

    async def analyze_listing_multimodal(self, text_content: str, image_paths: List[str]) -> Dict[str, Any]:
        """
        Analyzes listing text + images to find discrepancies and specific visual risks.
        """
        logger.info(f"Analyzing listing with {settings.GEMINI_MODEL}...")
        
        # Construct the prompt for the Digital Attorney
        prompt = f"""
        You are an expert Real Estate Forensic Auditor in Sofia, Bulgaria.
        Analyze this listing text and the attached images.
        
        TEXT:
        {text_content}
        
        TASK:
        1. Extract the likely address (Street, Number, Neighborhood).
        2. Estimate construction year based on visual style (Panel vs Brick).
        3. Detect "Atelier" status traps (North facing, small windows).
        4. Identify "Visual Lies": Does the text say "Luxury" but images show "Panel"?
        
        Return JSON only:
        {{
            "address_prediction": "str",
            "construction_year_est": int,
            "is_panel_block": bool,
            "is_atelier_trap": bool,
            "visual_defects": ["str"],
            "landmarks": ["str"]
        }}
        """
        
        try:
            # Prepare parts: Text + Images
            parts = [prompt]
            
            # TODO: Append actual image data here in production
            
            response = self.model.generate_content(parts)
            cleaned_text = response.text.replace('', '')
            return json.loads(cleaned_text)
            
        except Exception as e:
            logger.error(f"AI Analysis Failed: {e}")
            return {
                "address_prediction": "Unknown",
                "error": str(e)
            }


=========================================
FILE: ./src/services/repository.py
=========================================
from decimal import Decimal
from sqlalchemy.orm import Session
from src.db.models import Listing, PriceHistory
from src.core.utils import normalize_url

class RealEstateRepository:
    def __init__(self, db: Session):
        self.db = db

    def create_listing_initial(self, url: str) -> Listing:
        clean_url = normalize_url(url)
        existing = self.db.query(Listing).filter(Listing.source_url == clean_url).first()
        if existing: return existing
        new_l = Listing(source_url=clean_url)
        self.db.add(new_l)
        self.db.commit()
        self.db.refresh(new_l)
        return new_l

    # TYPE SAFETY FIX: price is now Decimal
    def update_listing_data(self, listing_id: int, price: Decimal, area: float, desc: str, chash: str):
        listing = self.db.query(Listing).get(listing_id)
        if listing:
            # SQLAlchemy handles Decimal comparison correctly here
            if listing.price_bgn is not None and listing.price_bgn != price:
                history = PriceHistory(listing_id=listing_id, price_bgn=listing.price_bgn)
                self.db.add(history)
            
            listing.price_bgn = price
            listing.advertised_area_sqm = area
            listing.description_raw = desc
            listing.content_hash = chash
            self.db.commit()


=========================================
FILE: ./src/services/scraper_mvp.py
=========================================
from bs4 import BeautifulSoup
import os

# CONFIG
SIMULATION_MODE = True
MOCK_FILE = "imot_simulation.html"

def run_recon():
    print("[*] INTEL: Starting Reconnaissance Protocol...")
    
    html_content = ""
    
    if SIMULATION_MODE:
        print(f"[*] MODE: SIMULATION (Bypassing WAF)")
        if not os.path.exists(MOCK_FILE):
            print(f"[!] Error: Mock file {MOCK_FILE} not found.")
            return
            
        with open(MOCK_FILE, "r", encoding="utf-8") as f:
            html_content = f.read()
    else:
        # Network logic removed for Termux Safety
        pass

    soup = BeautifulSoup(html_content, 'html.parser')
    links = soup.find_all('a', href=True)
    
    print("[*] Parsing DOM Structure...")
    
    count = 0
    listings_found = []
    
    for link in links:
        href = link['href']
        
        if 'act=5' in href:
            # Normalize URL
            full_url = "https:" + href if href.startswith("//") else href
            
            if full_url in listings_found:
                continue
                
            listings_found.append(full_url)
            text_content = link.get_text(separator=" ", strip=True)
            
            count += 1
            print(f"\n[TARGET #{count}]")
            print(f"   URL: {full_url}")
            print(f"   RAW: {text_content}")

    print(f"\n[*] Mission Complete. {count} mock targets extracted.")

if __name__ == "__main__":
    run_recon()


=========================================
FILE: ./src/services/scraper_service.py
=========================================
import httpx
import re
import asyncio
from decimal import Decimal, InvalidOperation
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
from src.schemas import ScrapedListing
from src.core.logger import logger

class WAFBlockError(Exception): pass

class ScraperService:
    def __init__(self, client: httpx.AsyncClient, simulation_mode=False):
        self.client = client
        self.simulation = simulation_mode
        self.headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 Mobile Safari/604.1",
            "Accept-Language": "bg-BG,bg;q=0.9"
        }

    async def scrape_url(self, url: str) -> ScrapedListing:
        # Standardize to mobile to reduce WAF friction
        clean_url = url.replace("www.imot.bg", "m.imot.bg")
        log = logger.bind(url=clean_url)
        
        try:
            # 1. Try Fast Path (HTTPX)
            return await self._scrape_fast(clean_url, log)
            
        except WAFBlockError:
            log.warning("waf_intercept_detected", strategy="switching_to_headless_browser")
            # 2. Fallback to Heavy Path (Playwright)
            return await self._scrape_heavy_browser(clean_url, log)

        except Exception as e:
            log.error("scrape_failed_fatal", error=str(e))
            raise e

    async def _scrape_fast(self, url: str, log) -> ScrapedListing:
        resp = await self.client.get(url, headers=self.headers, follow_redirects=True)
        content = resp.content.decode('windows-1251', errors='ignore')

        if any(x in content.lower() for x in ["captcha", "security check", "verify you are human"]):
            raise WAFBlockError("Fast scrape blocked")
            
        log.info("scrape_success_fast")
        return await asyncio.to_thread(self._parse_html, content, url)

    async def _scrape_heavy_browser(self, url: str, log) -> ScrapedListing:
        """Launches a headless browser to execute JS challenges."""
        async with async_playwright() as p:
            browser = await p.firefox.launch(headless=True)
            context = await browser.new_context(
                user_agent=self.headers["User-Agent"],
                viewport={"width": 390, "height": 844}
            )
            page = await context.new_page()
            
            try:
                await page.goto(url, wait_until="domcontentloaded")
                
                # Wait for core data to appear (Max 10s)
                try:
                    await page.wait_for_selector('div#price, .price, .advHeader', timeout=10000)
                except Exception:
                    log.warning("browser_wait_timeout_proceeding_anyway")
                
                content = await page.content()
                log.info("scrape_success_heavy")
                return await asyncio.to_thread(self._parse_html, content, url)
                
            finally:
                await browser.close()

    def _parse_html(self, content: str, url: str) -> ScrapedListing:
        soup = BeautifulSoup(content, 'html.parser')
        text = soup.get_text(" ", strip=True)

        # 1. Price Parsing
        p_match = re.search(r'([\d\s\.,]+)\s?(?:EUR|â‚¬|Ð»Ð²)', text)
        price_decimal = Decimal("0.00")
        if p_match:
            try:
                clean_str = re.sub(r'[^\d]', '', p_match.group(1))
                price_decimal = Decimal(clean_str)
            except (InvalidOperation, ValueError):
                logger.warning("price_parse_failed", url=url)

        # 2. Area Parsing
        a_match = re.search(r'(\d+)\s?(?:kv|ÐºÐ²)', text.lower())
        area = Decimal(a_match.group(1)) if a_match else Decimal("0.00")
        
        # 3. Neighborhood Extraction (Crucial for Geo-Forensics)
        # Matches patterns like "Ð›ÑŽÐ»Ð¸Ð½ 6, Ð³Ñ€Ð°Ð´ Ð¡Ð¾Ñ„Ð¸Ñ" or "Ð³Ñ€Ð°Ð´ Ð¡Ð¾Ñ„Ð¸Ñ, Ð›ÑŽÐ»Ð¸Ð½ 6"
        kv_match = re.search(r'([\w\s\d-]+),\s*Ð³Ñ€Ð°Ð´ Ð¡Ð¾Ñ„Ð¸Ñ', text)
        if not kv_match:
            kv_match = re.search(r'Ð³Ñ€Ð°Ð´ Ð¡Ð¾Ñ„Ð¸Ñ,\s*([\w\s\d-]+)', text)
        
        neighborhood = kv_match.group(1).strip() if kv_match else "Unknown"

        # 4. Image Extraction
        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src and 'imot.bg' in src and 'picturess' in src:
                if src.startswith("//"): src = "https:" + src
                images.append(src)

        return ScrapedListing(
            source_url=url,
            raw_text=text,
            price_predicted=price_decimal,
            area_sqm=area,
            neighborhood=neighborhood,
            image_urls=list(set(images)) # Deduplicate
        )


=========================================
FILE: ./src/services/risk_engine.py
=========================================
from typing import Dict, Any

class RiskEngine:
    def calculate_score_v2(self, data: Dict) -> Dict[str, Any]:
        score = 0
        flags = []
        is_fatal = False
        
        scraped = data.get("scraped", {})
        ai = data.get("ai", {})
        cad = data.get("cadastre") or {}
        comp = data.get("compliance", {})
        risk = data.get("city_risk", {})
        geo = data.get("geo", {})
        
        # 1. EXPROPRIATION (The Nuke)
        if risk.get("is_expropriated"):
            score = 100
            is_fatal = True
            flags.append("CRITICAL: Property is listed for EXPROPRIATION (Municipal Seizure).")

        # 2. LOCATION INTEGRITY
        if geo and not geo.get("match"):
            score += 40
            flags.append(geo.get("warning", "Location Fraud Detected."))

        # 3. CONSTRUCTION MATURITY (ACT 16 Logic)
        # Penalize if building is modern (>2010) but no cert is found in the registry
        est_year = ai.get("construction_year_est", 0)
        if est_year > 2010 and not comp.get("has_act16") and comp.get("checked"):
            score += 50
            flags.append(f"HIGH RISK: Building estimated from {est_year}, but no Act 16 found in NAG records.")

        # 4. INFRASTRUCTURE MISMATCH (TEC/Radiator Logic)
        raw_text = scraped.get("raw_text", "").upper()
        if "Ð¢Ð•Ð¦" in raw_text or "Ð¦Ð•ÐÐ¢Ð ÐÐ›ÐÐž ÐžÐ¢ÐžÐŸÐ›Ð•ÐÐ˜Ð•" in raw_text:
            inventory = ai.get("heating_inventory", {})
            if inventory.get("radiators", 0) == 0:
                score += 15
                flags.append("WARN: Listing claims Central Heating (TEC), but 0 radiators detected visually.")

        # 5. AREA FRAUD
        adv_area = float(scraped.get("area_sqm", 0))
        off_area = float(cad.get("official_area", 0))
        if adv_area > 0 and off_area > 0:
            diff_ratio = (adv_area - off_area) / off_area
            if diff_ratio > 0.25:
                score += 30
                flags.append(f"SCAM: Advertised area {adv_area}m is {diff_ratio:.1%} larger than Official {off_area}m.")

        # 6. ATELIER STATUS
        if ai.get("is_atelier"):
            score += 25
            flags.append("LEGAL: Non-residential 'Atelier' status confirmed via vision analysis.")

        final_score = 100 if is_fatal else min(score, 100)
        return {"score": final_score, "flags": flags, "is_fatal": is_fatal}


=========================================
FILE: ./src/services/cadastre_service.py
=========================================


=========================================
FILE: ./src/services/geospatial_service.py
=========================================
import httpx
from src.core.logger import logger
from src.schemas import GeoVerification
from typing import Optional

class GeospatialService:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://maps.googleapis.com/maps/api/geocode/json"

    async def verify_neighborhood(self, ai_prediction: str, ai_landmarks: list, claimed_kvartal: str) -> GeoVerification:
        if self.api_key == "mock-key":
            return GeoVerification(match=True, detected_neighborhood="Mock", confidence=100)

        # Build a search query prioritizing specific clues from Gemini
        search_query = f"{ai_prediction} {' '.join(ai_landmarks)}, Sofia, Bulgaria"
        
        async with httpx.AsyncClient() as client:
            resp = await client.get(self.base_url, params={"address": search_query, "key": self.api_key})
            data = resp.json()

            if data["status"] != "OK" or not data["results"]:
                return GeoVerification(match=True, detected_neighborhood="Not Found", confidence=0)

            result = data["results"][0]
            lat_lng = result["geometry"]["location"]
            formatted_address = result.get("formatted_address", "")
            
            # Extract neighborhood from Google components
            detected = ""
            for comp in result["address_components"]:
                if any(t in comp["types"] for t in ["sublocality", "neighborhood", "political"]):
                    detected = comp["long_name"]
                    break
            
            # Cross-reference
            claimed_norm = claimed_kvartal.lower().replace("Ð³Ñ€.", "").strip()
            detected_norm = detected.lower().strip()
            
            # Match if strings overlap (e.g., "Krastova Vada" vs "Manastirski Livadi - East")
            is_match = claimed_norm in detected_norm or detected_norm in claimed_norm
            
            return GeoVerification(
                match=is_match,
                detected_neighborhood=detected,
                confidence=90,
                lat=lat_lng["lat"],
                lng=lat_lng["lng"],
                warning=None if is_match else f"LOCATION FRAUD: Ad claims {claimed_kvartal}, but Vision/Maps identifies {detected}.",
                best_address=formatted_address
            )


=========================================
FILE: ./src/services/base_provider.py
=========================================
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any

class BaseRegistryProvider(ABC):
    @abstractmethod
    def fetch_details(self, address: str) -> Optional[Dict[str, Any]]:
        """Fetch official property data from a government registry."""
        pass

class BaseGeoProvider(ABC):
    @abstractmethod
    def geocode(self, address: str) -> Dict[str, Any]:
        """Convert address to GPS coordinates."""
        pass


=========================================
FILE: ./src/services/storage_service.py
=========================================
import os
import httpx
import asyncio
from typing import List

class StorageService:
    def __init__(self, upload_dir="storage/archive"):
        self.upload_dir = upload_dir
        os.makedirs(upload_dir, exist_ok=True)

    async def _download_single(self, client: httpx.AsyncClient, url: str, filename: str) -> str:
        try:
            resp = await client.get(url, timeout=7.0)
            if resp.status_code == 200:
                path = os.path.join(self.upload_dir, filename)
                with open(path, "wb") as f:
                    f.write(resp.content)
                return path
        except Exception as e:
            print(f"Archive Fail: {url} -> {e}")
        return None

    async def archive_images(self, listing_id: int, urls: List[str]) -> List[str]:
        if not urls: return []
        async with httpx.AsyncClient() as client:
            tasks = []
            for i, url in enumerate(urls):
                filename = f"listing_{listing_id}_{i}.jpg"
                tasks.append(self._download_single(client, url, filename))
            results = await asyncio.gather(*tasks)
            return [r for r in results if r is not None]


=========================================
FILE: ./src/services/legal_engine.py
=========================================
import os
import re
from typing import Optional, List
from src.core.patterns import ForensicPatterns

class LegalKnowledgeBase:
    """Search engine for the local law database."""
    def __init__(self, laws_path: str = "storage/laws"):
        self.laws_path = laws_path

    def get_article(self, law_name: str, article_num: int) -> Optional[str]:
        """Extracts a specific Article text from the law file."""
        file_path = os.path.join(self.laws_path, f"{law_name}.txt")
        if not os.path.exists(file_path):
            return None
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        
        # Matches 'Ð§Ð». [num].' until the next Article or end of file
        pattern = rf"(Ð§Ð»\. {article_num}\..*?)(?=\nÐ§Ð»\. \d+|\Z)"
        match = re.search(pattern, content, re.DOTALL)
        return match.group(1).strip() if match else None

    def search_context(self, keyword: str, limit: int = 1) -> List[str]:
        """Finds sentences containing specific legal terms."""
        results = []
        if not os.path.exists(self.laws_path): return results
        for law_file in os.listdir(self.laws_path):
            if not law_file.endswith(".txt"): continue
            with open(os.path.join(self.laws_path, law_file), "r", encoding="utf-8") as f:
                content = f.read()
                matches = re.findall(rf"([^.\n]*{keyword}[^.\n]*)", content, re.IGNORECASE)
                for m in matches[:limit]:
                    results.append(f"{m.strip()} (Ref: {law_file})")
        return results

# Initialize the Knowledge Base Instance
kb = LegalKnowledgeBase()

class LegalEngine:
    def analyze_listing(self, scraped_data: dict, ai_data: dict):
        risk_report = {
            "total_legal_score": 0,
            "pillars": {},
            "gatekeeper_verdict": "CLEAR",
            "flags": []
        }
        raw_text = scraped_data.get("raw_text", "").upper()
        regex_flags = ForensicPatterns.extract_flags(raw_text)
        risk_report["flags"].extend(regex_flags)
        
        p1_score = 0
        if ai_data.get("is_atelier") or "ÐÐ¢Ð•Ð›Ð˜Ð•" in raw_text:
            p1_score = 35
            risk_report["flags"].append("LEGAL: Non-residential status (Atelier).")
        
        risk_report["pillars"]["classification"] = p1_score
        risk_report["total_legal_score"] = p1_score
        return risk_report


=========================================
FILE: ./src/services/report_generator.py
=========================================
import datetime
from src.services.legal_engine import kb

class AttorneyReportGenerator:
    def generate_legal_brief(self, listing_data: dict, risk_data: dict, ai_data: dict) -> str:
        score = risk_data.get("score", 0)
        flags = risk_data.get("flags", [])
        forensics = risk_data.get("forensics", {})
        
        status_symbol = "ðŸŸ¢ CLEAR"
        if risk_data.get("is_fatal"): status_symbol = "ðŸ”´ DO NOT PROCEED"
        elif score > 60: status_symbol = "ðŸŸ  HIGH RISK ASSET"
        elif score > 30: status_symbol = "ðŸŸ¡ CAUTION ADVISED"

        sections = [
            f"# {status_symbol} (Risk Score: {score}/100)",
            f"**Generated:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}",
            "\n## I. Executive Summary",
            self._summary(risk_data),
            "\n## II. Forensic Evidence (Visual)",
            self._visual_forensics(ai_data, forensics.get("geo", {})),
            "\n## III. Statutory Analysis (Bulgarian Law)",
            self._legal_citations(ai_data, listing_data),
            "\n## IV. Cadastral & Registry Data",
            self._cadastre_section(listing_data, forensics.get("cadastre", {})),
            "\n## V. Identified Risk Factors",
            "\n".join([f"- {f}" for f in flags]) if flags else "- No specific flags raised."
        ]
        return "\n".join(sections)

    def _summary(self, risk):
        if risk.get("is_fatal"): return "CRITICAL: The property has fatal legal or registry defects."
        return "Manual verification against Lex.bg records and Registry IDs completed."

    def _visual_forensics(self, ai, geo):
        return f"- **Location Match:** {'âœ… OK' if geo.get('match') else 'âŒ MISMATCH'}\n" \
               f"- **View Check:** {ai.get('neighborhood_match')}\n" \
               f"- **Building Type:** {ai.get('building_type')}\n" \
               f"- **Infrastructure:** {ai.get('heating_inventory', {}).get('ac_units')} AC units detected."

    def _legal_citations(self, ai, scraped):
        citations = []
        if ai.get("is_atelier"):
            text = kb.get_article("naredba_7", 110)
            citations.append(f"**Classification: Atelier detected.**")
            if text: citations.append(f"Citing Ordinance No. 7: {text[:250]}...")
        
        if "Ð¿Ð»Ð¾Ñ‰" in scraped.get("raw_text", "").lower():
            ref = kb.search_context("Ð—Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð° Ð¿Ð»Ð¾Ñ‰")
            if ref: citations.append(f"**Area Standard:** {ref[0]}")
            
        return "\n".join(citations) if citations else "No significant statutory discrepancies found."

    def _cadastre_section(self, listing, cad):
        off = cad.get("official_area", 0)
        adv = listing.get("area_sqm", 0)
        return f"- **Official Area:** {off} m2\n- **Advertised Area:** {adv} m2\n- **Cadastre ID:** {cad.get('cadastre_id')}"


=========================================
FILE: ./src/services/compliance_service.py
=========================================
import httpx
import uuid
from src.core.logger import logger

class ComplianceService:
    BASE_URL = 'https://nag.sofia.bg/RegisterCertificateForExploitationBuildings'
    
    def __init__(self, client: httpx.AsyncClient = None):
        self.client = client

    async def check_act_16(self, cadastre_id: str) -> dict:
        log = logger.bind(cadastre_id=cadastre_id)
        if not cadastre_id: 
            return {"has_act16": False, "checked": False}

        # Use injected client (preferred) or create ephemeral one
        # REMOVED verify=False. Production requires valid SSL or mounted CAs.
        local_client = self.client if self.client else httpx.AsyncClient(timeout=10.0)
        
        try:
            search_params = {'searchQueryId': str(uuid.uuid4()), 'Identifier': cadastre_id}
            
            # 1. Search
            await local_client.get(f"{self.BASE_URL}/Search", params=search_params)
            
            # 2. Read
            res = await local_client.post(f"{self.BASE_URL}/Read", data={'page': '1', 'pageSize': '10'})
            res.raise_for_status()
            data = res.json()
            
            has_cert = len(data.get("Data", [])) > 0
            log.info("compliance_check", found=has_cert)
            
            return {
                "has_act16": has_cert, 
                "checked": True,
                "registry_status": "LIVE"
            }

        except (httpx.ConnectError, httpx.TimeoutException, httpx.HTTPStatusError) as e:
            log.warning("registry_down", error=str(e))
            return {"has_act16": False, "checked": False, "registry_status": "OFFLINE"}
            
        finally:
            # Only close if we created it locally
            if not self.client:
                await local_client.aclose()


=========================================
FILE: ./src/services/city_risk_service.py
=========================================
import httpx
from src.core.logger import logger

class CityRiskService:
    BASE_URL = 'https://nag.sofia.bg/RegisterExpropriation'
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'X-Requested-With': 'XMLHttpRequest',
        'Origin': 'https://nag.sofia.bg'
    }

    def __init__(self, client: httpx.AsyncClient = None):
        self.client = client

    async def check_expropriation(self, cadastre_id: str, district: str = "") -> dict:
        log = logger.bind(cadastre_id=cadastre_id)
        if not cadastre_id: return {"is_expropriated": False}

        local_client = self.client if self.client else httpx.AsyncClient(headers=self.HEADERS, timeout=10.0)

        try:
            # 1. Set Context
            await local_client.post(f"{self.BASE_URL}/Search", data={'CadNumber': cadastre_id, 'RegionName': district})
            
            # 2. Fetch
            res = await local_client.post(f"{self.BASE_URL}/Read", data={'page': '1', 'pageSize': '10'})
            res.raise_for_status()
            data = res.json()

            is_risk = data and data.get("Data") and len(data["Data"]) > 0
            
            if is_risk:
                log.critical("expropriation_risk_found", details=str(data["Data"][0]))
                return {
                    "is_expropriated": True, 
                    "details": str(data["Data"][0]),
                    "risk_level": "CRITICAL",
                    "registry_status": "LIVE"
                }
            
            return {"is_expropriated": False, "registry_status": "LIVE"}

        except (httpx.ConnectError, httpx.TimeoutException) as e:
            log.warning("registry_down", error=str(e))
            return {"is_expropriated": False, "registry_status": "OFFLINE"}
        finally:
            if not self.client:
                await local_client.aclose()


=========================================
FILE: ./src/services/forensics_service.py
=========================================
import httpx
import uuid
import time
import asyncio
from src.core.logger import logger
from typing import Optional, Dict, Any

class SofiaMunicipalForensics:
    """
    Direct interface to Sofia Municipality (NAG) Registries.
    Implements the "Nuclear Option" (Expropriation) and "Green Light" (Act 16) checks.
    """
    
    BASE_URL = "https://nag.sofia.bg"
    
    # Headers exactly as captured in curl (User-Agent + X-Requested-With are critical)
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:141.0) Gecko/20100101 Firefox/141.0',
        'Accept': '*/*',
        'Accept-Language': 'en-US,en;q=0.5',
        'X-Requested-With': 'XMLHttpRequest',
        'Connection': 'keep-alive',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-origin',
    }

    def __init__(self, client: httpx.AsyncClient = None):
        self.client = client

    async def run_full_audit(self, cadastre_id: str) -> Dict[str, Any]:
        """
        Runs the 3-Dimensional Audit:
        1. Expropriation (The "Death List")
        2. Act 16 (The "Green List")
        3. Permits (The "Construction History")
        """
        if not cadastre_id:
            return {"error": "No Cadastre ID provided"}

        # Use injected client or create a new ephemeral one
        local_client = self.client if self.client else httpx.AsyncClient(headers=self.HEADERS, timeout=20.0, follow_redirects=True)

        try:
            # Run all 3 checks in parallel for speed
            expropriation, act16, permits = await asyncio.gather(
                self._check_expropriation(local_client, cadastre_id),
                self._check_act16(local_client, cadastre_id),
                self._check_permits(local_client, cadastre_id)
            )

            return {
                "expropriation": expropriation,
                "compliance_act16": act16,
                "building_permits": permits,
                "audit_timestamp": time.time()
            }
        except Exception as e:
            logger.error(f"audit_failed_fatal: {e}")
            return {"error": str(e)}
        finally:
            if not self.client:
                await local_client.aclose()

    async def _check_expropriation(self, client, cid: str) -> Dict:
        """
        Checks 'RegisterExpropriation' (The Death List).
        """
        # 1. SEARCH: Sets the server-side session state
        search_id = str(uuid.uuid4())
        params = {
            'searchQueryId': search_id,
            'RegionName': '', 'RegPlan': '', 'Quarter': '', 'Upi': '',
            'CadNumber': cid,  # <--- TARGET
            'Area': '', 'Obj': '', 'GroupType': '', 'Typology': '', 'StructureZone': '',
            'ApprYear': '', 'X-Requested-With': 'XMLHttpRequest',
            '_': int(time.time() * 1000)
        }
        
        try:
            # Step A: Prime the search
            await client.get(f"{self.BASE_URL}/RegisterExpropriation/Search", params=params)
            
            # Step B: Read the results (Page 1)
            # The server expects a POST to /Read after the GET /Search
            res = await client.post(f"{self.BASE_URL}/RegisterExpropriation/Read", data={'page': 1, 'pageSize': 10})
            data = res.json()
            
            # SAFEGUARD: Ensure hits is a list, even if API returns null
            hits = data.get("Data") or []
            is_fatal = len(hits) > 0
            
            return {
                "is_expropriated": is_fatal,
                "risk_level": "CRITICAL" if is_fatal else "NONE",
                "details": hits[0] if is_fatal else None,
                "found_count": len(hits)
            }
        except Exception as e:
            logger.error(f"expropriation_check_failed: {e}")
            return {"error": str(e), "is_expropriated": False}

    async def _check_act16(self, client, cid: str) -> Dict:
        """
        Checks 'RegisterCertificateForExploitationBuildings' (The Green List).
        """
        search_id = str(uuid.uuid4())
        params = {
            'searchQueryId': search_id,
            'IssuedById': '', 'FromDate': '', 'ToDate': '', 'StatusId': '',
            'DocumentTypeName': '', 'Number': '', 'Status': '', 'Issuer': '',
            'Employer': '', 'ConstructionalOversightName': '', 'Object': '',
            'Region': '', 'Terrain': '', 'RegulationNeighbourhood': '', 'Upi': '',
            'Identifier': cid, # <--- TARGET
            'Address': '', 'RegionId': '', 'TakeEffectFilter': '', 
            'MapOfRestoredProperty': '', 'AdditionalDescriptionEstate': '',
            'AdditionalDescriptionAdministrativeAddress': '', 'Scope': '',
            'X-Requested-With': 'XMLHttpRequest',
            '_': int(time.time() * 1000)
        }

        try:
            await client.get(f"{self.BASE_URL}/RegisterCertificateForExploitationBuildings/Search", params=params)
            res = await client.post(f"{self.BASE_URL}/RegisterCertificateForExploitationBuildings/Read", data={'page': 1, 'pageSize': 10})
            data = res.json()
            
            # SAFEGUARD: Use 'or []' to handle None
            hits = data.get("Data") or []
            has_cert = len(hits) > 0
            
            # Extract description if found
            cert_desc = hits[0].get("Ð¡Ñ‚Ñ€Ð¾ÐµÐ¶/ÐžÐ±ÐµÐºÑ‚", "N/A") if has_cert else None
            
            return {
                "has_act16": has_cert,
                "description": cert_desc,
                "raw_hits": len(hits)
            }
        except Exception as e:
            logger.error(f"act16_check_failed: {e}")
            return {"error": str(e), "has_act16": False}

    async def _check_permits(self, client, cid: str) -> Dict:
        """
        Checks 'RegisterBuildingPermitsPortal' (Construction History).
        """
        search_id = str(uuid.uuid4())
        params = {
            'searchQueryId': search_id,
            'IssuedById': '', 'FromDate': '', 'ToDate': '', 'TakeEffectFrom': '',
            'TakeEffectTo': '', 'StatusId': '', 'DocumentTypeName': '', 'Number': '',
            'Status': '', 'Issuer': '', 'Employer': '', 'ConstructionalOversightName': '',
            'Object': '', 'Region': '', 'Terrain': '', 'RegulationNeighbourhood': '',
            'Upi': '', 
            'Identifier': cid, # <--- TARGET
            'Address': '', 'RegionId': '', 'TakeEffectFilter': '', 
            'MapOfRestoredProperty': '', 'AdditionalDescriptionEstate': '',
            'AdditionalDescriptionAdministrativeAddress': '', 'Scope': '',
            'X-Requested-With': 'XMLHttpRequest',
            '_': int(time.time() * 1000)
        }

        try:
            await client.get(f"{self.BASE_URL}/RegisterBuildingPermitsPortal/Search", params=params)
            res = await client.post(f"{self.BASE_URL}/RegisterBuildingPermitsPortal/Read", data={'page': 1, 'pageSize': 10})
            data = res.json()
            
            return {
                "total_permits": data.get("Total", 0),
                "latest_permit": data.get("Data", [])[0] if data.get("Data") else None
            }
        except Exception as e:
            return {"error": str(e), "total_permits": 0}


=========================================
FILE: ./src/__init__.py
=========================================


=========================================
FILE: ./src/main.py
=========================================
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from src.api import routes

app = FastAPI(
    title="Glashaus API",
    description="Automated Real Estate Due Diligence Engine",
    version="1.0.0"
)

# Allow connections from Frontend/Dashboard
origins = [
    "http://localhost",
    "http://localhost:3000",
    "http://localhost:8080",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(routes.router)

@app.get("/")
def health_check():
    return {
        "system": "GLASHAUS", 
        "status": "OPERATIONAL", 
        "version": "1.0.0-PROD"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("src.main:app", host="0.0.0.0", port=8000)


=========================================
FILE: ./src/db/session.py
=========================================
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
from src.core.config import settings

# SQLite requires a specific argument for threading
connect_args = {"check_same_thread": False} if "sqlite" in settings.DATABASE_URL else {}

engine = create_engine(
    settings.DATABASE_URL, 
    connect_args=connect_args,
    pool_pre_ping=True
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


=========================================
FILE: ./src/db/models.py
=========================================
import enum
from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, ForeignKey, JSON, Text, Enum, Numeric
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from src.db.session import Base

class ReportStatus(str, enum.Enum):
    PENDING = "PENDING"
    PROCESSING = "PROCESSING"
    VERIFIED = "VERIFIED"
    MANUAL_REVIEW = "MANUAL_REVIEW"
    REJECTED = "REJECTED"

class Listing(Base):
    __tablename__ = "listings"
    id = Column(Integer, primary_key=True, index=True)
    source_url = Column(String, unique=True, nullable=False, index=True)
    content_hash = Column(String(64), index=True)
    
    # Financial Precision
    price_bgn = Column(Numeric(12, 2)) 
    
    # Area Precision (Fixed from Float)
    advertised_area_sqm = Column(Numeric(10, 2))
    
    description_raw = Column(Text)
    scraped_at = Column(DateTime(timezone=True), server_default=func.now())
    
    reports = relationship("Report", back_populates="listing", cascade="all, delete-orphan")
    price_history = relationship("PriceHistory", back_populates="listing")

class Building(Base):
    __tablename__ = "buildings"
    id = Column(Integer, primary_key=True)
    cadastre_id = Column(String, unique=True, index=True)
    address_full = Column(String)
    latitude = Column(Float)
    longitude = Column(Float)
    construction_year = Column(Integer)
    reports = relationship("Report", back_populates="building")

class PriceHistory(Base):
    __tablename__ = "price_history"
    id = Column(Integer, primary_key=True)
    listing_id = Column(Integer, ForeignKey("listings.id"))
    price_bgn = Column(Numeric(12, 2))
    changed_at = Column(DateTime(timezone=True), server_default=func.now())
    listing = relationship("Listing", back_populates="price_history")

class Report(Base):
    __tablename__ = "reports"
    id = Column(Integer, primary_key=True, index=True)
    listing_id = Column(Integer, ForeignKey("listings.id"))
    building_id = Column(Integer, ForeignKey("buildings.id"), nullable=True)
    status = Column(Enum(ReportStatus), default=ReportStatus.PENDING)
    risk_score = Column(Integer)
    ai_confidence_score = Column(Integer, default=0)
    legal_brief = Column(Text)
    discrepancy_details = Column(JSON)
    image_archive_urls = Column(JSON)
    
    cost_to_generate = Column(Numeric(10, 4)) 
    
    manual_review_notes = Column(Text, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    listing = relationship("Listing", back_populates="reports")
    building = relationship("Building", back_populates="reports")


=========================================
FILE: ./src/worker.py
=========================================
import os
from celery import Celery
from src.core.config import settings

celery_app = Celery(
    "glashaus_worker",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL
)

celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="Europe/Sofia",
    enable_utc=True,
)

# Auto-discover tasks in src/tasks.py
celery_app.autodiscover_tasks(['src.tasks'])


=========================================
FILE: ./src/tasks.py
=========================================
from asgiref.sync import async_to_sync
import asyncio
import httpx
from src.worker import celery_app
from src.db.session import SessionLocal
from src.db.models import Listing, Report, ReportStatus, Building
from src.services.scraper_service import ScraperService
from src.services.ai_engine import GeminiService
from src.services.storage_service import StorageService
from src.services.geospatial_service import GeospatialService
from src.services.cadastre_service import CadastreService
from src.services.forensics_service import SofiaMunicipalForensics
from src.services.risk_engine import RiskEngine
from src.services.report_generator import AttorneyReportGenerator
from src.core.config import settings
from src.core.logger import logger
from src.core.utils import normalize_sofia_street

@celery_app.task(name="src.tasks.audit_listing")
def audit_listing_task(listing_id: int):
    return async_to_sync(run_audit_pipeline)(listing_id)

async def run_audit_pipeline(listing_id: int):
    log = logger.bind(listing_id=listing_id)
    
    async with httpx.AsyncClient(timeout=30.0) as http_client:
        with SessionLocal() as db:
            listing = db.query(Listing).get(listing_id)
            if not listing: return "Error: Listing not found"

            # 1. SCRAPE
            scraper = ScraperService(client=http_client)
            scraped_data = await scraper.scrape_url(listing.source_url)
            
            # 2. VISION
            storage = StorageService()
            img_paths = await storage.archive_images(listing_id, scraped_data.image_urls)
            ai_service = GeminiService(api_key=settings.GEMINI_API_KEY)
            ai_data = await ai_service.analyze_listing_multimodal(scraped_data.raw_text, img_paths)
            
            # 3. GEO TRIANGULATION
            geo_service = GeospatialService(api_key=settings.GOOGLE_MAPS_API_KEY)
            geo_report = await geo_service.verify_neighborhood(
                ai_data.get("address_prediction"), 
                ai_data.get("landmarks", []), 
                scraped_data.neighborhood
            )
            
            # 4. REGISTRY (LINEAR)
            cadastre = CadastreService(client=http_client)
            best_address = normalize_sofia_street(geo_report.best_address or ai_data.get("address_prediction"))
            cad_data = await cadastre.get_official_details(best_address)
            
            # NEW: UNIFIED MUNICIPAL AUDIT
            mun_report = {"expropriation": {}, "compliance_act16": {}}
            building_id = None
            
            if cad_data.cadastre_id:
                # Parallel Municipal Strike
                forensics = SofiaMunicipalForensics(client=http_client)
                mun_report = await forensics.run_full_audit(cad_data.cadastre_id)
                
                # Persist Building Context
                existing_building = db.query(Building).filter(Building.cadastre_id == cad_data.cadastre_id).first()
                if not existing_building:
                    new_building = Building(
                        cadastre_id=cad_data.cadastre_id,
                        address_full=cad_data.address_found or geo_report.best_address,
                        latitude=geo_report.lat,
                        longitude=geo_report.lng,
                        construction_year=ai_data.get("construction_year_est")
                    )
                    db.add(new_building)
                    db.flush()
                    building_id = new_building.id
                else:
                    building_id = existing_building.id

            # 5. SCORING
            forensic_data = {
                "scraped": scraped_data.model_dump(),
                "ai": ai_data,
                "geo": geo_report.model_dump(),
                "cadastre": cad_data.model_dump(),
                # Map new service output to Risk Engine expected keys
                "compliance": mun_report.get("compliance_act16", {}),
                "city_risk": mun_report.get("expropriation", {})
            }
            risk_engine = RiskEngine()
            score_res = risk_engine.calculate_score_v2(forensic_data)
            
            # 6. REPORTING
            report_gen = AttorneyReportGenerator()
            report_text = report_gen.generate_legal_brief(scraped_data.model_dump(), {**score_res, "forensics": forensic_data}, ai_data)
            
            new_report = Report(
                listing_id=listing_id,
                building_id=building_id,
                risk_score=score_res["score"],
                legal_brief=report_text,
                discrepancy_details=forensic_data,
                status=ReportStatus.VERIFIED if score_res["score"] < 40 else ReportStatus.MANUAL_REVIEW
            )
            db.add(new_report)
            db.commit()
            
            return f"Audit Done: {score_res['score']}"


=========================================
FILE: ./src/schemas.py
=========================================
from pydantic import BaseModel, Field
from decimal import Decimal
from typing import List, Optional, Literal

class ScrapedListing(BaseModel):
    source_url: str
    raw_text: str
    price_predicted: Decimal
    area_sqm: Decimal
    neighborhood: str 
    image_urls: List[str] = []

class HeatingInventory(BaseModel):
    ac_units: int = 0
    radiators: int = 0
    has_central_heating: bool = False

class AIAnalysisResult(BaseModel):
    address_prediction: str
    landmarks: List[str] = []
    neighborhood_match: str
    building_type: str
    heating_inventory: HeatingInventory
    visual_red_flags: List[str] = []
    light_exposure: str
    confidence_score: int = Field(default=0, ge=0, le=100)

class GeoVerification(BaseModel):
    match: bool
    detected_neighborhood: str
    confidence: int
    lat: Optional[float] = None
    lng: Optional[float] = None
    warning: Optional[str] = None
    best_address: Optional[str] = None

class CadastreData(BaseModel):
    official_area: float = 0.0
    cadastre_id: Optional[str] = None
    status: Literal["LIVE", "OFFLINE", "NOT_FOUND", "ERROR"] = "NOT_FOUND"
    address_found: Optional[str] = None


=========================================
FILE: ./scripts/context_dump.sh
=========================================
#!/bin/bash
# Scans the repo and dumps text files for LLM context

output="glashaus_context.txt"
echo "--- GLASHAUS PROJECT DUMP ---" > "$output"
date >> "$output"

echo -e "\n\n--- GIT HISTORY ---" >> "$output"
git log --oneline --graph --decorate -n 20 >> "$output"

echo -e "\n\n--- FILE STRUCTURE ---" >> "$output"
# Exclude git, pycache, and compiled python files from tree view
tree -L 3 -I '.git|__pycache__|*.pyc' >> "$output" 2>/dev/null || find . -maxdepth 3 -not -path '*/.*' >> "$output"

echo -e "\n\n--- FILE CONTENTS ---" >> "$output"
find . -type f \
    -not -path '*/.*' \
    -not -path '*/__pycache__*' \
    -not -name '*.pyc' \
    -not -name 'cookies.txt' \
    -not -path './glashaus_context.txt' \
    -not -name 'bypass_audit.py' \
    -not -name 'manual_session_audit.py' \
    -not -path './scraper_service.py' \
    -not -name '*.png' \
    -not -name '*.jpg' \
    -not -name '*.sqlite' \
    | while read -r file; do
    echo -e "\n\n=========================================" >> "$output"
    echo "FILE: $file" >> "$output"
    echo "=========================================" >> "$output"
    cat "$file" >> "$output"
done

echo "Dump complete. Clean contents saved to $output"


=========================================
FILE: ./scripts/scrape_lex.py
=========================================
import httpx
from bs4 import BeautifulSoup
import os
import re
import sys

def scrape_law(url: str, filename: str):
    print(f"[*] Targeting Lex.bg: {url}")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 Mobile Safari/604.1"
    }

    try:
        # 1. Fetch raw bytes to handle encoding manually
        with httpx.Client(headers=headers, timeout=60.0) as client:
            resp = client.get(url)
            resp.raise_for_status()
            
            # Decode using windows-1251 as identified in our curl test
            content = resp.content.decode('windows-1251', errors='replace')
            
        # 2. Parse HTML
        soup = BeautifulSoup(content, 'html.parser')
        
        # 3. Clean up the DOM (Remove ads, menus, scripts)
        # We saw these in your 'cnt' div test
        for noise in soup(["script", "style", "iframe", "ins"]):
            noise.decompose()
            
        ad_patterns = [r"adocean", r"mobileMenu", r"tma-ad", r"h-cnt"]
        for div in soup.find_all("div"):
            div_id = div.get("id", "")
            div_class = " ".join(div.get("class", []))
            if any(re.search(p, div_id) or re.search(p, div_class) for p in ad_patterns):
                div.decompose()

        # 4. Target the Law Body
        # Based on your grep, 'cnt' is the wrapper and 'boxi' is the inner content
        law_body = soup.find("div", class_="boxi") or soup.find("div", class_="cnt")
        
        if not law_body:
            print("[!] Could not find law container. Saving whole page.")
            law_body = soup.body

        # 5. Extract text with formatting
        # We use a separator to keep Articles (Ð§Ð».) on new lines
        raw_text = law_body.get_text(separator="\n")
        
        # 6. Post-Processing Cleanup
        # Remove massive gaps of whitespace
        clean_text = re.sub(r'\n\s*\n', '\n\n', raw_text)
        # Standardize the 'Article' prefix for easier RAG searching later
        clean_text = re.sub(r'(?m)^Ð§Ð»\.', '\nÐ§Ð».', clean_text)

        # 7. Save to disk
        os.makedirs("storage/laws", exist_ok=True)
        path = f"storage/laws/{filename}.txt"
        
        with open(path, "w", encoding="utf-8") as f:
            f.write(f"SOURCE: {url}\n")
            f.write("-" * 30 + "\n\n")
            f.write(clean_text)
            
        print(f"[SUCCESS] {filename}.txt created ({len(clean_text)} characters)")

    except Exception as e:
        print(f"[FATAL] Error during scraping: {str(e)}")

if __name__ == "__main__":
    # Link 1: Ordinance No. 7 (Crucial for Atelier vs Apartment height/light rules)
    ORD_7_URL = "https://lex.bg/mobile/ldoc/2135476546"
    scrape_law(ORD_7_URL, "naredba_7")

    # Link 2: ZUT (The Foundation)
    ZUT_URL = "https://lex.bg/mobile/ldoc/2135163904"
    scrape_law(ZUT_URL, "zut_law")



=========================================
FILE: ./scripts/debug_kais.py
=========================================
import requests
from bs4 import BeautifulSoup
import time
import re
import argparse
import sys
import json

# CONFIG
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
BASE_URL = "https://kais.cadastre.bg/bg/Map"

def run_forensics(args):
    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "X-Requested-With": "XMLHttpRequest"
    })

    print("[1] HANDSHAKE...")
    try:
        resp = session.get(BASE_URL)
        soup = BeautifulSoup(resp.text, 'html.parser')
        token_el = soup.find('input', {'name': '__RequestVerificationToken'})
        if not token_el:
            print("FAILED: No token found. KAIS might be blocking.")
            return
        token = token_el.get('value')
    except Exception as e:
        print(f"Handshake Failed: {e}")
        return

    # --- QUERY CONSTRUCTION ---
    if args.query:
        search_kw = args.query
    else:
        # Construct "Google-style" query for FastSearch
        parts = []
        if args.district: parts.append(args.district)
        if args.street:   parts.append(args.street)
        if args.block:    parts.append(f"Ð±Ð» {args.block}")
        if args.number:   parts.append(args.number)
        if args.entrance: parts.append(f"Ð²Ñ… {args.entrance}")
        search_kw = " ".join(parts)

    print(f"[2] TARGETING: '{search_kw}'...")
    
    # Always use FastSearch (more reliable than Detailed)
    session.get(f"{BASE_URL}/FastSearch", params={"KeyWords": search_kw})
    time.sleep(0.5)

    print("[3] FETCHING LIST...")
    headers = {"X-CSRF-TOKEN": token, "Referer": BASE_URL}

    # Fetch enough results to filter client-side
    resp = session.post(
        f"{BASE_URL}/ReadFoundObjects",
        data={"page": 1, "pageSize": args.limit * 5}, 
        headers=headers
    )

    data = resp.json()
    all_objects = data.get("Data", [])
    print(f"    -> Hits: {len(all_objects)}")

    # --- INTELLIGENT FILTERING ---
    filtered_objects = []
    for obj in all_objects:
        raw_display = f"{obj.get('DisplayText', '')} {obj.get('Address', '')}".lower()
        cad_num = obj.get('Number', '')
        
        # Filter: Exclude Apartments if requested (finding plots/buildings only)
        if args.plots_only:
             # PI (2 dots) or Building (3 dots). Apartments have 4+.
            if cad_num.count('.') > 3: continue 

        match = True
        
        # Filter: Strict Number Match
        if args.number and not args.query:
            # Regex for " 10 ", "No 10", "â„–10"
            pattern = r'(?:â„–|no\.?|\s|^)' + re.escape(args.number) + r'(?:\s|$|,|\.)'
            if not re.search(pattern, raw_display):
                match = False

        if match:
            filtered_objects.append(obj)

    target_list = filtered_objects[:args.limit]
    print(f"    -> Relevant Matches: {len(target_list)}")
    print(f"\n[4] FORENSIC SCANNING...")

    for obj in target_list:
        cad_num = obj.get('Number') or "N/A"
        
        try:
            info_url = f"{BASE_URL}/GetObjectInfo"
            resp_deep = session.get(info_url, params=obj)
            full_text = BeautifulSoup(resp_deep.text, 'html.parser').get_text(" ", strip=True)
            
            # --- PARSING LOGIC ---
            
            # Area
            area_m = re.search(r'Ð¿Ð»Ð¾Ñ‰\s*([\d\.]+)\s*ÐºÐ²', full_text, re.IGNORECASE)
            if not area_m: area_m = re.search(r'([\d\.]+)\s*ÐºÐ²\.\s*Ð¼', full_text, re.IGNORECASE)
            area = area_m.group(1) if area_m else "?"

            # Floor
            floor_m = re.search(r'(?:ÐµÑ‚Ð°Ð¶|ÐµÑ‚\.)\s*(\d+)', full_text, re.IGNORECASE)
            floor = floor_m.group(1) if floor_m else "-"

            # Ownership
            own_m = re.search(r'Ð²Ð¸Ð´ ÑÐ¾Ð±ÑÑ‚Ð²\.\s*([^,]+)', full_text, re.IGNORECASE)
            ownership = own_m.group(1).strip() if own_m else "?"

            # Neighbors (Crucial for Triangulation)
            neighbors_m = re.search(r'Ð¡ÑŠÑÐµÐ´Ð¸\s*:?\s*([\d\.,\s]+)', full_text, re.IGNORECASE)
            neighbors = neighbors_m.group(1).strip() if neighbors_m else "NONE"

            # Property Type Classification
            ft_lower = full_text.lower()
            if "Ð¿Ð¸ " in ft_lower[:20] or "Ð¿Ð¾Ð·ÐµÐ¼Ð»ÐµÐ½" in ft_lower: ptype = "LAND (ÐŸÐ˜)"
            elif "ÑÐ³Ñ€Ð°Ð´Ð°" in ft_lower: ptype = "BUILDING"
            elif "Ð¶Ð¸Ð»Ð¸Ñ‰Ðµ" in ft_lower or "Ð°Ð¿Ð°Ñ€Ñ‚Ð°Ð¼ÐµÐ½Ñ‚" in ft_lower: ptype = "APARTMENT"
            else: ptype = "UNKNOWN"

            # --- REPORT ---
            print(f"\n--- {cad_num} ---")
            print(f"TYPE:  {ptype}")
            print(f"OWNER: {ownership}")
            print(f"AREA:  {area} m2")
            print(f"FLOOR: {floor}")
            if args.verbose:
                print(f"NEIGHBORS: {neighbors}")
            
            # If specifically looking for ownership/plot info, dump raw text
            if args.dump or ownership == "?" or "ÐÑÐ¼Ð° Ð´Ð°Ð½Ð½Ð¸" in ownership:
                print(f"RAW DUMP: {full_text[:500]}...")

        except Exception as e:
            print(f"    Error: {e}")

        time.sleep(0.2)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='KAIS Forensic Tool')
    
    # Flexible Input
    parser.add_argument('-q', '--query', help='Direct query string (e.g. ID or Address)')
    
    # Structured Input
    parser.add_argument('-d', '--district', help='District name')
    parser.add_argument('-b', '--block', help='Block number')
    parser.add_argument('-s', '--street', help='Street name')
    parser.add_argument('-n', '--number', help='Street number')
    parser.add_argument('-e', '--entrance', help='Entrance')
    
    # Toggles
    parser.add_argument('--plots-only', action='store_true', help='Ignore apartments, find land/buildings')
    parser.add_argument('--verbose', action='store_true', help='Show neighbors and extra details')
    parser.add_argument('--dump', action='store_true', help='Always dump raw text')
    parser.add_argument('--limit', type=int, default=15, help='Max results')

    args = parser.parse_args()
    
    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(1)
        
    run_forensics(args)



=========================================
FILE: ./scripts/audit_target.py
=========================================
import sys
import os
import argparse
Ensure src is in pythonpath
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(file), '..')))
from src.services.cadastre_service import CadastreForensicsService
def run_audit(target: str, mode: str):
service = CadastreForensicsService()
print(f"--- GLASHAUS FORENSICS: {target} ---")

if mode == "scan":
    # Standard Scan
    results = service.search_object(target)
    print(f"[*] Found {len(results)} matches. Analyzing...")
    
    for obj in results[:20]: # Limit default view to 20
        data = service.deep_scan_unit(obj)
        print(f"\nUNIT: {data.get('cadastre_id')}")
        print(f"  TYPE:  {data.get('type')}")
        print(f"  OWNER: {data.get('ownership')}")
        print(f"  AREA:  {data.get('area')} m2")
        if data.get('floor'):
            print(f"  FLOOR: {data.get('floor')}")
        
        # If finding plots, show neighbors
        if data.get('type') == 'LAND':
            print(f"  NEIGHBORS: {data.get('neighbors')}")

elif mode == "social_check":
    # The Social Housing Audit
    report = service.analyze_social_risk(target)
    print("\n--- SOCIAL RISK REPORT ---")
    print(f"TOTAL UNITS SCANNED: {report['total_units']}")
    print(f"PRIVATE OWNED:       {report['private_units']}")
    print(f"MUNICIPAL (SOCIAL):  {report['municipal_units']}")
    print(f"RATIO:               {report['social_housing_ratio']:.2%}")
    print(f"VERDICT:             {report['risk_verdict']}")
    print("--------------------------")

if name == "main":
parser = argparse.ArgumentParser()
parser.add_argument("target", help="Query (Address or ID)")
parser.add_argument("--mode", choices=["scan", "social_check"], default="scan")
args = parser.parse_args()
run_audit(args.target, args.mode)



=========================================
FILE: ./prompts/detective_prompt_v1.md
=========================================
# Role: Senior Real Estate Forensic Detective (Sofia, BG)
You are an expert in Sofia property forensics. Your goal is to cross-examine listing photos against the provided text to detect "Broker Lies" and legal risks.

## Forensic Tasks:
1. **Landmark Geolocation (Triangulation):** 
   - Search backgrounds for unique Sofia landmarks (Vitosha Mountain angle, TV Tower, NDK, Paradise Mall, etc.).
   - Extract names of shops, metro stations, or street signs visible in the pixels or mentioned in text.
2. **Object Localization (Inventory):** 
   - Count visible AC units and heating radiators.
   - Look for the 'TEC' (Central Heating) nodes or gas boilers.
3. **Architectural Era Analysis:** Identify if the building is Pre-1989 (Panel/EPC) or Post-2000 (Brick) based on facade style and materials.
4. **The Atelier Trap:** Identify "shop-style" glass, ground-floor placements, or lack of balconies which suggest non-residential status.
5. **Orientation Check:** Compare the light levels. If "Sunny South" is claimed but shadows suggest North, flag it.

## Output Format (JSON Only):
{
  "address_prediction": "Specific street or clue",
  "landmarks": ["Name of shop/landmark found"],
  "neighborhood_match": "Confident/Suspicious/Inconsistent",
  "building_type": "Panel / Brick / EPC / New Construction",
  "heating_inventory": {
    "ac_units": 0,
    "radiators": 0,
    "has_central_heating": boolean
  },
  "visual_red_flags": ["List specific discrepancies"],
  "light_exposure": "North/South/East/West",
  "confidence_score": 0-100
}


=========================================
FILE: ./db/schema_v1.sql
=========================================
-- GLASHAUS REFERENCE SCHEMA (Synced with Production)
-- Includes logic from migrations 001 (workflow), 002 (currency), and 003 (area precision)

CREATE EXTENSION IF NOT EXISTS postgis;

CREATE TYPE report_status AS ENUM ('PENDING', 'PROCESSING', 'VERIFIED', 'MANUAL_REVIEW', 'REJECTED');

CREATE TABLE buildings (
    id SERIAL PRIMARY KEY,
    cadastre_id VARCHAR(50) UNIQUE NOT NULL,
    address_full VARCHAR(255),
    latitude FLOAT,
    longitude FLOAT,
    construction_year INT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE listings (
    id SERIAL PRIMARY KEY,
    source_url TEXT UNIQUE NOT NULL,
    content_hash VARCHAR(64), -- Idempotency check
    price_bgn NUMERIC(12, 2), -- Financial Precision
    advertised_area_sqm NUMERIC(10, 2), -- Area Precision
    description_raw TEXT,
    scraped_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
CREATE INDEX idx_listings_content_hash ON listings(content_hash);

CREATE TABLE reports (
    id SERIAL PRIMARY KEY,
    listing_id INT REFERENCES listings(id) ON DELETE CASCADE,
    building_id INT REFERENCES buildings(id),
    status report_status DEFAULT 'PENDING',
    risk_score INT,
    ai_confidence_score INT DEFAULT 0,
    legal_brief TEXT,
    discrepancy_details JSONB,
    image_archive_urls JSONB,
    cost_to_generate NUMERIC(10, 4), -- API Usage Cost
    manual_review_notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE price_history (
    id SERIAL PRIMARY KEY,
    listing_id INT REFERENCES listings(id),
    price_bgn NUMERIC(12, 2),
    changed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);


=========================================
FILE: ./db/migration_001_status_workflow.sql
=========================================
-- 1. Create the Workflow Status Enum
-- This supports the PENDING -> VERIFIED -> MANUAL_REVIEW flow
DO $$ BEGIN
    CREATE TYPE report_status AS ENUM ('PENDING', 'PROCESSING', 'VERIFIED', 'MANUAL_REVIEW', 'REJECTED');
EXCEPTION
    WHEN duplicate_object THEN null;
END $$;

-- 2. Update 'reports' table
ALTER TABLE reports 
ADD COLUMN IF NOT EXISTS status report_status DEFAULT 'PENDING',
ADD COLUMN IF NOT EXISTS ai_confidence_score INT DEFAULT 0,
ADD COLUMN IF NOT EXISTS manual_review_notes TEXT;

-- 3. Update 'listings' table for Idempotency
-- We hash the file/content to prevent duplicate processing of the same upload
ALTER TABLE listings
ADD COLUMN IF NOT EXISTS content_hash VARCHAR(64);

CREATE INDEX IF NOT EXISTS idx_listings_content_hash ON listings(content_hash);

-- 4. Create the Manual Review Queue View
-- This allows the Admin Panel to easily select tasks needing human eyes
CREATE OR REPLACE VIEW view_manual_review_queue AS
SELECT 
    r.id as report_id,
    l.source_url,
    r.ai_confidence_score,
    r.risk_score,
    r.created_at
FROM reports r
JOIN listings l ON r.listing_id = l.id
WHERE r.status = 'MANUAL_REVIEW'
ORDER BY r.risk_score DESC;


=========================================
FILE: ./db/migrations/env.py
=========================================
from logging.config import fileConfig
from sqlalchemy import engine_from_config
from sqlalchemy import pool
from alembic import context
import os
import sys

# Add src to path so we can import models
sys.path.append(os.getcwd())

from src.db.session import Base
from src.core.config import settings
from src.db.models import Listing, Report, Building 

config = context.config

if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Overwrite config URL with Environment Settings
config.set_main_option("sqlalchemy.url", settings.DATABASE_URL)

target_metadata = Base.metadata

def run_migrations_offline() -> None:
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online() -> None:
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()


=========================================
FILE: ./db/migrations/versions/migration_003_area_precision.py
=========================================
"""fix_area_precision

Revision ID: 003
Revises: 002
Create Date: 2025-12-19 20:30:00.000000

"""
from alembic import op
import sqlalchemy as sa

# revision identifiers, used by Alembic.
revision = '003'
down_revision = '002'
branch_labels = None
depends_on = None

def upgrade() -> None:
    # 1. Alter Listings Table
    op.alter_column('listings', 'advertised_area_sqm',
               existing_type=sa.Float(),
               type_=sa.Numeric(precision=10, scale=2),
               postgresql_using='advertised_area_sqm::numeric',
               existing_nullable=True)

def downgrade() -> None:
    op.alter_column('listings', 'advertised_area_sqm',
               existing_type=sa.Numeric(precision=10, scale=2),
               type_=sa.Float(),
               existing_nullable=True)


=========================================
FILE: ./db/migration_002_fix_currency.sql
=========================================
-- Fix Financial Precision for Listings and Reports
-- RUN THIS MANUALLY OR VIA ALEMBIC

ALTER TABLE listings 
ALTER COLUMN price_bgn TYPE NUMERIC(12, 2) 
USING price_bgn::numeric;

ALTER TABLE reports
ALTER COLUMN cost_to_generate TYPE NUMERIC(10, 4)
USING cost_to_generate::numeric;

ALTER TABLE price_history
ALTER COLUMN price_bgn TYPE NUMERIC(12, 2)
USING price_bgn::numeric;


=========================================
FILE: ./README.md
=========================================
# GLASHAUS: The Real Estate Integrity Engine

## Mission
To eliminate information asymmetry in the Sofia real estate market via automated due diligence.
We leverage OSINT, LLM reasoning, and Official Registry cross-referencing.

## Architecture
- **Text Layer:** Gemini Flash (Cost optimized)
- **Vision Layer:** Gemini Pro (Geospatial reasoning)
- **Data Layer:** PostgreSQL (Structured) + S3 (Archives)

## Status
- **Phase:** Pre-Alpha / Architectural Blueprint
- **Deploy Target:** Jan 2026 (Launch)


=========================================
FILE: ./requirements.txt
=========================================
fastapi==0.109.0
uvicorn==0.27.0
sqlalchemy==2.0.25
psycopg2-binary==2.9.9
httpx==0.26.0
playwright==1.41.0
asgiref==3.7.2
pydantic==2.6.0
pydantic-settings==2.1.0
google-generativeai>=0.7.0
beautifulsoup4==4.12.3
celery==5.3.6
redis==5.0.1
alembic==1.13.1
Pillow==10.2.0
structlog>=24.1.0


=========================================
FILE: ./-G
=========================================


=========================================
FILE: ./Dockerfile
=========================================
# STAGE 1: Builder
FROM python:3.11-slim as builder

WORKDIR /app
RUN apt-get update && apt-get install -y \
    libpq-dev gcc build-essential \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# STAGE 2: Runtime
FROM python:3.11-slim as runtime

WORKDIR /app

# Install only runtime libs (libpq for Postgres)
RUN apt-get update && apt-get install -y \
    libpq5 netcat-openbsd \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /root/.local /root/.local
ENV PATH=/root/.local/bin:$PATH

# Copy Application Code
COPY . .

# Create a non-root user for security
RUN useradd -m glashaus_user
USER glashaus_user

EXPOSE 8000

CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]


=========================================
FILE: ./docker-compose.yml
=========================================
version: '3.8'

services:
  # 1. GATEWAY (Entry Point)
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - api

  # 2. API (Internal)
  api:
    build: .
    expose:
      - "8000" # Not mapped to host anymore, only accessible via Nginx
    environment:
      - POSTGRES_SERVER=db
      - POSTGRES_USER=glashaus_user
      - POSTGRES_PASSWORD=secret_password
      - POSTGRES_DB=glashaus_db
      - REDIS_URL=redis://redis:6379/0
      # - GEMINI_API_KEY=${GEMINI_API_KEY} # Uncomment for prod
    depends_on:
      - db
      - redis
    volumes:
      - ./src:/app/src

  # 3. WORKER
  worker:
    build: .
    command: celery -A src.worker.celery_app worker --loglevel=info
    environment:
      - POSTGRES_SERVER=db
      - POSTGRES_USER=glashaus_user
      - POSTGRES_PASSWORD=secret_password
      - POSTGRES_DB=glashaus_db
      - REDIS_URL=redis://redis:6379/0
      # - GEMINI_API_KEY=${GEMINI_API_KEY}
    depends_on:
      - db
      - redis
    volumes:
      - ./src:/app/src

  # 4. REDIS
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  # 5. DB
  db:
    image: postgis/postgis:15-3.4
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=glashaus_user
      - POSTGRES_PASSWORD=secret_password
      - POSTGRES_DB=glashaus_db
    ports:
      - "5432:5432"

volumes:
  postgres_data:


=========================================
FILE: ./tests/test_api.py
=========================================
from fastapi.testclient import TestClient
from src.main import app

client = TestClient(app)

def test_health_check():
    response = client.get("/")
    assert response.status_code == 200
    assert response.json()["status"] == "OPERATIONAL"

def test_audit_flow():
    payload = {"url": "https://www.imot.bg/pcgi/imot.cgi?act=5&adv=mock123"}
    response = client.post("/audit", json=payload)
    
    assert response.status_code == 200
    data = response.json()
    assert data["status"] == "QUEUED"
    assert "listing_id" in data


=========================================
FILE: ./alembic.ini
=========================================
[alembic]
script_location = db/migrations
prepend_sys_path = .
sqlalchemy.url = driver://user:pass@localhost/dbname

[post_write_hooks]

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S


=========================================
FILE: ./nginx/nginx.conf
=========================================
events {}

http {
    upstream glashaus_api {
        server api:8000;
    }

    server {
        listen 80;
        
        # Proxy all API requests to the FastAPI container
        location / {
            proxy_pass http://glashaus_api;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        # Health check endpoint
        location /health {
            return 200 'alive';
            add_header Content-Type text/plain;
        }
    }
}


=========================================
FILE: ./--data-urlencode
=========================================


=========================================
FILE: ./-H
=========================================


=========================================
FILE: ./-X
=========================================


=========================================
FILE: ./forensics/headers.txt
=========================================
HTTP/2 200 
date: Fri, 19 Dec 2025 10:22:43 GMT
content-type: text/html
server: cloudflare
vary: Accept-Encoding
set-cookie: imot_session_redirect=adv%091c171899111%09act%095%09; domain=.imot.bg; path=/
referrer-policy: no-referrer-when-downgrade
strict-transport-security: max-age=15552000
nel: {"report_to":"cf-nel","success_fraction":0.0,"max_age":604800}
cf-cache-status: DYNAMIC
report-to: {"group":"cf-nel","max_age":604800,"endpoints":[{"url":"https://a.nel.cloudflare.com/report/v4?s=j9v062ydriN%2F%2BXubjtcJm8HyyWIt5Iad0Ozkt1xE1LO0Maoge0%2FxNlheTZuu%2FtFcpdrE5nQ8tbcs1nOXtqzEhMuYqUOOk1Zv"}]}
cf-ray: 9b062e33d9d0d0d4-SOF
alt-svc: h3=":443"; ma=86400



=========================================
FILE: ./forensics/page_utf8.html
=========================================


=========================================
FILE: ./-d
=========================================


=========================================
FILE: ./test_forensics.py
=========================================
import asyncio
import sys
import json
from src.services.forensics_service import SofiaMunicipalForensics

# The identifier found in your curl examples (Bankya/Verdikal)
TEST_ID = "02659.2196.1102"

async def main():
    print(f"[*] Starting Forensic Audit for Cadastre ID: {TEST_ID}...")
    
    service = SofiaMunicipalForensics()
    try:
        results = await service.run_full_audit(TEST_ID)
        
        print("\n--- AUDIT RESULTS ---")
        print(json.dumps(results, indent=2, ensure_ascii=False))
        
        # Validation Logic
        if results.get("expropriation", {}).get("is_expropriated"):
            print("\n[!] CRITICAL ALERT: Property is flagged for Expropriation!")
        
        if results.get("compliance_act16", {}).get("has_act16"):
            print("\n[+] SUCCESS: Act 16 Certificate Found.")
        else:
            print("\n[-] WARNING: No Act 16 found.")
            
    except Exception as e:
        print(f"\n[!] FATAL ERROR: {e}")

if __name__ == "__main__":
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
